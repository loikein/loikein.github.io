---
title: "Notes on Time Series"
author: "loikein"
date: "2019-04-15"
slug: "notes-time-series"
tags: ["notes","R","stat"]
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE, tidy = FALSE, echo = TRUE, eval = FALSE)
```

## Definition

- Time series: collection of observations indexed by date
$$\begin{aligned}\{y_t\}_{t=-\infty}^{\infty} &= \{y_t\}_{t\in\mathbb{Z}} \\
&= \{ \dots, y_{-1}, y_0, y_1, y_2,\dots, y_{T-1}, y_{T}, y_{T+1},\dots\} \end{aligned}$$
    - Observed sample: $\{ y_1, y_2,\dots, y_{T-1}, y_{T}\}$
    - Underlying random variables: $\{ Y_1, Y_2,\dots, Y_{T-1}, Y_{T}\}$
- Stochastic process: ordered sequence of random variables
$$\{Y_t\}_{t=0,\pm 1,\pm 2,\dots} =\{Y_t\}_{t\in\mathbb{Z}}$$
    - Probability space: $(Ω,\mathcal{A},P)$
    - Mapping: $\{Y_t(\cdot)\}: Ω\mapsto \mathbb{R}^{\mathbb{Z}}$ s.t. $\{Y_t(ω) \}_{t\in\mathbb{Z}} = \{Y_t(ω) : t\in\mathbb{Z} \}$
    - Fix $t$, vary $ω\implies$ scalar random variable $Y_t(ω)$
    - Fix $ω$, vary $t\implies$  
    Trajectory (realization): $\{ y_1, y_2,\dots, y_{T-1}, y_{T}\}$ 

### Property

- First moment: $μ_t = E[Y_t]$
- Second moments
    - Variance: $γ_{0t} = E\big[(Y_t - μ_t)^2\big]$
    - Covariance: $γ_{jt} = E\big[(Y_t - μ_t)(Y_{t-j} - μ_{t-j})\big]$
    - $E[Y_t^2]<\infty\implies γ_{jt}<\infty$
- Weakly (Covariance) Stationary 
$$\begin{cases} E[Y_t] = μ < \infty & \text{for all }t \\
Var(Y_t) = γ_0 < \infty & \text{for all }t\\
Cov(Y_t, Y_{t-j}) = γ_j & \text{for all }t,j\end{cases}$$ 
    - $γ_{j,t} = γ_{j,t+s}$ for all $t,s$
- AutoCorrelation Function (ACF)
$$ρ_j = \frac{Cov(Y_t,Y_{t-j})}{Var(Y_t)} = \frac{γ_j}{γ_0}$$ 
    - Sample ACF: $$\hat{ρ}_j = \frac{\frac{1}{T-j}\sum^T_{t=j+1}(Y_t-\bar{Y})(Y_{t=j}-\bar{Y})}{\frac{1}{T}\sum^T_{t=1}(Y_t-\bar{Y})^2}$$ 
    - $\sqrt{T}(\hat{ρ}_j-ρ_j)\to\mathcal{N}(0,\exists v_k)$ 
- Ergodicity for the mean  
For stationary process $Y_t$,
$$\begin{aligned}\bar{Y}_T &= \frac{1}{T}\sum_T y_t \\
&\overset{p}{\to}μ && T\to\infty\end{aligned}$$
    - Let $Y_t$ be s.t. $\sum_{\infty} |γ_j|<\infty$, then $Y_t$ is ergodic

## Analysis

### Procedure

1. Plot and check for
    - Trend
    - Seasonality
    - Abrupt changes
    - Outliers
2. Remove the trend and seasonality to obtain stationary residuals
3. Choose a model to fit the residuals (ARMA, GARCH)
4. Forecast the residuals
5. Invert transformations to forecast the original series

### Trend

- Model with trend
$$\begin{aligned}Y_t &= \underbrace{m_t}_{\text{Trend component}} + \underbrace{X_t}_{\text{Random noise component}} \\
E[X_t] &= 0\end{aligned}$$
- Moving average filter  
For $t = q+1, q+2,\dots, T-q$, 
$$\begin{aligned}\hat{m}_t &= \frac{1}{2q+1}\sum_{j=-q}^q Y_{t-j} \\
&= \frac{1}{2q+1}\sum_{j=-q}^q m_{t-j} + \frac{1}{2q+1}\sum_{j=-q}^q X_{t-j} \\
&\approx m_t \\
Y_t - \hat{m}_t &\approx X_t \end{aligned}$$
- Exponential smoothing (better for forecasting)
$$\begin{aligned}\hat{m}_1 &= Y_1 \\
\hat{m}_t &= αY_t  + (1-α)\hat{m}_{t-1} \\
&= \sum_{j=0}^{t-2}α(1-α)^jY_{t-j}  + (1-α)^{t-1}Y_1\end{aligned}$$
- Polynomial fitting  
Let $$m_t = a_0 + a_1t+ a_2t^2 + \dots$$
Choose $a$'s as 
$$\begin{aligned}(\hat{a}_0, \hat{a}_1, \hat{a}_2,\dots) &= \text{argmin} \ \sum^T_{t=1}(Y_t - m_t)^2 \\
\hat{m}_t &= \hat{a}_0 + \hat{a}_1t + \hat{a}_2t^2 + \dots\end{aligned}$$
- Elimination  
For polynomial trend, choose $k$ s.t.
$$\begin{aligned}m_t &= a_0 + a_1t + \dots + a_kt^k \\
Δ^km_t &= k!a_k \\
Δ^k Y_t &= Δ^km_t + Δ^X_t \\
&= k!a_k + Δ^X_t \\
E[Δ^k Y_t] &= k!a_k\end{aligned}$$

### Trend & Seasonality

- Model with trend & seasonality (period $d$)
$$\begin{aligned}Y_t &= m_t + \underbrace{s_t}_{\text{Seasonal component}} + X_t \\
E[X_t] &= 0 \\
s_{t+d} &= s_t \\
\sum_{j=1}^d s_j &= 0 \end{aligned}$$
- Estimation
    1. Estimate trend (moving average)  
    For $t = q+1, q+2,\dots, T-q$, when $d = 2q$  
    $$\begin{aligned}\hat{m} &= \frac{1}{d}\Big(\frac{1}{2}Y_{t-q} + Y_{t-(q-1)} + \dots + Y_{t+(q-1)} + \frac{1}{2}Y_{t+q} \Big) \\ \end{aligned}$$
    When $d = 2q+1$  
    $$\begin{aligned}\hat{m}_t &= \frac{1}{d}\sum_{j=-q}^q Y_{t-j}\end{aligned}$$
    2. Estimate seasonal component  
    For $t = 1, 2,\dots, d$  
    $$\begin{aligned}w_t &= \frac{1}{\#\{j: q<t+jd \leq T-q\}}\sum_{j: q<t+jd \leq T-q} (Y_{t+jd} - \hat{m}_{t+jd}) \\
    \hat{s}_t &= w_t - \frac{1}{d}\sum_{i=1}^d w_i\end{aligned}$$
    3. Re-estimate trend  
    De-seasonalized data:
    $$\begin{aligned}d_t &= Y_t - \hat{s}_t \\
    \hat{X}_t &= Y_t - \hat{m}_t - \hat{s}_t \end{aligned}$$
- Elimination  
First liminate seasonality: 
$$\begin{aligned}Δ_d Y_t &= Y_t - Y_{t-d} \\
&= m_t - m_{t-d} + X_t - X_{t-d} \end{aligned}$$
Then [#eliminate trend](#trend)

### Lag & Difference Operator

- Lag operator: $L^0=1$, $Ly_t = y_{t-1}$, $L^2 y_t = y_{t-2}$, … 
    - Lag polynomial
    $$\begin{aligned} θ(L) &= 1 - θ_1L - θ_2L^2 -\dots - θ_pL^p \\
    y_t &= θ_1Ly_t -\dots - θ_p L^p y_t + ε_t \\
    θ(L) y_t &= ε_t\end{aligned}$$ 
- Difference operator: $Δ=1-L$, $Δ^k = (1-L)^k$
    - Difference polynomial
    $$\begin{aligned}Δ^2 y_t &= Δ(Δ y_t) \\
    &= (1-L)(1-L)y_t \\
    &= (1-2L +L^2)y_t \\
    &= y_t - 2y_{t-1} + y_{t-2}\end{aligned}$$
- Lag-$d$ differencing operator: $Δ_d = (1 - L^d)$

## ARMA

### White Noise

- Gaussian White Noise: $ε_t\overset{iid.rvs}{\sim}\mathcal{N}(0,σ^2)$ 
    - White Noise: $ε_t\overset{iid.rvs}{\sim}(0,σ^2)$ 
    - Stationary
- Random walk $$Y_t = \sum_{s=1}^t ε_s$$
    - Not stationary
$$\begin{aligned}\text{Mean:} && E[Y_t] &= 0 \\
\text{Variance:} && Var(Y_t) &= tσ^2 \\
\text{Covariance:} && Cov(Y_t,Y_{t-j}) &= 0 \\
\text{ACF:} && Corr(Y_t,Y_{t-j}) &= \frac{Cov(Y_t,Y_{t-j})}{Var(Y_t)}= 0\end{aligned}$$

### MA

- MA (1) $$Y_t = μ + ε_t + θε_{t-1}$$
$$\begin{aligned}\text{Mean:} && E[Y_t] &= μ \\
\text{Variance:} && Var(Y_t) &= σ^2\cdot (1 + θ^2) \\
\text{Covariance:} && Cov(Y_t,Y_{t-j}) &= \begin{cases} θσ^2 & j=1\\ 0 & j\geq 2\end{cases} \\
\text{ACF:} && Corr(Y_t,Y_{t-j}) &= \begin{cases} \frac{θ}{1 + θ^2} \leq \frac{1}{2} & j=1\\ 0 & j\geq 2\end{cases}\end{aligned}$$
- MA (q)
$$\begin{aligned}Y_t &= μ + ε_t + θ_1 ε_{t-1} +\dots + θ_q ε_{t-q} \\
&= μ + \sum_{j=0}^q θ_j ε_{t-j}\end{aligned}$$
$$\begin{aligned}\text{Mean:} && E[Y_t] &= μ \\
\text{Variance:} && Var(Y_t) &= σ^2\cdot (1 + θ_1^2 + θ_2^2 + \dots + θ_q^2) \\
\text{Covariance:} && Cov(Y_t,Y_{t-j}) &= \begin{cases} σ^2\cdot\sum_{i=0}^{q-j} θ_iθ_{i+j} & 0\leq j\leq q\\ 0 & j > q\end{cases} \\
\text{ACF:} && Corr(Y_t,Y_{t-j}) &= \begin{cases} ? & 0\leq j\leq q\\ 0 & j > q\end{cases}\end{aligned}$$
- MA ($\infty$)
$$Y_t = μ + \sum_{j=0}^{\infty} ψ_j ε_{t-j}$$
$$\begin{aligned}\text{Mean:} && E[Y_t] &= μ \\
\text{Variance:} && Var(Y_t) &= σ^2\cdot \sum_{i=0}^{\infty} ψ_i^2 = γ_0 \\
\text{Covariance:} && Cov(Y_t,Y_{t-j}) &= σ^2\cdot\sum_{i=0}^{\infty} ψ_iψ_{i+j} = γ_j \\
\text{ACF:} && Corr(Y_t,Y_{t-j}) &= ?\end{aligned}$$

### MA Stationarity

- Any MA (q) process is stationary
- Any MA (∞) process is stationary
    - Absolute summability
    $$\begin{aligned}&\sum_{j=0}^{\infty} |ψ_j| < \infty \\
    &γ_k = σ^2\cdot\sum_{j=0}^{\infty} ψ_jψ_{j+k} \\
    \implies &\sum_{j=0}^{\infty} |γ_j| < \infty\end{aligned}$$
    - Square summability $$\sum_{j=0}^{\infty} ψ_j^2 < \infty$$

### AR

- AR (1)
$$\begin{aligned}Y_t &= c + φ Y_{t-1} + ε_t \\
&= c  + ε_t + φ\cdot(c + φY_{t-2} + ε_{t-1}) \\
&= c + φc + ε_t + φ ε_{t-1} + φ^2Y_{t-2} \\
&= c + φc + \dots + φ^j c \\
&\phantom{\ggg} + ε_t + φε_{t-1} +\dots + φ^jε_{t-j} + φ^{j+1}Y_{t-(j+1)} \\
&= c\cdot\sum_{j=0}^{\infty} φ^j + \sum_{j=0}^{\infty} φ^j\cdot ε_{t-j}\end{aligned}$$
When $|φ|<1$, $φ^{j+1}Y_{t-(j+1)}\to 0$ as $j\to\infty$, AR (1) becomes MA ($\infty$)
    - Absolute summability
    - Stationary
    $$\begin{aligned}MA(\infty) && Y_t &= μ + \sum_{j=0}^{\infty} ψ_j\cdot ε_{t-j} \\
    AR(1) && Y_t &= \frac{c}{1-φ} + \sum_{j=0}^{\infty} φ^j\cdot ε_{t-j} \\
    \text{Mean:} && E[Y_t] &= \frac{c}{1-φ} = μ \\
    \text{Variance:} && Var(Y_t) &= \frac{σ^2}{1-φ^2} = γ_0 \\
    \text{Covariance:} && Cov(Y_t,Y_{t-j}) &= \frac{σ^2φ^j}{1-φ^2}  = γ_j \\
    \text{ACF:} && Corr(Y_t,Y_{t-j}) &= φ^j\end{aligned}$$
- AR (p)
$$Y_t = c + φ_1 Y_{t-1} +\dots + φ_p Y_{t-p} + ε_t$$
- ARMA (p,q)
$$Y_t = θ_1Y_{t-1} +\dots + θ_pY_{t-p} + ε_t + α_1 ε_{t-1} +\dots + α_q ε_{t-q}$$

<!-- 
$$\begin{aligned}\text{Mean:} && E[Y_t] &= δ + θ E[Y_{t-1}] \\
&& &= \frac{δ}{1-θ} \\ 
&& &= μ \\
\text{Variance:} && Var(Y_t) &= θ^2Var(Y_{t-1}) + Var(ε_t) \\
&& &= \frac{σ^2}{1-θ^2} \\
\text{Covariance:} && Cov(Y_t,Y_{t-j}) &= θ^k\frac{σ^2}{1-θ^2} \\
\text{ACF:} && Corr(Y_t,Y_{t-j}) &= \frac{Cov(Y_t,Y_{t-j})}{Var(Y_t)} \\
&& &= θ^k \end{aligned}$$
 -->

### AR (1) Stationarity

Reference:  
[Characteristic Equation AR (p) | Real Statistics Using Excel](http://www.real-statistics.com/time-series-analysis/autoregressive-processes/characteristic-equation-autoregressive-processes/)  
[Intuition behind the characteristic equation of an AR or MA process](https://stats.stackexchange.com/questions/208656/intuition-behind-the-characteristic-equation-of-an-ar-or-ma-process)

- AR (1) with __characteristic equation__
$$\begin{aligned} Y_t &= c + φLY_t + ε_t \\
φ(L) &= 1 - φL \\
φ(L) \cdot Y_t &= (1 - φL)\cdot Y_t \\
&= c + ε_t \end{aligned}$$ 
- Characteristic root
$$|z|\begin{cases}<1 & \text{explosive} \\ =1 & \text{unit root (explosive)} \\ >1 & \text{stationary}\end{cases}$$ 
- AR (1) is stationary $\iff φ(L)$ is invertible  
$\iff |z| > 1$ $\implies |φ|<1$ 
$$\begin{aligned}1 - φ\cdot z &= 0 \\
|z| &= \Bigg| \frac{1}{φ}\Bigg|> 1 \\
|φ| &< 1\end{aligned}$$
therefore, 
$$\begin{aligned}AR (1) && Y_t &= \frac{c + ε_t}{φ(L)} \\
&& &= \frac{c}{1-φL} + \frac{ε_t}{1-φL}\\
&& &= \frac{c}{1-φL} + (ε_t + φLε_t + φ^2 L^2 ε_t + \dots )\\
&& &= \frac{c}{1-φ} + \sum_{j=0}^{\infty} φ^j\cdot ε_{t-j} \\
MA(\infty) && Y_t &= μ + \sum_{j=0}^{\infty} ψ_j ε_{t-j}\end{aligned}$$

### AR (p) Stationarity

- A process is __causal form $ε_t$__  
$\iff \{Y_t\}$ has a covariance-stationary representation  
$\iff \{Y_t\}$ is stationary
$$\begin{cases}Y_t = μ + \sum_{j=0}^{\infty} ψ_j ε_{t-j} \\
\sum_{j=0}^{\infty} |ψ_j| < \infty\end{cases}$$
- Some __linear processes__ are not causal form $ε_t$  
but $\{ε_t\}$ is stationary $\implies\{Y_t\}$ is stationary
$$\begin{cases}Y_t = μ + \sum_{j=-\infty}^{\infty} ψ_j ε_{t-j} \\
ψ_0 = 1 \\
\sum_{j=-\infty}^{\infty} |ψ_j| < \infty\end{cases}$$
- AR (p) with characteristic equation
$$\begin{aligned}Y_t &= c + φ_1LY_t + \dots + φ_p L^p Y_t + ε_t \\
φ(L) &= 1 - φ_1L - φ_2L^2 -\dots - φ_pL^p \\
φ(L) Y_t &= c + ε_t \end{aligned}$$
- AR (p) has a covariance-stationary representation  
$\iff |z|>1$ for all $z$ (outside the unit circle)  
$\implies\sum_{j=1}^p φ_j <1$  
$$\begin{aligned}1 - φ_1z - φ_2z^2 - \dots - φ_pz^p &= 0 \\
|z| &= \Big|\frac{1}{φ_1L + φ_2L^2 + \dots + φ_pL^p}\Big| \\
&= \Bigg|\frac{1}{\sum_{j=1}^p φ_jL^j}\Bigg| \\
&= \Bigg|\frac{1}{\sum_{j=1}^p φ_j}\Bigg| > 1 \\
\Big|\sum_{j=1}^p φ_j\Big| &< 1\end{aligned}$$
therefore,
$$\begin{aligned}AR (p) &&Y_t &= \big(φ(L)\big)^{-1}\cdot (c + ε_t) \\
&& &= \big(φ(L)\big)^{-1}\cdot c + \big(φ(L)\big)^{-1}\cdot ε_t \\
MA(\infty) && Y_t &= μ + \sum_{j=0}^{\infty} ψ_j ε_{t-j}\end{aligned}$$

### AR (p) as MA (∞)

- Compare coefficients  
When $c = μ = 0$, 
$$\begin{aligned}φ(L) Y_t &= ε_t \\
Y_t &= \sum_{j=0}^{\infty} ψ_j ε_{t-j} \\
&= ψ(L) ε_t\\
φ(L)ψ(L) ε_t &= ε_t \\
φ(L)ψ(L) &= 1 \end{aligned}$$
for AR (2), 
$$\begin{aligned}&\phantom{\ggg}(1 - φ_1L - φ_2L^2)\cdot (ψ_0 + ψ_1L + ψ_2L^2 + ψ_3L^3 + \dots) = 1\\
&= (ψ_0 + ψ_1L + ψ_2L^2 + ψ_3L^3 + \dots ) - φ_1L(ψ_0 + ψ_1L + ψ_2L^2 + \dots) \\
&\phantom{\ggg} - φ_2L^2(ψ_0 + ψ_1L + ψ_2L^2 + \dots) \\
&= ψ_0 + L(ψ_1 - φ_1ψ_0) + L^2(ψ_2 - φ_1ψ_1 - φ_2ψ_0) \\
&\phantom{\ggg} + L^3(ψ_3 - φ_1ψ_2 - φ_2ψ_1) + \dots\end{aligned}$$
matching powers of $L$ gives
$$\begin{aligned}
ψ_0 &= 1 \\
ψ_1 - φ_1ψ_0 &= 0 \\
ψ_2 - φ_1ψ_1 - φ_2ψ_0 &= 0 \\
ψ_3 - φ_1ψ_2 - φ_2ψ_1 &= 0 \\
&\dots \end{aligned}$$
therefore,
$$\begin{aligned}ψ_0 &= 1 \\
ψ_1 &= φ_1 \\
ψ_2 &= φ_1^2 + φ_2 \\
ψ_3 &= φ_1^3 + φ_1φ_2 + φ_2 \\
&\dots \end{aligned}$$
- Impulse response function (IRF)
$$\begin{aligned}MA(\infty) && Y_t &= μ + \sum_{j=0}^{\infty} ψ_j ε_{t-j} \\
FOC && \frac{\partial Y_t}{\partial ε_{t-j}} &= ψ_j \\
&& \frac{\partial Y_{t+j}}{\partial ε_{t}} &= ψ_j\end{aligned}$$
- Permanent effect: AR (1)
$$\begin{aligned}&\phantom{\ggg}\lim_{j\to\infty} \Big(\frac{\partial Y_{t+j}}{\partial ε_{t}} + \frac{\partial Y_{t+j}}{\partial ε_{t+1}} + \dots + \frac{\partial Y_{t+j}}{\partial ε_{t+j}}\Big) \\
&= \lim_{j\to\infty} (φ^j + φ^{j-1} + \dots + φ^{0}) \\
&= \frac{1}{1-φ} \end{aligned}$$

### Yule-Walker Equations

Reference:  
[The Yule Walker Equations for the AR Coefficients by Gidon Eshel](http://www-stat.wharton.upenn.edu/~steele/Courses/956/Resource/YWSourceFiles/YW-Eshel.pdf)  
[Yule\-Walker Equations in Matrix Form | Coursera](https://www.coursera.org/lecture/practical-time-series-analysis/yule-walker-equations-in-matrix-form-yw0tb)

- Take expectation of AR (p)
$$\begin{aligned}Y_t &= c + φ_1 Y_{t-1} +\dots + φ_p Y_{t-p} + ε_t \\
E[Y_t] &= E[c + φ_1 Y_{t-1} +\dots + φ_p Y_{t-p} + ε_t] \\
μ &= c + φ_1 μ +\dots + φ_p μ + 0 \\
&= \frac{c}{1-φ_1 -\dots - φ_p} \\
c &= μ\cdot(1-φ_1 -\dots - φ_p)\end{aligned}$$
therefore,
$$\begin{aligned} 
Y_t &= μ\cdot(1-φ_1 -\dots - φ_p)+ φ_1 Y_{t-1} +\dots + φ_p Y_{t-p} + ε_t \\
Y_t - μ &= μ\cdot(-φ_1 -\dots - φ_p)+ φ_1 Y_{t-1} +\dots + φ_p Y_{t-p} + ε_t \\
&= φ_1 (Y_{t-1}-μ) +\dots + φ_p (Y_{t-p}-μ) + ε_t \\
(Y_t - μ)(Y_{t-j} - μ) &= φ_1 (Y_{t-1}-μ)(Y_{t-j} - μ) +\dots + φ_p (Y_{t-p}-μ)(Y_{t-j} - μ) + ε_t(Y_{t-j} - μ)\end{aligned}$$
take expectation
$$\begin{aligned}γ_j &= E\big[ (Y_t - μ)(Y_{t-j} - μ)\big] \\
&= \begin{cases} φ_1γ_{1} + φ_2γ_{2} + \dots + φ_pγ_{p} + σ^2 & j=0\\
φ_1γ_{j-1} + φ_2γ_{j-2} + \dots + φ_pγ_{j-p} & j>0\end{cases}\end{aligned}$$
- Matrix representation  
<!-- 
Let $\underset{(p+1)\times 1}{γ} = \begin{pmatrix}γ_0 \\ γ_1 \\ \vdots \\ γ_p \end{pmatrix}$, $\underset{(p+1)\times 1}{r} = \begin{pmatrix}σ^2 \\ 0 \\ \vdots \\ 0\end{pmatrix}$,  
therefore,
$$\underset{(p+1)\times (p+1)}{Γ_1} = \begin{pmatrix}1 & 0 & 0 & \cdots & 0\\ -φ_1 & 1 & 0 &\cdots &0 \\-φ_2 & -φ_1 & \ddots & \ddots & \vdots \\\vdots & \vdots &\ddots & \ddots & 0\\-φ_p & -φ_{p-1} &\cdots &-φ_1 & 1\end{pmatrix}$$
and
$$\underset{(p+1)\times (p+1)}{Γ_2} = \begin{pmatrix}0 & -φ_1 & -φ_2 & \cdots & -φ_p \\ 0 & -φ_2 & \cdots & -φ_p & 0 \\ \vdots & \vdots &\iddots &\iddots &\vdots \\ 0 & -φ_p & 0 &\cdots & 0 \\ 0 & 0 & \cdots & \cdots & 0\end{pmatrix}$$
Let 
$$\begin{aligned}Γ &= Γ_1 + Γ_2 \\
Γ\cdot γ &= r \\
γ &= Γ^{-1}\cdot r\end{aligned}$$ -->
Collect all equations for $j>0$
$$\begin{cases}γ_1 = φ_1γ_{0} + φ_2γ_{1} + \dots + φ_pγ_{p-1} \\
γ_2 = φ_1γ_{1} + φ_2γ_{2} + \dots + φ_pγ_{p-2} \\
\phantom{=}\cdots \\
γ_p = φ_1γ_{p-1} + φ_2γ_{p-2} + \dots + φ_pγ_{0}\end{cases}$$
devide both sides by $γ_0$
$$\begin{cases}ρ_1 = φ_1ρ_{0} + φ_2ρ_{1} + \dots + φ_pρ_{p-1} \\
ρ_2 = φ_1ρ_{1} + φ_2ρ_{2} + \dots + φ_pρ_{p-2} \\
\phantom{=}\cdots \\
ρ_p = φ_1ρ_{p-1} + φ_2ρ_{p-2} + \dots + φ_pρ_{0}\end{cases}$$
rewrite as matrix, since $ρ_0=1$, 
$$\begin{aligned}
\underbrace{\begin{pmatrix}ρ_1\\ρ_2\\\vdots\\ ρ_{p-1} \\ ρ_p\end{pmatrix}}_{ρ}
&=\underbrace{\begin{pmatrix}1 & ρ_1 & ρ_2 & \cdots & ρ_{p-1}\\ ρ_1 & 1 & ρ_1 & \cdots & ρ_{p-2}\\ \vdots & & \ddots &\ddots & \vdots\\ ρ_{p-2} & ρ_{p-3} & \cdots & 1 &ρ_1 \\ρ_{p-1} & ρ_{p-2} & \cdots & ρ_1 & 1\end{pmatrix}}_{Ρ}
\cdot \underbrace{\begin{pmatrix}φ_1\\φ_2\\\vdots\\φ_{p-1} \\ φ_p \end{pmatrix}}_{Φ} \\
ρ &= Ρ\cdot Φ \\
Φ &= Ρ^{-1}\cdot ρ\end{aligned}$$

### ARMA

## ARCH & GARCH

### ARCH

- AutoRegressive Conditional Heteroskedasticity
    - $σ_t^2\neq σ_ε^2$
- ARCH (p): 
$$\begin{aligned}Y_t &= σ_t\cdot ε_t \\
σ_t^2 &= ω + \sum_{i=1}^p α_i\cdot Y_{t-i}^2\end{aligned}$$
    - $ε_t\overset{iid}{\sim}(0,1)$
    - $ω>0$, $α_i\geq 0$, $p\in\mathbb{N}$
    - If $α_i=0$ for all $i\implies$ white noise

### ARCH Stationarity

- $0\leq \sum_p α_i <1$ $\implies$ stationary, causal $\{Y_t\}$
- For ARCH (1): $E[Y_t^4]<\infty\iff$ $α_1<\frac{1}{\sqrt{3}}$ 
- For ARCH (p): $E[Y_t^4]=c<\infty$, then
$$\begin{aligned}
\text{ARCH (p)} && Y_t^2 &= ω + \sum_{i=1}^p α_i\cdot Y_{t-i}^2 + η_t \\
\text{Error}&& η_t &= σ_t^2(ε_t^2 - 1) \\
\text{AR (p)} && Y_t &= c + \sum_{i=1}^p φ_i Y_{t-i} + ε_t\end{aligned}$$
$η_t$ is zero-mean, stationary, uncorrelated error

### GARCH

- Generalized ARCH
- GARCH (1,1) $\implies$ ARCH(∞)
$$\begin{aligned}Y_t &= σ_t\cdot ε_t \\
σ_t^2 &= ω + α\cdot Y_{t-1}^2 + β\cdot σ_{t-1}^2\end{aligned}$$
- GARCH (p,q):
$$\begin{aligned}Y_t &= σ_t\cdot ε_t \\
σ_t^2 &= ω + \sum_{i=1}^p α_i\cdot Y_{t-i}^2 + \sum_{j=1}^p β_j\cdot σ_{t-j}^2\end{aligned}$$
    - $ε_t\overset{iid}{\sim}(0,1)$
    - $ω>0$, $α_i\geq 0$, $β_j\geq 0$, $p,q\in\mathbb{N}$

### GARCH Stationarity

- $0\leq \sum_p α_i + \sum_q β_j<1$ $\implies$ stationary, causal $\{Y_t\}$
- For GARCH (1,1): 
- For GARCH (p,q): $E[Y_t^4]=c<\infty$, then
$$\begin{aligned}
\text{GARCH (p,q)} && Y_t^2 &= ω + \sum_{i=1}^{\tilde{p}} \tilde{α}_i\cdot Y_{t-i}^2 + \sum_{j=1}^q β_j\cdot η_{t-j} +η_t \\
&& \tilde{p} &= \max\{p,q\} \\
&& \tilde{α}_i&= α_i + β_i \\
\text{Error}&& η_t &= σ_t^2(ε_t^2 - 1) \\
\text{ARMA } (\tilde{p},q) && Y_t &= \sum_{i=1}^{\tilde{p}}θ_iY_{t-i} + \sum_{j=i}^{q} α_jε_{t-j} + ε_t\end{aligned}$$
$η_t$ is zero-mean, stationary, uncorrelated error


## Old Stuff

### Integration

- $I(0)$ denotes stationary series
    - Mean reverting
    - Finite variance
    - Limited memory of past behaviour
- $I(1)$ denotes series that become stationary after first-differencing
    - Infinitely long memory

<!--
### Deterministic (Linear) Trend

- Another possible cause of nonstationarity: $γ\neq 0$ (even when $|θ|<1$) 
$$Y_t = δ + θY_{t-1}+γ\cdot t + ε_t$$ 
- Trend stationary
- Solution: 
    - Regress $Y_t$ on constant and trend and then use residuals  
    - include $t$ as additional variable in regression
-->

### OLS Estimation

- For AR (p)
    - Consistency: $E[Y_{t-j}ε_t] = 0$ for all $j = 1,2,3,\dots,p$  
    - Small sample bias caused ((A2) violated)
- For MA (1) with invertible lag polynomial
    - tbe

### ML Estimation

tbe

### Model Selection

- Residual analysis (Ljung–Box test)
- AIC & BIC
- Overfitting: to test ARMA (p,q), estimate ARMA (p+1,q) and ARMA (p,q+1)

### Spurious Regression

textbook P352, slide 12 P28

### Cointegration

textbook P353, slide 12 P31  
[Lecture: Introduction to Cointegration \- Applied Econometrics](http://staff.utia.cas.cz/barunik/files/appliedecono/Lecture7.pdf)

- Cointegration $\implies$ not spurious
