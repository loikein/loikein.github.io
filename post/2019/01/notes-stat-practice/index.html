<!DOCTYPE html>
<html lang="en-gb">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.72.0" />

<title>Notes on Econometrics - Practice Part - loikein&#39;s notes</title>
<meta property="og:title" content="Notes on Econometrics - Practice Part - loikein&#39;s notes">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-143089736-1', 'auto');
  
  ga('send', 'pageview');
}
</script>


  <link rel='icon' href='/favicon.ico' type='image/x-icon'/>


  






<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />


<link rel="stylesheet" href="/css/normalize.css" media="all">
<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">


<link rel="stylesheet" href="/css/clumsy-toc.css">

<link rel="stylesheet" href="/css/jump-after-toc.css">



</head>
<body>
<section class="a11y-links">
<button class="a11y-links__link" tabindex="0" onclick="jumpAfterTOC()">Skip to main content</button>
</section>
<header class="header">
<nav class="nav">
<a href="/" class="nav-logo">
<img src="/logo.png" alt="Home">
</a>
<ul class="nav-links">


  <li>
  <a class=" active" href="/post/">Notes</a>
  </li>

  <li>
  <a class="" href="/writing/">Writings</a>
  </li>

  <li>
  <a class="" href="/tags/">Tags</a>
  </li>

  <li>
  <a class="" href="/archived/">Archived</a>
  </li>

  <li>
  <a class="" href="https://duckduckgo.com/?q=site%3Anotes.loikein.one">Search</a>
  </li>

  <li>
  <a class="" href="https://github.com/loikein/loikein.github.io">Source</a>
  </li>

</ul>
</nav>

</header>

<main class="content" role="main">
  <article class="article">
    
    <span class="article-duration">17 min read</span>
    
    <h1 class="article-title">Notes on Econometrics - Practice Part</h1>
    
    <span class="article-date">2019-01-12</span>
    
    
    
    <span class="tags">
    
    
    Tags:
    
    <a href='/tags/r'>R</a>
    
    <a href='/tags/stat'>stat</a>
    
    
    
    </span>
    
    <div class="article-content">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#compare-models">Compare Models</a>
<ul>
<li><a href="#choose-explanatory-variables">Choose Explanatory Variables</a></li>
<li><a href="#choose-approach-ols-gls">Choose Approach: OLS / GLS</a></li>
<li><a href="#weird-model">Weird Model</a></li>
</ul></li>
<li><a href="#r-workout">R Workout</a>
<ul>
<li><a href="#data-generation">Data Generation</a></li>
<li><a href="#ols-regression">OLS Regression</a></li>
</ul></li>
</ul>
</div>

<p>Warning: under proofreading</p>
<p>All I wanted was a clear &amp; complete guidance (for upcoming exams).<br />
Textbook: <a href="http://93.174.95.27/book/index.php?md5=744048ECF4C4A865F45A5877AA7C2BD5">A Guide to Modern Econometrics</a></p>
<div id="compare-models" class="section level2">
<h2>Compare Models</h2>
<div id="choose-explanatory-variables" class="section level3">
<h3>Choose Explanatory Variables</h3>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
1. Compute F-statistics for comparing the loglinear model with and without squared experience.<br />
<img src="/post-img/notes-stat1--PS2Q3-3.png" width="419" /><br />
<img src="/post-img/notes-stat1--PS2Q3-4.png" width="420" /></p>
</blockquote>
<p><span class="math display">\[\begin{aligned} F &amp;= \frac{\frac{1}{J} \cdot (R^2_1 - R^2_0)}{\frac{1}{N-K} \cdot (1-R^2_1)} \\
&amp;= \frac{\frac{1}{1} \cdot (0.3783 - 0.3761)}{\frac{1}{1472 - 5} \cdot (1 - 0.3783)} \\
&amp;= 5.19\end{aligned}\]</span> ~</p>
<pre class="r"><code># quantiles of F distribution
#       (1-α),     (J),       (N - K)
qf( p = 0.95, df1 = 1, df2 = (1472-5) )
## [1] 3.847805
qf( p = 0.99, df1 = 1, df2 = (1472-5) )
## [1] 6.652194</code></pre>
<p>Therefore, <span class="math inline">\(H_0\)</span> is rejected.</p>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
2. In the loglinear model without squared experience, what is expected wage gap in percentages?<br />
<img src="/post-img/notes-stat1--PS2Q3-4.png" width="420" /></p>
</blockquote>
<p>The model:<br />
<span class="math display">\[\begin{aligned} \log y_i &amp;= 1.145 + 0.120 \cdot male + \dots \\ 
y_i &amp;= e^{1.145} + e^{0.120} \cdot e^{male} + \dots\end{aligned}\]</span><br />
Wage gap: <span class="math inline">\(e^{0.120} \cdot 100 \% - 100 \% = 12.7 \%\)</span></p>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
3. Consider the implied estimates regarding the expected increases of log wage due to increases in education (slide 5 P27).<br />
<img src="/post-img/notes-stat1--PS2Q3-4.png" width="420" /><br />
<img src="/post-img/notes-stat1--PS2Q3-0.png" width="497" /><br />
Redo the computation for the loglinear model with squared experience and compare the results:<br />
<img src="/post-img/notes-stat1--PS2Q3-3.png" width="419" /></p>
</blockquote>
<p>Specification 4:<br />
<span class="math display">\[\begin{aligned} 0.437 \times [\log(5) - \log(1)] = 0.703 \\
0.437 \times [\log(4) - \log(1)] = 0.606 \\
0.437 \times [\log(3) - \log(1)] = 0.480 \\
0.437 \times [\log(2) - \log(1)] = 0.302
 \end{aligned}\]</span><br />
Specification 3:<br />
<span class="math display">\[\begin{aligned} 0.442 \times [\log(5) - \log(1)] = 0.711 \\
0.437 \times [\log(4) - \log(1)] = 0.613 \\
0.437 \times [\log(3) - \log(1)] = 0.486 \\
0.437 \times [\log(2) - \log(1)] = 0.306
 \end{aligned}\]</span> ~</p>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
4. Explain why the comparison of specifications 5 and 4 involve <span class="math inline">\(J = 3\)</span> restrictions in the corresponding F-statistic.<br />
<img src="/post-img/notes-stat1--PS2Q3-4.png" width="420" /><br />
<img src="/post-img/notes-stat1--PS2Q3-5.png" width="425" /></p>
</blockquote>
<p>Specification 5 have 3 more explanatory variables.</p>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
5. Compute an F-statistic for comparing specifications 6 and 5.<br />
<img src="/post-img/notes-stat1--PS2Q3-6.png" width="393" /><br />
<img src="/post-img/notes-stat1--PS2Q3-5.png" width="425" /></p>
</blockquote>
<p><span class="math display">\[\begin{aligned} F &amp;= \frac{\frac{1}{J} \cdot (R^2_1 - R^2_0)}{\frac{1}{N-K} \cdot (1-R^2_1)} \\
&amp;= \frac{\frac{1}{5} \cdot (0.4032 - 0.3976)}{\frac{1}{1472 - 12} \cdot (1 - 0.4032)} \\
&amp;= 2.740\end{aligned}\]</span> ~</p>
<pre class="r"><code># quantiles of F distribution
#       (1-α),     (J),       (N - K)
qf( p = 0.95, df1 = 5, df2 = (1472 - 12) )
## [1] 2.220228
qf( p = 0.99, df1 = 5, df2 = (1472 - 12) )
## [1] 3.029772</code></pre>
<p>Therefore, <span class="math inline">\(H_0\)</span> is rejected at <span class="math inline">\(5 \%\)</span> level, but not rejected at <span class="math inline">\(1 \%\)</span> level.</p>
</div>
<div id="choose-approach-ols-gls" class="section level3">
<h3>Choose Approach: OLS / GLS</h3>
<blockquote>
<p>PS3.Q2<br />
<code>hetero.txt</code> contains data on <span class="math inline">\(y_i\)</span> and three regressors <span class="math inline">\(x_{2i}\)</span>, <span class="math inline">\(x_{3i}\)</span>, <span class="math inline">\(x_{4i}\)</span> for <span class="math inline">\(i = 1, 2, ..., 250\)</span>.<br />
The task is to find the underlying data generating process (DGP).<br />
The regression model to be estimated shall contain an intercept and all three regressors.<br />
Apply a nominal significance level of five percent in each testing situation.
1. Apply OLS to find the estimated coefficients of the regression model <span class="math inline">\(y_i =β_1 +β_2x_{2i} +β_3x_{3i} +β_4x_{4i} +ε_i\)</span><br />
~~ Report the results in a Table (see PS3.Q1).</p>
</blockquote>
<pre class="r"><code>PS3Q2_data = read.table( file=&quot;hetero.txt&quot;, header = TRUE )
N = dim( PS3Q2_data )[1]
colnames( PS3Q2_data )
## [1] &quot;y&quot;  &quot;x1&quot; &quot;x2&quot; &quot;x3&quot;
str( PS3Q2_data )
## &#39;data.frame&#39;:    250 obs. of  4 variables:
##  $ y : num  10.93 3.99 5.26 1.41 -15.51 ...
##  $ x1: num  0.269 1.708 1.356 0.767 0.74 ...
##  $ x2: num  1.897 0.126 1.472 1.9 -2.64 ...
##  $ x3: num  2.255 7.186 0.994 0.365 4.796 ...

# plot
# library(ggplot2)
# ggplot(PS3Q2_data, aes(x = x1, y = y)) + geom_point()
# ggplot(PS3Q2_data, aes(x = x2, y = y)) + geom_point()
# ggplot(PS3Q2_data, aes(x = x3, y = y)) + geom_point()

# regression
PS3Q2_reg = lm( data = PS3Q2_data, y ~ x1 + x2 + x3 )
summary( PS3Q2_reg )
## 
## Call:
## lm(formula = y ~ x1 + x2 + x3, data = PS3Q2_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -32.543  -2.827   0.024   3.518  19.846 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.78915    0.44450   1.775   0.0771 .  
## x1          -0.87944    0.40636  -2.164   0.0314 *  
## x2           0.98717    0.23097   4.274 2.75e-05 ***
## x3           0.89088    0.09544   9.334  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.01 on 246 degrees of freedom
## Multiple R-squared:  0.294,  Adjusted R-squared:  0.2854 
## F-statistic: 34.16 on 3 and 246 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># autogen regression tables
library(stargazer)</code></pre>
<blockquote>
<p>PS3.Q2<br />
2. Compute the correlation matrix of the regressors to see if there is any sign of multi-collinearity.</p>
</blockquote>
<pre class="r"><code>X.mat  = cbind( PS3Q2_data$x1,PS3Q2_data$x2,PS3Q2_data$x3 )
cor( X.mat )
##            [,1]        [,2]        [,3]
## [1,] 1.00000000  0.07205362  0.07434651
## [2,] 0.07205362  1.00000000 -0.05255600
## [3,] 0.07434651 -0.05255600  1.00000000</code></pre>
<p>There is no strong sign of multicollinarity.</p>
<blockquote>
<p>PS3.Q2<br />
3. Generate scatter plots of (i) the residuals and the fitted values and (ii) the residuals and each individual regressor.<br />
Are there any visual signs of heteroskedasticity?</p>
</blockquote>
<pre class="r"><code>PS3Q2_res = PS3Q2_reg$res
PS3Q2_fit = PS3Q2_reg$fit
par( mfrow = c( 2, 2 ))
plot( PS3Q2_fit, PS3Q2_res )
plot( PS3Q2_data$x1, PS3Q2_res)
plot( PS3Q2_data$x2, PS3Q2_res)
plot( PS3Q2_data$x3, PS3Q2_res)</code></pre>
<p><img src="/post/2019-01-12-notes-stat-practice_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p><span class="math inline">\(x_2\)</span> may have some heteroskedasticity, cannot be sure.</p>
<blockquote>
<p>PS3.Q2<br />
4. In order to investigate more formally whether the residual variances are constant, run heteroskedasticity OLS regressions of the following type<br />
<span class="math display">\[e^2_i =α_1+α_2x^2_{j,i}+v_i\]</span><br />
where <span class="math inline">\(j = 2,3,4\)</span>.<br />
Test in each of these simple regressions <span class="math inline">\(H_0 : α_2 = 0\)</span>.<br />
Find an appropriate alternative hypothesis.<br />
Why we can do these simple regressions using a single regressor safely? Or should we worry about an omitted variable bias here?</p>
</blockquote>
<p>Since variance is always positive, <span class="math inline">\(H_1: α_2 &gt; 0\)</span></p>
<pre class="r"><code># calculate critical value
qnorm(0.95)

# simple regressions
PS3Q2_reg_res = lm( I(PS3Q2_res^2) ~ I(PS3Q2_data$x1^2) )
summary( PS3Q2_reg_res )

PS3Q2_reg_res = lm( I(PS3Q2_res^2) ~ I(PS3Q2_data$x2^2) )
summary( PS3Q2_reg_res )

PS3Q2_reg_res = lm( I(PS3Q2_res^2) ~ I(PS3Q2_data$x3^2) )
summary( PS3Q2_reg_res )</code></pre>
<p>Note: <span class="math inline">\(x\)</span>’s are not correlated <span class="math inline">\(\not\implies\)</span> transformed <span class="math inline">\(x\)</span>’s are not correlated<br />
Check for omitted variable bias for transformed <span class="math inline">\(x\)</span>’s:</p>
<pre class="r"><code># sth on multicollinearity in the heterosked regressions
X2.mat = cbind( PS3Q2_data$x1^2, PS3Q2_data$x2^2, PS3Q2_data$x3^2 )
cor(X2.mat)
##             [,1]        [,2]        [,3]
## [1,]  1.00000000 -0.01370166 -0.07887687
## [2,] -0.01370166  1.00000000 -0.14747583
## [3,] -0.07887687 -0.14747583  1.00000000</code></pre>
<p>There is no strong sign of multicollinarity. Therefore, it is safe to use simple regressions.</p>
<blockquote>
<p>PS3.Q2<br />
5. Collect the significant regressors from the three simple regressions to estimated a joint model containing all significant regressors simultaneously.<br />
Such an approach would be called specific-to-general. What is your chosen specification?</p>
</blockquote>
<pre class="r"><code>PS3Q2_reg_res = lm( I(PS3Q2_res^2) ~ I(PS3Q2_data$x1^2) + I(PS3Q2_data$x2^2) )
summary( PS3Q2_reg_res )
## 
## Call:
## lm(formula = I(PS3Q2_res^2) ~ I(PS3Q2_data$x1^2) + I(PS3Q2_data$x2^2))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -206.74  -34.21  -12.90    3.27  901.68 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)           6.868      8.366   0.821   0.4125    
## I(PS3Q2_data$x1^2)    7.657      3.482   2.199   0.0288 *  
## I(PS3Q2_data$x2^2)    8.680      1.160   7.480  1.3e-12 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 91.24 on 247 degrees of freedom
## Multiple R-squared:  0.1964, Adjusted R-squared:  0.1899 
## F-statistic: 30.18 on 2 and 247 DF,  p-value: 1.884e-12</code></pre>
<blockquote>
<p>PS3.Q2<br />
6. In contrast to specific-to-general, consider the reverse idea and start with a general model:<br />
<span class="math display">\[ e_{2i} = α_1 + α_2 x^2_{2i} + α_3 x^2_{3i} + α_4 x^2_{4i} + v_i \]</span><br />
Eliminate the most insignificant regressor (only one in each step) and re-estimate the restricted model.<br />
Continue until all contained regressors are significant. What is your chosen specification (model)?<br />
Compare the adjusted R2 values across all specifications you have estimated. Which is the one you would choose on the basis of the adjusted R2 measure?</p>
</blockquote>
<pre class="r"><code># using three regressors
PS3Q2_reg_res = lm( I(PS3Q2_res^2) ~ I(PS3Q2_data$x1^2) + I(PS3Q2_data$x2^2) + I(PS3Q2_data$x3^2) )
summary(PS3Q2_reg_res)
## 
## Call:
## lm(formula = I(PS3Q2_res^2) ~ I(PS3Q2_data$x1^2) + I(PS3Q2_data$x2^2) + 
##     I(PS3Q2_data$x3^2))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -206.76  -34.46  -12.26    3.08  901.97 
## 
## Coefficients:
##                    Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         7.18421    9.85254   0.729    0.467    
## I(PS3Q2_data$x1^2)  7.63937    3.50095   2.182    0.030 *  
## I(PS3Q2_data$x2^2)  8.66901    1.17582   7.373 2.54e-12 ***
## I(PS3Q2_data$x3^2) -0.01168    0.19118  -0.061    0.951    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 91.42 on 246 degrees of freedom
## Multiple R-squared:  0.1964, Adjusted R-squared:  0.1866 
## F-statistic: 20.04 on 3 and 246 DF,  p-value: 1.186e-11</code></pre>
<p>We reach the same conclusion as in the spscific to general approach.</p>
<blockquote>
<p>PS3.Q2<br />
7. Plot the estimated <span class="math inline">\(h_i^2\)</span> values. Are there any signs of heteroskedasticity?<br />
Transform the original regression model from (1) to carry out a feasible GLS regression.<br />
Compare the FGLS regression results to the OLS results.</p>
</blockquote>
<pre class="r"><code># find the estimated h values
h.estim = PS3Q2_reg_res$fit
# ts.plot(h.estim)

# fit
ts.plot( PS3Q2_res^2 )
lines( h.estim, col = 2 )</code></pre>
<p><img src="/post/2019-01-12-notes-stat-practice_files/figure-html/unnamed-chunk-12-1.png" width="672" />
Clearly there is heteroskedasticity.</p>
<pre class="r"><code># FGLS transform
y.star = PS3Q2_data$y / sqrt(h.estim)
x1.star = PS3Q2_data$x1 / sqrt(h.estim)
x2.star = PS3Q2_data$x2 / sqrt(h.estim)
x3.star = PS3Q2_data$x3 / sqrt(h.estim)

PS3Q2_reg_star = lm(y.star ~ x1.star + x2.star + x3.star)
summary( PS3Q2_reg_star )
## 
## Call:
## lm(formula = y.star ~ x1.star + x2.star + x3.star)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.86902 -0.52308 -0.00906  0.65934  2.04890 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.16371    0.05991   2.733 0.006741 ** 
## x1.star     -1.00224    0.34994  -2.864 0.004544 ** 
## x2.star      0.94319    0.25081   3.761 0.000212 ***
## x3.star      0.81718    0.05689  14.363  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9471 on 246 degrees of freedom
## Multiple R-squared:  0.4721, Adjusted R-squared:  0.4656 
## F-statistic: 73.33 on 3 and 246 DF,  p-value: &lt; 2.2e-16</code></pre>
<blockquote>
<p>PS3.Q2<br />
8. Apply White standard errors to the OLS regression from (1).<br />
Recompute the t-values and compare the results. What do you find?</p>
</blockquote>
<pre class="r"><code># load sandwich package
library(sandwich)

# apply robust standard errors to OLS regression
vcovHC( PS3Q2_reg )

# round standard deviations
round( sqrt( diag( vcovHC( PS3Q2_reg ))), 3 )

# call summary again
summary( PS3Q2_reg )

# compare:
# summary(reg.star)

# t-ratios
round( summary(PS3Q2_reg)$coef[,1] / sqrt(diag(vcovHC(PS3Q2_reg))), 3 )</code></pre>
<blockquote>
<p>PS3.Q2<br />
True model used in data generation:</p>
</blockquote>
<pre class="r"><code>sd.eps = 1
sd.x1 = 1
sd.x2 = 2
sd.x3 = 5

xi = rnorm( N, 0, sd.xi )
h = 1 + 10*x1^2 + 10*x2^2
y = 1 + 0.8*x2 +0.8*x3 + sqrt(h) * rnorm(N, 0, sd.eps)</code></pre>
</div>
<div id="weird-model" class="section level3">
<h3>Weird Model</h3>
<blockquote>
<p>PS4.Q1<br />
Consider the nonlinear consumption function:<br />
<span class="math display">\[c_t = α + γ y_t^δ\]</span><br />
where <span class="math inline">\(c_t\)</span> is real consumption and <span class="math inline">\(y_t\)</span> real income. <span class="math inline">\(α ≥ 0\)</span>, <span class="math inline">\(γ &gt; 0\)</span> and <span class="math inline">\(δ &gt; 0\)</span>.<br />
1. Can this function be linearized?<br />
2. Under which parameter restriction is the function linear?<br />
3. Could you estimate the parameters <span class="math inline">\(α, γ, δ\)</span> via OLS directly?</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>No.<br />
<span class="math inline">\(\log(c_t) = \log(α + γ y_t^δ)\)</span><br />
</li>
<li><span class="math inline">\(δ=1\)</span> or <span class="math inline">\(α = 0\)</span><br />
</li>
<li>No. </li>
</ol>
<blockquote>
<p>PS4.Q1<br />
Consider the nonlinear consumption function:<br />
<span class="math display">\[c_t = α + γ y_t^δ\]</span><br />
4. An econometrician who favors OLS wants to estimate the parameters in a reasonable way.<br />
The following possibilities are considered. Comment on each of the possibilities (provide arguments), figure out when they could work (and when not), and choose the best option.<br />
i) <span class="math inline">\(c_t = β_1 + β_2 y_t + ε_t\)</span> s.t. hopefully <span class="math inline">\(b_1 = \hat{a}\)</span> and <span class="math inline">\(b_2 = \hat{γ}\)</span>.<br />
ii) <span class="math inline">\(\ln(c_t) = β_1 + β_2 \ln(y_t) + εt\)</span> s.t. hopefully <span class="math inline">\(b_1 = \ln(\hat{γ})\)</span> and <span class="math inline">\(b_2 = \hat{δ}\)</span>.<br />
iii) <span class="math inline">\(c_t = β_1 + β_2y_t^2 + ε_t\)</span> s.t. hopefully <span class="math inline">\(b1 = \hat{α}\)</span>, <span class="math inline">\(b_2 = \hat{γ}\)</span> and <span class="math inline">\(δ = 2\)</span>.<br />
iv) <span class="math inline">\(c_t = β + β \sqrt{y_t} + ε\)</span> s.t. hopefully <span class="math inline">\(b = \hat{α}\)</span>, <span class="math inline">\(b = \hat{γ}\)</span> and <span class="math inline">\(\hat{δ} = \frac{1}{2}\)</span>.<br />
v) <span class="math inline">\(c_t =β_1+β_2 y_t+ε_t\)</span> where <span class="math inline">\(y_t =y_t a\)</span> with <span class="math inline">\(a\in A =[\underline{a}, \bar{a}]\)</span> and <span class="math inline">\(0&lt;\underline{a}&lt;\bar{a}&lt;\infty\)</span>.<br />
~~ The regression is considered for many possible values of <span class="math inline">\(a\)</span>, hoping that <span class="math inline">\(δ\in A\)</span>.<br />
~~ So hopefully <span class="math inline">\(δ = \text{argmin}_{a\in A} s^2(a)\)</span>, and correspondingly, <span class="math inline">\(b_1 = α\)</span> and <span class="math inline">\(b_2 = γ\)</span> from the regression which provides the minimal value for <span class="math inline">\(s^2(a)\)</span>.</p>
</blockquote>
<pre class="r"><code># number of obs
PS4Q1_N = 250

# log-normally distributed real income
PS4Q1_y = rlnorm( PS4Q1_N, 3, 1 )

# parameters
PS4Q1_beta1 = 1
PS4Q1_beta2 = 0.6
PS4Q1_beta3 = 0.8

# real consumption according to the nonlinear consumption function + random error
PS4Q1_c = PS4Q1_beta1 + PS4Q1_beta2 * PS4Q1_y ^(PS4Q1_beta3) + rnorm( PS4Q1_N, 0, 0.1 )
plot( x = PS4Q1_y, y = PS4Q1_c )</code></pre>
<p><img src="/post/2019-01-12-notes-stat-practice_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(c_t = β_1 + β_2 y_t + ε_t\)</span> s.t. hopefully <span class="math inline">\(b_1 = \hat{a}\)</span> and <span class="math inline">\(b_2 = \hat{γ}\)</span>.<br />
Works when <span class="math inline">\(δ = 1\)</span>:</li>
</ol>
<pre class="r"><code>PS4Q1_reg1 = lm( PS4Q1_c ~ PS4Q1_y )
summary( PS4Q1_reg1 )</code></pre>
<ol start="2" style="list-style-type: lower-roman">
<li><span class="math inline">\(\ln(c_t) = β_1 + β_2 \ln(y_t) + ε_t\)</span> s.t. hopefully <span class="math inline">\(b_1 = \ln(\hat{γ})\)</span> and <span class="math inline">\(b_2 = \hat{δ}\)</span><br />
Works when <span class="math inline">\(α = 0\)</span>, <span class="math inline">\(\hat{γ} = \exp(b_1)\)</span> and <span class="math inline">\(\hat{δ} = b_2\)</span><br />
(<strong>the delta method</strong>)</li>
</ol>
<pre class="r"><code>PS4Q1_reg2 = lm( log(PS4Q1_c) ~ log(PS4Q1_y) )
summary( PS4Q1_reg2 )
exp( PS4Q1_reg2$coef[1] )</code></pre>
<ol start="3" style="list-style-type: lower-roman">
<li><span class="math inline">\(c_t = β_1 + β_2y_t^2 + ε_t\)</span> s.t. hopefully <span class="math inline">\(b_1 = \hat{α}\)</span> and <span class="math inline">\(b_2 = \hat{γ}\)</span> and <span class="math inline">\(\hat{δ} = 2\)</span>.<br />
Works when <span class="math inline">\(δ = 2\)</span></li>
</ol>
<pre class="r"><code>PS4Q1_reg3 = lm( PS4Q1_c ~ I(PS4Q1_y^2) )
summary( PS4Q1_reg3 )</code></pre>
<ol start="4" style="list-style-type: lower-roman">
<li><span class="math inline">\(c = β_1 + β_2 \sqrt{y} + ε\)</span> s.t. hopefully <span class="math inline">\(b_1 = \hat{α}\)</span> and <span class="math inline">\(b_2 = \hat{γ}\)</span> and <span class="math inline">\(\hat{δ} = 1/2\)</span>.<br />
Works when <span class="math inline">\(δ = \frac{1}{2}\)</span></li>
</ol>
<pre class="r"><code>PS4Q1_reg4 = lm( PS4Q1_c ~ I(sqrt(PS4Q1_y)) )
summary( PS4Q1_reg4 )</code></pre>
<ol start="22" style="list-style-type: lower-alpha">
<li><span class="math inline">\(c_t =β_1+β_2\tilde{y}_t+ε_t\)</span> where <span class="math inline">\(\tilde{y}_t =y_t^a\)</span> with <span class="math inline">\(a\in A=[\underline{a},\bar{a}]\)</span> and <span class="math inline">\(0 &lt; \underline{a} &lt; \bar{a} \leq\infty\)</span><br />
Works without extra assumptions.</li>
</ol>
<pre class="r"><code>PS4Q1_beta3_fix = 1.2
PS4Q1_reg5 = lm( PS4Q1_c ~ I( PS4Q1_y^PS4Q1_beta3_fix ))
summary( PS4Q1_reg5 )
## 
## Call:
## lm(formula = PS4Q1_c ~ I(PS4Q1_y^PS4Q1_beta3_fix))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.3068  -1.3838  -0.1457   1.4764   3.6828 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                 5.11312    0.15853   32.25   &lt;2e-16 ***
## I(PS4Q1_y^PS4Q1_beta3_fix)  0.06473    0.00116   55.81   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.183 on 248 degrees of freedom
## Multiple R-squared:  0.9262, Adjusted R-squared:  0.9259 
## F-statistic:  3115 on 1 and 248 DF,  p-value: &lt; 2.2e-16

PS4Q1_beta3_fix = 1.1
PS4Q1_reg5 = lm( PS4Q1_c ~ I( PS4Q1_y^PS4Q1_beta3_fix ))
summary( PS4Q1_reg5 )
## 
## Call:
## lm(formula = PS4Q1_c ~ I(PS4Q1_y^PS4Q1_beta3_fix))
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -13.9897  -1.0435  -0.0767   1.2072   2.6159 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                4.344728   0.127380   34.11   &lt;2e-16 ***
## I(PS4Q1_y^PS4Q1_beta3_fix) 0.114275   0.001557   73.42   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.686 on 248 degrees of freedom
## Multiple R-squared:  0.956,  Adjusted R-squared:  0.9558 
## F-statistic:  5390 on 1 and 248 DF,  p-value: &lt; 2.2e-16

PS4Q1_beta3_fix = 1
PS4Q1_reg5 = lm( PS4Q1_c ~ I( PS4Q1_y^PS4Q1_beta3_fix ))
summary( PS4Q1_reg5 )
## 
## Call:
## lm(formula = PS4Q1_c ~ I(PS4Q1_y^PS4Q1_beta3_fix))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -9.9524 -0.6944 -0.0240  0.8558  1.6932 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                3.427636   0.091815   37.33   &lt;2e-16 ***
## I(PS4Q1_y^PS4Q1_beta3_fix) 0.200109   0.001845  108.47   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.155 on 248 degrees of freedom
## Multiple R-squared:  0.9794, Adjusted R-squared:  0.9793 
## F-statistic: 1.176e+04 on 1 and 248 DF,  p-value: &lt; 2.2e-16

PS4Q1_beta3_fix = 0.9
PS4Q1_reg5 = lm( PS4Q1_c ~ I( PS4Q1_y^PS4Q1_beta3_fix ))
summary( PS4Q1_reg5 )
## 
## Call:
## lm(formula = PS4Q1_c ~ I(PS4Q1_y^PS4Q1_beta3_fix))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2415 -0.3319  0.0162  0.4366  0.8828 
## 
## Coefficients:
##                            Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                2.330697   0.050714   45.96   &lt;2e-16 ***
## I(PS4Q1_y^PS4Q1_beta3_fix) 0.347620   0.001647  211.08   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5981 on 248 degrees of freedom
## Multiple R-squared:  0.9945, Adjusted R-squared:  0.9944 
## F-statistic: 4.455e+04 on 1 and 248 DF,  p-value: &lt; 2.2e-16

PS4Q1_beta3_fix = 0.8
PS4Q1_reg5 = lm( PS4Q1_c ~ I( PS4Q1_y^PS4Q1_beta3_fix ))
summary( PS4Q1_reg5 )
## 
## Call:
## lm(formula = PS4Q1_c ~ I(PS4Q1_y^PS4Q1_beta3_fix))
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.306497 -0.064712 -0.001418  0.064237  0.310347 
## 
## Coefficients:
##                             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                1.0087893  0.0095558   105.6   &lt;2e-16 ***
## I(PS4Q1_y^PS4Q1_beta3_fix) 0.5996132  0.0004922  1218.3   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.1039 on 248 degrees of freedom
## Multiple R-squared:  0.9998, Adjusted R-squared:  0.9998 
## F-statistic: 1.484e+06 on 1 and 248 DF,  p-value: &lt; 2.2e-16</code></pre>
<blockquote>
<p>PS4.Q1<br />
5. Use the US data from 1960:Q1 to 2009:Q4 with N = 200 observations available in the data set usdata.txt to estimate the parameters empirically.<br />
In the data set, the first column is real consumption, and the second is real disposable income.<br />
Comment on your findings and summarize them in a regression output table.<br />
6. Following (5), test the restriction <span class="math inline">\(δ = 1\)</span></p>
</blockquote>
<ol start="5" style="list-style-type: decimal">
<li></li>
</ol>
<pre class="r"><code>PS4Q1_data = read.table( file = &quot;usdata.txt&quot; )
PS4Q1_c = PS4Q1_data[,1]
PS4Q1_y = PS4Q1_data[,2]
PS4Q1_N = length( PS4Q1_y )

# note the data is trending
ts.plot( PS4Q1_data )

# using (v) from (4)
PS4Q1_beta_fix_vec = seq( 0.01, 2, 0.01 )
PS4Q1_reg5_coef = matrix( NA, length(PS4Q1_beta_fix_vec), 2 )
PS4Q1_s2 = matrix( NA, length(PS4Q1_beta_fix_vec), 1 )

for( i in 1:length( PS4Q1_beta_fix_vec )){
    PS4Q1_beta3_fix = PS4Q1_beta_fix_vec[i]
    PS4Q1_reg5 = lm( c ~ I( PS4Q1_y ^ PS4Q1_beta3_fix ))
    PS4Q1_reg5_coef[i,] = PS4Q1_reg5$coef
    PS4Q1_s2[i,1] = summary( PS4Q1_reg5 )$sigma^2 
}

# plot objective function
plot( PS4Q1_s2 )

# find the arg min and run this particular regression
which.min( PS4Q1_s2 )

PS4Q1_i = which.min(s2)
( PS4Q1_beta3_fix = PS4Q1_beta_fix_vec[i] )

PS4Q1_reg5 = lm( PS4Q1_c ~ I( PS4Q1_y^PS4Q1_beta3_fix ))
summary( PS4Q1_reg5 )
round( PS4Q1_reg5$coef, 2 )</code></pre>
<ol start="6" style="list-style-type: decimal">
<li></li>
</ol>
<pre class="r"><code># restricted model
PS4Q1_reg5_r = lm( formula( PS4Q1_c ~ PS4Q1_y))
summary( PS4Q1_reg5r )</code></pre>
<blockquote>
<p>PS4.Q1<br />
7. Plot the residuals and their autocorrelation function.<br />
Find the empirical first-order autocorrelation coefficient.<br />
Use the Box-Pierce test with one and five lags.<br />
State the null and the alternative hypotheses in both cases.<br />
Compute the test statistics and compare them to the appropriate critical values. Interpret your findings briefly.</p>
</blockquote>
<pre class="r"><code># residuals of unrestricted model
PS4Q1_e = PS4Q1_reg5$res
par( mfrow = PS4Q1_c( 1, 2 ))

# plot
ts.plot( PS4Q1_e )
abline( h = 0 )

# note that the auto-correlations are getting smaller
## plot
acf( PS4Q1_e )
## table
acf( PS4Q1_e, plot = FALSE )</code></pre>
<p>Box-Pierce test with one and five lags:</p>
<pre class="r"><code>PS4Q1_rho1 = acf( PS4Q1_e, plot=FALSE)$acf[2]
PS4Q1_Q1 = PS4Q1_N * PS4Q1_rho1^2
qchisq( 0.95, 1 )
qchisq( 0.99, 1 )

PS4Q1_Q5 = PS4Q1_N * sum( I( acf( PS4Q1_e, plot = FALSE)$acf[2:6]^2 ))

qchisq( 0.95, 5 )

# choose lag
log( PS4Q1_N )</code></pre>
</div>
</div>
<div id="r-workout" class="section level2">
<h2>R Workout</h2>
<div id="data-generation" class="section level3">
<h3>Data Generation</h3>
<blockquote>
<p>PS1.Q3<br />
Conduct Monte Carlo simulations.<br />
The idea is to take a data generating process (DGP) with an assumed parameterization, simulate data from it, and estimate the parameters by OLS.<br />
1. The first DGP is given by <span class="math inline">\(y_t = β_1 + β_2 x_t + ε_t\)</span> where <span class="math inline">\(ε_t ∼ \mathcal{N} (0, 1)\)</span>.<br />
~~ The exogenous regressor <span class="math inline">\(x_t\)</span> is set to be always drawn from <span class="math inline">\(\mathcal{N} (0, 1)\)</span>.<br />
~~ Given this regressor, simulate 10000 samples for <span class="math inline">\(y\)</span> of sizes <span class="math inline">\(n = \{10, 50, 100, 500\}\)</span>, using <span class="math inline">\(β_1 = 1\)</span> and <span class="math inline">\(β_2 = 2\)</span>.<br />
2. For each of these samples, compute the OLS estimator <span class="math inline">\(b_2\)</span>.<br />
3. Save the result in a matrix and plot it as a smoothed histogram (i.e. kernel density estimator).<br />
4. Do the same for the DGP where <span class="math inline">\(x_t\)</span> and <span class="math inline">\(ε_t\)</span> are drawn individually from a <span class="math inline">\(t(5)\)</span>-distribution instead.<br />
5. What do you find? Are the assumptions A1–A4 are satisfied?</p>
</blockquote>
</div>
<div id="ols-regression" class="section level3">
<h3>OLS Regression</h3>
<blockquote>
<p>PS2.Q4<br />
For N = 500 observations, generate data from the linear regression model as follows:<br />
<span class="math display">\[y_t = 0.5x_t + ε_t\]</span><br />
with <span class="math inline">\(x_t ∼^{iid} N(0,σ_x^2)\)</span> independent of <span class="math inline">\(ε_t ∼^{iid} N(0,σ_ε^2)\)</span>.<br />
Fix <span class="math inline">\(σ_ε = 4\)</span> and choose different values for <span class="math inline">\(σ_x\)</span>, say 0.01 and 100.<br />
Compute the OLS estimator for the true model (excluding the intercept) and its standard deviation.<br />
1. Verify that the standard deviation can be computed as <span class="math inline">\(\sqrt{ \frac{\sum_t e_t^2}{N-1} \cdot\frac{1}{\sum_t x_t^2}}\)</span>, where <span class="math inline">\(e_t\)</span> denotes the OLS residual.<br />
2. Generate a scatter plot for different values of <span class="math inline">\(σ_x\)</span> to explain why an increasing variance of the regressor makes the OLS estimator more precise.<br />
3. Vary also the sample size and the variance of the errors and repeat (2). What do you find?</p>
</blockquote>
<pre class="r"><code># normal distribution
?rnorm
sd = 100

y &lt;- rnorm( 100, mean = 0, sd)
x &lt;- rnorm( 100, mean = 0, sd)

lm( formula = y ~ x - 1)

plot( x, y )</code></pre>
</div>
</div>

    </div>
  </article>
  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  (function() { 
  var d = document, s = d.createElement('script');
  s.src = 'https://loikein-github.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>

</main>

<footer class="footer">
<ul class="footer-links">
<li>
  <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
</li>
<li>Theme by loikein with love</li>
</ul>

</footer>

<script async src="/js/jump-after-toc.js"></script>






<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/applescript.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/vim.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



<script src="/js/math-code.js"></script>
<script async defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=TeX-MML-AM_CHTML"></script>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-143089736-1', 'auto');
  
  ga('send', 'pageview');
}
</script>

</body>
</html>
