<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.59.0" />

<title>Notes on Econometric Theory - loikein&#39;s notes</title>
<meta property="og:title" content="Notes on Econometric Theory - loikein&#39;s notes">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-143089736-1', 'auto');
  
  ga('send', 'pageview');
}
</script>


  


<link rel="icon" type="image/x-icon" href="/favicon.ico" />


<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">
<link rel="stylesheet" href="/css/clumsy-toc.css" media="all">




  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/post/" class="nav-logo">
    <img src="/images/loikein-logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>
  <ul class="nav-links">
    
    <li><a href="/post/">Posts</a></li>
    
    <li><a href="/tags/">Tags</a></li>
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/loikein/loikein.github.io">GitHub</a></li>
    
  </ul>
</nav>
      </header>
<main class="content" role="main">
  <article class="article">
    
    <span class="article-duration">18 min read</span>
    
    <h1 class="article-title">Notes on Econometric Theory</h1>
    
    <span class="article-date">2019/10/08</span>
    
    
    
    <span class="tags">
    
    
    Tags:
    
    <a href='/tags/notes'>notes</a>
    
    <a href='/tags/stat'>stat</a>
    
    
    
    </span>
    
    <div class="article-content">
      

<div id="TOC">
<ul>
<li><a href="#literature">Literature</a></li>
<li><a href="#probability-measures">Probability Measures</a><ul>
<li><a href="#borel-field">σ- &amp; Borel Field</a></li>
<li><a href="#distribution-function">Distribution Function</a></li>
<li><a href="#extension-to-rk">Extension to R^k</a></li>
</ul></li>
<li><a href="#random-variables">Random Variables</a><ul>
<li><a href="#expectation">Expectation</a></li>
<li><a href="#variance">Variance</a></li>
<li><a href="#moments-of-rk">Moments of R^k</a></li>
</ul></li>
<li><a href="#stochastic-convergence">Stochastic Convergence</a><ul>
<li><a href="#convergence-in-expectations-almost-sure-convergence">Convergence in Expectations &amp; Almost Sure Convergence</a></li>
<li><a href="#convergence-in-probability">Convergence in Probability</a></li>
<li><a href="#convergence-in-probability-vs.-almost-convergence">Convergence in Probability vs. Almost Convergence</a></li>
<li><a href="#convergence-in-probability-vs.-covergence-in-p-th-mean">Convergence in Probability vs. Covergence in p-th Mean</a></li>
<li><a href="#convergence-in-distribution">Convergence in Distribution</a></li>
<li><a href="#convergence-in-distribution-vs.-convergence-in-probability">Convergence in Distribution vs. Convergence in Probability</a></li>
<li><a href="#central-limit-theorem">Central Limit Theorem</a></li>
<li><a href="#stochastic-boundedness-tightness">Stochastic Boundedness (Tightness)</a></li>
</ul></li>
</ul>
</div>

<!-- parametric estimation -->
<ul>
<li>Uppercase variables: random variables</li>
<li>Lowercase variables: deterministic variables</li>
</ul>
<div id="literature" class="section level2">
<h2>Literature</h2>
<p>Asymptotic Theory:</p>
<ul>
<li>Casella, Berger: Statistical Inference</li>
<li>Bruce Hansen: Econometrics (Lecture Notes), Ch5</li>
<li>Davidson: Econometric Theory</li>
<li>White: Asymptotic Theory for Econometricians</li>
</ul>
<p>M-, Z-, Extremum Estimators:</p>
<ul>
<li>?</li>
</ul>
<!-- 
## Asymptotic Statistics

- Observations: $(x_n) = x_1, x_2,\dots$
    - Assumption: $x_n\to x$
- Linear model: $y_i = x_i^T\cdot β + ε_i$, $i=1,\dots,m$
- Estimator: $\hat{β} = \hat{β}_n = \hat{β}_n\big((y_1, x_1),\dots,(y_n,x_n)\big)$
    - $(\hat{β}_n) = \hat{β}_1, \hat{β}_2$
    - Consistency: $\hat{β}_n\to β$

## M-Estimators, Z-Estimators, Extremum Estimators

- Assume $\{z_i: i=1,\dots,m\}\overset{iid}{\sim}P_{θ_0}$
    - $P$ is some distribution
    - $θ_0 = (μ_0, σ_0^2)$
- Linear model: $y_i = x_i^T\cdot θ_0 + ε_i$
    - Assume $E[ε_i\ |\ x_i] = 0\implies E[y_i\ |\ x_i] = x_i^T\cdot θ_0$
    - Theoretically, $θ_0 = \text{argmin}_{θ\in Θ}\ Q(θ)$
    - In practice, let $\hat{θ}_0 = \text{argmin}_{θ\in Θ}\ \hat{Q}_n(θ)$
    - Consistency: $\hat{θ}_n\overset{p}{\to} θ_0$
    - $\sqrt{m}(\hat{θ}_n - θ_0)\overset{d}{\to} \mathcal{N}(0,Σ)$
 -->
</div>
<div id="probability-measures" class="section level2">
<h2>Probability Measures</h2>
<ul>
<li><span class="math inline">\(Ω\)</span>: sample space, a set of possible outcomes of an experiment
<ul>
<li>Countable set: <span class="math inline">\(\| Ω \|\leq \mathbb{N}\)</span></li>
</ul></li>
<li><span class="math inline">\(A\subset Ω\)</span>: event, a collection of possible outcomes
<ul>
<li>Disjoint sets: <span class="math inline">\(A_i\cap A_j = \emptyset\)</span> for <span class="math inline">\(i\neq j\)</span></li>
</ul></li>
<li><span class="math inline">\(\mathcal{A}\)</span>: family of <span class="math inline">\(A\)</span></li>
<li>Step functions: <span class="math inline">\(F\)</span> is a step function with finitely or countably many jumps <span class="math inline">\(x_1&lt;x_2&lt;\dots\)</span> and jump heights <span class="math inline">\(b_1,b_2,\dots &gt;0\)</span></li>
</ul>
<blockquote>
<p>Def 1.1. Axioms of probability / Kolmogorov Axioms<br />
A probability measure <span class="math inline">\(P\)</span> on <span class="math inline">\((Ω,\mathcal{A})\)</span> is a mapping <span class="math inline">\(P:\mathcal{A}\to[0,1]\)</span> with the following properties:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(P(\emptyset)= 0\)</span></li>
<li><span class="math inline">\(P(Ω) =1\)</span></li>
<li><span class="math inline">\(P\big(\cup_{i=1}^{\infty} A_i\big) = \sum_{i=1}^{\infty} P(A_i)\)</span> for all <span class="math inline">\(A_i\in\mathcal{A}\)</span> that are disjoint</li>
</ol>
</blockquote>
<p>Remarks:</p>
<ol style="list-style-type: lower-alpha">
<li>Def 1.1. formally defines the probability <span class="math inline">\(P(A)\)</span> that event <span class="math inline">\(A\)</span> occurs</li>
<li>By (i), the probability that nothing happens is 0<br />
By (ii), the probability that something happens is 1</li>
<li>By (iii) <span class="math inline">\(\implies\)</span> finite additivity (σ-additivity) for disjoint <span class="math inline">\(A_i\)</span>’s:
<span class="math display">\[P\big(\cup_{i=1}^{I} A_i\big) = \sum_{i=1}^{I} P(A_i)\]</span></li>
<li>If <span class="math inline">\(Ω\)</span> is countable, we can always take <span class="math inline">\(\mathcal{A} = \mathcal{P}(Ω)\)</span> (power set)</li>
<li>If <span class="math inline">\(Ω\)</span> is uncountable, it is in general impossible to define <span class="math inline">\(P\)</span> for all <span class="math inline">\(A\subset Ω\)</span> s.t. (i) - (iii) are satisfied<br />
thus, we cannot set <span class="math inline">\(\mathcal{A} = \mathcal{P}(Ω)\)</span> in general, but have to restrict the class of possible events <span class="math inline">\(A\)</span></li>
</ol>
<blockquote>
<p>PS1.Q3<br />
Let <span class="math inline">\(Ω\)</span> be a non-empty set, <span class="math inline">\(\mathcal{A}\)</span> a σ-field on <span class="math inline">\(Ω\)</span> and <span class="math inline">\(A_1,\dots,A_n\in \mathcal{A}\)</span>.<br />
Show that a probability measure <span class="math inline">\(P\)</span> on <span class="math inline">\((Ω, \mathcal{A})\)</span> has the following properties:</p>
<ol style="list-style-type: lower-roman">
<li>Let <span class="math inline">\(A_1,\dots,A_n\)</span> be pairwise disjoint, then <span class="math inline">\(P(\cup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)\)</span></li>
<li><span class="math inline">\(P(A^c)=1−P(A)\)</span></li>
<li><span class="math inline">\(A_1\subset A_2\implies P(A_2\backslash A_1)=P(A_2)−P(A_1)\)</span></li>
<li><span class="math inline">\(A_1\subset A_2\implies P(A_1)\leq P(A_2)\)</span></li>
<li><span class="math inline">\(P(\cup_{i=1}^n A_i)\leq \sum_{i=1}^n P (A_i)\)</span></li>
<li><span class="math inline">\(P(A_1\cup A_2)=P(A_1)+P(A_2)−P(A_1\cup A_2)\)</span></li>
</ol>
</blockquote>
<ol style="list-style-type: lower-roman">
<li>Construct <span class="math inline">\(A_i = \emptyset\)</span> for all <span class="math inline">\(i\geq n+1\)</span>,
<span class="math display">\[\begin{aligned}P(\cup_{i=1}^n A_i) &amp;= P(\cup_{i=1}^{\infty} A_i) \\
&amp;= \sum_{i=1}^{\infty} P(A_i) \\
&amp;= \sum_{i=1}^n P(A_i) + \sum_{i=n+1}^{\infty} \underbrace{P(A_i)}_{=0} \\
&amp;= \sum_{i=1}^n P(A_i)\end{aligned}\]</span></li>
<li>For all <span class="math inline">\(A\in Ω\)</span>,
<span class="math display">\[\begin{aligned}P(Ω) = 1 &amp;= P(A\cup A^C) \\
&amp;= P(A) + P(A^C)\end{aligned}\]</span></li>
<li><span class="math inline">\(A_1\subset A_2\implies\)</span>
<span class="math display">\[\begin{aligned}A_2 &amp;= A_1\cup (A_2\backslash A_1) \\
P(A_2) &amp;= P(A_1) + P(A_2\backslash A_1) \end{aligned}\]</span></li>
<li>Following (iii), <span class="math inline">\(A_1\subset A_2\implies\)</span>
<span class="math display">\[\begin{aligned}P(A_1) &amp;= P(A_2) - P(A_2\backslash A_1) \\
&amp;\leq P(A_2)\end{aligned}\]</span></li>
<li>Way 1: By induction using (vi)<br />
Way 2: Let <span class="math inline">\(A_1,\dots,A_n\in\mathcal{A}\)</span>,<br />
Define <span class="math inline">\(B_1 = A_1\)</span>, <span class="math inline">\(B_2 = A_2\backslash A_1\)</span>, <span class="math inline">\(B_3 = A_3\backslash(A_1\cup A_2)\)</span>…<br />
Therefore, following (iv),
<span class="math display">\[\begin{aligned}P\big( \cup_{i=1}^{\infty} A_i) &amp;= P\big( \cup_{i=1}^{\infty} B_i) \\
&amp;= \sum_{i=1}^{\infty} \underbrace{P(B_i)}_{\leq P(A_i)} \\
&amp;\leq \sum_{i=1}^{\infty} P(A_i) \end{aligned}\]</span></li>
<li>If <span class="math inline">\(A_1\)</span>, <span class="math inline">\(A_2\)</span> are not disjoint,
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg} P(A_1)+P(A_2)−2P(A_1\cup A_2) \\
&amp;= P\big(A_1\backslash (A_1\cup A_2)\big) + P\big(A_2\backslash (A_1\cup A_2)\big) \\
&amp;= P\big[ \big(A_1\backslash (A_1\cup A_2)\big)\cup\big(A_2\backslash (A_1\cup A_2)\big)\big] \\
&amp;= P\big((A_1\cup A_2)\backslash (A_1\cap A_2) \big) \\
&amp;= P(A_1\cup A_2) - P(A_1\cap A_2)\end{aligned}\]</span></li>
</ol>
<blockquote>
<p>Example: Laplace distribution<br />
Let <span class="math inline">\(Ω\)</span>, <span class="math inline">\(\mathcal{A} = \mathcal{P}(Ω)\)</span> the class of all subsets of <span class="math inline">\(Ω\)</span>, then<br />
<span class="math display">\[P(A) := \frac{|A|}{|Ω|}\]</span></p>
</blockquote>
<p>Consider tossing a fair dice once, <span class="math inline">\(Ω = \{1,\dots,6\}\)</span>, then <span class="math inline">\(P(A=\{6\}) = \frac{1}{6}\)</span><br />
Consider tossing a fair dice twive, <span class="math inline">\(Ω = \{(1,1),\dots,(6,6)\}\)</span>, then <span class="math inline">\(P(A=\{(2,6), (3,5),(4,4)\}) = \frac{3}{36}\)</span></p>
<blockquote>
<p>Prop 1.2., PS1.Q4<br />
Let <span class="math inline">\(Ω\)</span> be countable, mapping <span class="math inline">\(p:Ω\to[0,1]\)</span> is <span class="math inline">\(\sum_{ω\in Ω}p(ω)=1\)</span><br />
Then <span class="math inline">\(P(A):= \sum_{ω\in A}p(ω)\)</span> for all <span class="math inline">\(A\in\mathcal{P}(Ω)\)</span> defines a probability measure on <span class="math inline">\(\mathcal{P}(Ω)\)</span></p>
</blockquote>
<p>Proof: check Kolmogorov Axioms</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(P(\emptyset) = \sum_{ω\in\emptyset} p(ω)= 0\)</span></li>
<li><span class="math inline">\(P(Ω) = \sum_{ω\in Ω} p(ω)= 1\)</span></li>
<li>Let <span class="math inline">\(A_1,A_2,\dots\)</span> be disjoint<br />
<span class="math inline">\(\implies\)</span> for any <span class="math inline">\(ω\in A_i\)</span>, <span class="math inline">\(ω\not\in A_j\)</span> for any <span class="math inline">\(j\neq i\)</span><br />
<span class="math display">\[\begin{aligned}P\big(\cup_{i=1}^{\infty} A_i\big) &amp;= \sum_{ω\in \cup_{i=1}^{\infty} A_i} p(ω) \\
&amp;= \sum_{i=1}^{\infty}\underbrace{\sum_{ω\in A_i} p(ω)}_{P(A_i)} \\
&amp;= \sum_{i=1}^{\infty} P(A_i)\end{aligned}\]</span></li>
</ol>
<blockquote>
<p>Example: Probability measure</p>
<ol style="list-style-type: decimal">
<li>Laplace distribution<br />
</li>
<li>Bernoulli distribution<br />
<span class="math inline">\(Ω = \{0,1\}\)</span>, <span class="math inline">\(p(1)=q\)</span> for some <span class="math inline">\(q\in[0,1]\)</span>, <span class="math inline">\(p(0)=1-q\)</span><br />
</li>
<li>Binomial distribution<br />
<span class="math inline">\(\mathcal{B}(m,p)\)</span>, <span class="math inline">\(m\geq 1\)</span>, <span class="math inline">\(0\leq q\leq 1\)</span><br />
<span class="math inline">\(Ω = \{0,\dots,m\}\)</span>, <span class="math inline">\(p(k)=(m, k)^T\cdot q^k\cdot(1-q)^{m-k}\)</span> for some <span class="math inline">\(q\in[0,1]\)</span>, <span class="math inline">\(p(0)=1-q\)</span><br />
<span class="math inline">\(\sum_{k=0}^{\infty} p(k)= \sum_{k=0}^{\infty} (m, k)^T\cdot q^k\cdot(1-q)^{m-k}= \big(q + (1-q)\big)^m =1\)</span></li>
<li>Poisson distribution<br />
<span class="math inline">\(Ω = \mathbb{N}_0\)</span>, <span class="math inline">\(p(k)=\frac{λ^k}{k!}\cdot l^{-λ}\)</span><br />
<span class="math inline">\(\sum_{k=0}^{\infty} p(k)=\Big(\sum_{k=0}^{\infty}\frac{λ^k}{k!}\Big)\cdot l^{-λ} = l^{λ}\cdot l^{-λ} =1\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>PS1.Q5<br />
Suppose that for a probability measure <span class="math inline">\(P\)</span> and a sequence of sets <span class="math inline">\(A_1,A_2,\dots\)</span>, it holds that <span class="math inline">\(P(A_i) = 0\)</span>.<br />
Which value has <span class="math inline">\(P\big(\sum_{i=1}^{\infty} A_i\big)\)</span>?</p>
</blockquote>
<p>Way 1:</p>
<ol style="list-style-type: lower-roman">
<li>For finite union, prove <span class="math inline">\(P\big(\sum_{i=1}^n A_i\big) = 0\)</span><br />
Following PS1.Q3 (vi), for <span class="math inline">\(n=2\)</span>,
<span class="math display">\[\begin{aligned}P(A_1\cup A_2) &amp;= \underbrace{P(A_1)}_{\geq 0} + \underbrace{P(A_2)}_{\geq 0} - P(A_1\cap A_2) \\
&amp;\geq 0 \\
P(A_1\cap A_2) &amp;= 0 \end{aligned}\]</span>
By induction on <span class="math inline">\(n\)</span>, it follows that <span class="math inline">\(P\big(\sum_{i=1}^n A_i\big) = 0\)</span></li>
<li>For countably infinite union, prove <span class="math inline">\(P\big(\sum_{i=1}^{\infty} A_i\big) = 0\)</span><br />
Following (i),
<span class="math display">\[P\big(\cup_{i=1}^{\infty} A_i\big) = P\big(\cup_{i=1}^{\infty} A_i \backslash \cup_{i=n+1}^n A_i\big) + \underbrace{P\big(\cup_{i=n+1}^n A_i\big)}_{=0}\]</span>
Therefore, set <span class="math inline">\(B_n = \cup_{i=1}^{\infty} A_i \backslash \cup_{i=n+1}^n A_i\)</span> is monotonicly decreasing
<span class="math display">\[\begin{aligned}\lim_{n\to\infty} B_n &amp;= P\big(\cap_{i=1}^{\infty} B_1\big) \\
&amp;= P\big(\cup_{i=1}^{\infty} A_i \backslash \cup_{i=n+1}^{\infty} A_i \big) \\
&amp;= P(\emptyset) \\&amp;= 0 \end{aligned}\]</span></li>
</ol>
<p>Way 2: follow PS1.Q3 (v) way 2,<br />
Define <span class="math inline">\(B_1 = A_1\)</span>, <span class="math inline">\(B_2 = A_2\backslash A_1\)</span>, <span class="math inline">\(B_3 = A_3\backslash(A_1\cup A_2)\)</span>…<br />
<span class="math display">\[\begin{aligned}P\big(\cup_{i=1}^{\infty} A_i\big) &amp;= P\big(\cup_{i=1}^{\infty} B_i\big) \\
&amp;= \sum_{i=1}^{\infty} \underbrace{P(B_i)}_{\leq P(A_i)} \\
&amp;\leq \sum_{i=1}^{\infty} P(A_i) \\
&amp;= 0 \end{aligned}\]</span></p>
<blockquote>
<p>Remark 1.1.d.<br />
If <span class="math inline">\(Ω\)</span> is uncountable, it is in general impossible to define <span class="math inline">\(P\)</span> for all <span class="math inline">\(A\subset Ω\)</span> s.t. Kolmogorov Axioms are satisfied</p>
</blockquote>
<p>Solution: σ- &amp; Borel Field</p>
<div id="borel-field" class="section level3">
<h3>σ- &amp; Borel Field</h3>
<blockquote>
<p>Def 1.3. σ-field (algebra)<br />
Define <span class="math inline">\(P\)</span> on <span class="math inline">\(\mathcal{A}\subset\mathcal{P}(Ω)\)</span><br />
If all <span class="math inline">\(A\in\mathcal{A}\)</span> have the following properties, then <span class="math inline">\(\mathcal{A}\)</span> is called a σ-algebra or a σ-field</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\emptyset\in\mathcal{A}\)</span></li>
<li><span class="math inline">\(A\in\mathcal{A}\implies A^C\in\mathcal{A}\)</span></li>
<li><span class="math inline">\(A_1,A_2,\dots\in\mathcal{A}\implies \cup_{i=1}^{\infty}A_i\in\mathcal{A}\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>PS1.Q2<br />
Let <span class="math inline">\(Ω\)</span> be a non-empty set and <span class="math inline">\(\mathcal{A}\)</span> a σ-field on <span class="math inline">\(Ω\)</span>. Show the following two statements:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Ω\in\mathcal{A}\)</span></li>
<li>If <span class="math inline">\(A_1, A_2\in\mathcal{A}\)</span>, then <span class="math inline">\(A_1\cap A_2\in\mathcal{A}\)</span></li>
</ol>
</blockquote>
<ol style="list-style-type: lower-roman">
<li>Since the properties of <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(\emptyset\in\mathcal{A}\)</span><br />
So <span class="math inline">\(\emptyset ^C = Ω\in\mathcal{A}\)</span></li>
<li>Since <span class="math inline">\(A_1\in\mathcal{A}\implies A_1^C\in\mathcal{A}\)</span><br />
And <span class="math inline">\(A_2\in\mathcal{A}\implies A_2^C\in\mathcal{A}\)</span><br />
Therefore, <span class="math inline">\((A_1^C)\cup(A_2^C)\in\mathcal{A}\)</span><br />
<span class="math inline">\(\implies \big(A_1^C)\cup(A_2^C)\big)^C = A_1\cap A_2\in\mathcal{A}\)</span></li>
</ol>
<blockquote>
<p>Def 1.4. Borel σ-field<br />
Let <span class="math inline">\(Ω=\mathbb{R}\)</span><br />
The σ-field <span class="math inline">\(\mathcal{B}\)</span> that contains all open intervals <span class="math inline">\((a,b)\)</span> for <span class="math inline">\(-\infty\leq a&lt;b\leq\infty\)</span> is called Borel-σ-field<br />
A set <span class="math inline">\(A\in\mathcal{B}\)</span> is called a Borel set</p>
</blockquote>
<p>Remark: <span class="math inline">\(\mathcal{B}\neq\mathcal{P}(Ω)\)</span></p>
<blockquote>
<p>Remark 1.5. Set<br />
<span class="math inline">\(\mathcal{A}_1=\{(a,b]:-\infty\leq a&lt;b&lt;\infty\}\)</span><br />
<span class="math inline">\(\mathcal{A}_2=\{[a,b):-\infty &lt; a&lt;b\leq\infty\}\)</span><br />
<span class="math inline">\(\mathcal{A}_3=\{[a,b]:-\infty &lt; a&lt;b&lt;\infty\}\)</span><br />
<span class="math inline">\(\mathcal{A}_4=\{(-\infty,b]:-\infty &lt;b&lt;\infty\}\)</span><br />
For <span class="math inline">\(j=1,\dots,4\)</span>, <span class="math inline">\(\mathcal{B}\)</span> is the smallest σ-field that contains <span class="math inline">\(\mathcal{A}_j\)</span></p>
</blockquote>
<p>Proof: exercise<br />
e.g. use <span class="math inline">\((a,b)=\cup_{m=1}^{\infty} \big[a+\frac{1}{m},b-\frac{1}{m}\big]\)</span></p>
</div>
<div id="distribution-function" class="section level3">
<h3>Distribution Function</h3>
<blockquote>
<p>Def 1.6. Distribution function<br />
Let <span class="math inline">\(P\)</span> be a probability measure on <span class="math inline">\(\mathbb{R},\mathcal{B}\)</span><br />
The distribution function <span class="math inline">\(F:\mathbb{R}\to[0,1]\)</span> is defined by setting<br />
<span class="math display">\[F(b) = P\big((-\infty,b]\big) \text{ for all } b\in\mathbb{R}\]</span></p>
</blockquote>
<p>Note: <span class="math inline">\(P\big((a,b]\big)=F(b)-F(a)\)</span></p>
<blockquote>
<p>Thm 1.7.<br />
Let <span class="math inline">\(F:\mathbb{R}\to\mathbb{R}\)</span> be a function with the following properties:</p>
<ol style="list-style-type: lower-roman">
<li>Non-decreasing: <span class="math inline">\(F(a)\leq F(b)\)</span> for <span class="math inline">\(a&lt;b\)</span></li>
<li>Continuous from above (the right): <span class="math inline">\(F(b_n)\to F(b)\)</span> for <span class="math inline">\(b_n\searrow b\)</span></li>
<li><span class="math inline">\(\lim_{x\to -\infty}F(x)=0\)</span>, <span class="math inline">\(\lim_{x\to\infty}F(x)=1\)</span></li>
</ol>
<p>Then there exists a unique probability measure <span class="math inline">\(P\)</span> on <span class="math inline">\((\mathbb{R},B)\)</span> s.t.
<span class="math display">\[F(b) = P\big((-\infty,b]\big) \text{ for all } b\in\mathbb{R}\]</span></p>
</blockquote>
<p>Remarks:</p>
<ol style="list-style-type: lower-alpha">
<li>This says that any function <span class="math inline">\(F\)</span> with the properties uniquely characterizes a probability measure on <span class="math inline">\((\mathbb{R},B)\)</span><br />
</li>
<li>The reverse holds true too, see Thm 1.8. below</li>
</ol>
<blockquote>
<p>Thm 1.8.<br />
Any distribution function <span class="math inline">\(F\)</span> satisfies properties (i)-(iii) in Thm 1.7.</p>
</blockquote>
<p>Remarks:</p>
<ol style="list-style-type: lower-alpha">
<li>A probability measure <span class="math inline">\(P\)</span> on <span class="math inline">\((\mathbb{R},B)\)</span> is uniquely determined by the corresponding distribution function <span class="math inline">\(F\)</span> with <span class="math inline">\(F(b) = P\big((-\infty,b]\big)\)</span></li>
<li>Class of distribution fuctions = class of functions with properties (i)-(iii)</li>
</ol>
<p>Proof: (i) and (iii): exercise<br />
(ii):<br />
FSOC assume <span class="math inline">\(F:\mathbb{R}\to\mathbb{R}\)</span> is a distribution function that is not continuous from the right<br />
<span class="math inline">\(\implies\)</span> exists a sequence <span class="math inline">\(\{b_n\}\)</span>, <span class="math inline">\(b_n\searrow b\)</span> monotonically for some <span class="math inline">\(b\in\mathbb{R}\)</span> s.t. <span class="math inline">\(F(b_n)\not\to F(b)\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}1-F(b_n) &amp;= 1-P\big((-\infty, b_n]\big)\\
&amp;= P\big((b_n,\infty)\big) \\
&amp;= P\Big[ (b_1,\infty)\cup\big(\cup_{k=2}^n (b_k,b_{k-1}]\big)\Big] \\
&amp;= P\big((b_1,\infty)\big) + \textstyle{\sum}_{k=2}^n P\big((b_k,b_{k-1}]\big) \\
:&amp;= p_n\end{aligned}\]</span>
It is obvious that <span class="math inline">\(p_n\)</span> is monotonically increasing and bounded,<br />
<span class="math inline">\(\implies\)</span>
<span class="math display">\[\begin{aligned}p_n &amp;\to p \\
&amp;= P\big((b_1,\infty)\big) + \sum_{k=2}^{\infty} P\big((b_k,b_{k-1}]\big)\\
&amp;= P\Big[(b_1,\infty)\cup\big(\cup_{k=2}^{\infty}(b_k,b_{k-1}] \big)\Big] \\
&amp;= P\big((b,\infty)\big) \\
&amp;= 1 - P\big((-\infty, b]\big) \\
&amp;= 1 - F(b) \end{aligned}\]</span>
therefore, <span class="math inline">\(1 - F(b_n) = p_n\to p = 1-F(b)\)</span><br />
<span class="math inline">\(\implies 1 - F(b_n)\to 1-F(b)\)</span>, contradiction</p>
<blockquote>
<p>Example: Step function
Let <span class="math inline">\(F\)</span> be a step function, <span class="math inline">\(P\)</span> be a probability function corresponds to <span class="math inline">\(F\)</span>,<br />
<span class="math inline">\(P\big(\{x_i\}\big) = b_i\)</span> with <span class="math inline">\(\sum_{i=1}^{\infty}b_i = 1\)</span><br />
Hence we can regard the sample space as being complete</p>
</blockquote>
<p>In this case, <span class="math inline">\(Ω = \{x_1,x_2,\dots\}\)</span></p>
<blockquote>
<p>Example: Absolutely continuous functions<br />
Let <span class="math inline">\(F\)</span> have the representation <span class="math inline">\(F(x) = \int_{-\infty}^x f(y)\, dy\)</span><br />
where <span class="math inline">\(f\)</span> is bounded &amp; integrable on any interval <span class="math inline">\([a,b]\)</span> and <span class="math inline">\(\lim_{a,b\to\infty} \int_a^b f(y)\, dy =1\)</span><br />
<span class="math inline">\(f\)</span> is called (probability) density function.<br />
Let <span class="math inline">\(P\)</span> be the corresponding probability measure, it holds that <span class="math inline">\(P(\{x\})=0\)</span> for all <span class="math inline">\(x\in\mathbb{R}\)</span></p>
</blockquote>
<p>Example:</p>
<ol style="list-style-type: decimal">
<li>Uniform distribution on <span class="math inline">\([a,b]\)</span><br />
<span class="math inline">\(f(x) = \frac{1}{b-a}\cdot 1\)</span>, <span class="math inline">\(x\in[a,b]\)</span><br />
then <span class="math inline">\(F(x)=\begin{cases}0 &amp; x\leq a \\ \frac{x-a}{b-a} &amp; a\leq x\leq b \\ 1 &amp; x\geq b \end{cases}\)</span><br />
</li>
<li>Normal distribution <span class="math inline">\(\mathcal{N}(μ,σ)\)</span>, <span class="math inline">\(μ\in\mathbb{R}\)</span>, <span class="math inline">\(σ&gt;0\)</span><br />
<span class="math inline">\(f_{μ,σ}(x) = \frac{1}{\sqrt{2πσ^2}}\cdot \exp\big(-\frac{1}{2}\cdot\frac{(x-μ)^2}{σ^2}\big)\)</span><br />
then <span class="math inline">\(F(x) = \int_{-\infty}^x f_{μ,σ}(y)\, dy\)</span></li>
</ol>
</div>
<div id="extension-to-rk" class="section level3">
<h3>Extension to R^k</h3>
<ul>
<li>Borel σ-field <span class="math inline">\(\mathcal{B}^k = \mathcal{B}\)</span> is defined as the smallest σ-field generated by the open cubes <span class="math inline">\((a_1,b_1)\times\dots\times (a_k,b_k)\)</span></li>
<li>Distribution function <span class="math inline">\(F:\mathbb{R}^k\to\mathbb{R}\)</span> corresponding to the probability measure <span class="math inline">\(P\)</span> is defined as
<span class="math display">\[F(b_1,\dots,b_k)=P\big((x_1,\dots,x_k) : x_1\leq b_1,\dots,x_k\leq b_k\big)\]</span></li>
<li>Absolute continuity: <span class="math inline">\(F\)</span> s.t. exists <span class="math inline">\(f:\mathbb{R}^k\to\mathbb{R}\)</span> s.t.
<span class="math display">\[F(b_1,\dots,b_k) = \int_{-\infty}^{b_1}\cdots\int_{-\infty}^{b_k}f(x_1,\dots,x_k)\, dx_1\dots dx_k\]</span></li>
</ul>
</div>
</div>
<div id="random-variables" class="section level2">
<h2>Random Variables</h2>
<ul>
<li><span class="math inline">\(\mathbb{R}^k\)</span>-valued random variable: <span class="math inline">\(X\)</span> is a random element of <span class="math inline">\(\mathbb{R}^k\)</span></li>
</ul>
<blockquote>
<p>Def 1.9. Random variable<br />
<span class="math inline">\(\mathbb{R}^k\)</span>-valued random variable <span class="math inline">\(X\)</span> is a function <span class="math inline">\(X:Ω\to\mathbb{R}^k\)</span>,<br />
where <span class="math inline">\((Ω,\mathcal{A}, P)\)</span> is a probability space, and <span class="math inline">\(X\)</span> has measurability (M):<br />
<span class="math inline">\(X^{-1}(B) = \{ω\in Ω : X(ω)\in B\}\)</span>, <span class="math inline">\(X^{-1}(B)\in\mathcal{A}\)</span> for all <span class="math inline">\(B\in\mathcal{B}^k\)</span></p>
</blockquote>
<blockquote>
<p>Def. Distribution of <span class="math inline">\(X\)</span><br />
Measurability allows to define a probability measure on <span class="math inline">\(\mathbb{R}^k\)</span> as follows:<br />
Define the probability that <span class="math inline">\(X\in B\)</span> by <span class="math inline">\(P^X(B) = Pr(X\in B)\)</span>, then<br />
<span class="math display">\[\begin{aligned}P^X(B) &amp;:= P\big(X^{-1}(B) \big) \\ &amp;\phantom{:}= P\big(\{ω:X(ω)\in B\} \big) \\ &amp;:= P(X\in B)\end{aligned}\]</span></p>
</blockquote>
<p>Remarks:</p>
<ul>
<li>Random variable: <span class="math inline">\(X: Ω\to\mathbb{R}\)</span></li>
<li>One can show that <span class="math inline">\(P^X\)</span> is a probability measure on <span class="math inline">\((\mathbb{R}^k, \mathcal{B}^k)\)</span></li>
<li><span class="math inline">\(X\)</span> is a deterministic function with (M)<br />
<span class="math inline">\(\implies\)</span> the probability structure on <span class="math inline">\(Ω\)</span> is also mapped appropreately to <span class="math inline">\(\mathbb{R}^k\)</span></li>
<li>Def. of a random variable as a deterministic function is parsimonious in the sense that it avoids introducing a new probability structure on <span class="math inline">\(\mathbb{R}^k\)</span></li>
</ul>
<blockquote>
<p>Example: Tossing a coin 3 times<br />
<span class="math display">\[Ω=\big\{ω =(ω_1, ω_2, ω_3) : ω_j\in\{0,1\}\big\}\]</span>
P: Laplace distribution on <span class="math inline">\(Ω\)</span>:
<span class="math display">\[P(A) = |A|\backslash |Ω| = |A|\backslash 8\]</span>
Define the random variable <span class="math inline">\(X:Ω\to\mathbb{R}\)</span>:
<span class="math display">\[ω =(ω_1, ω_2, ω_3) \mapsto \sum_{j=1}^3 ω_j\]</span>
therefore, <span class="math inline">\(X\in\{0,1,2,3\}\)</span><br />
<span class="math display">\[\begin{aligned}P^X\big(\{0\}\big) &amp;= P\big(\{ω\in Ω : X(ω) = 0\}\big) \\&amp;= P\big(\{0,0,0\}\big) \\&amp;=\frac{1}{8}\\
P^X\big(\{1\}\big) &amp;= P\big(\{ω\in Ω : X(ω) = 1\}\big) \\&amp;= P\big(\{1,0,0\},\{0,1,0\},\{0,0,1\}\big) \\&amp;=\frac{3}{8}\end{aligned}\]</span>
and so on</p>
</blockquote>
<blockquote>
<p>PS1.Q1<br />
Consider the experiment of tossing a dice three times.</p>
<ol style="list-style-type: lower-roman">
<li>Write down the sample space <span class="math inline">\(Ω\)</span> for this experiment.</li>
<li>Write down the event <span class="math inline">\(A_1\)</span> that “5” occurs exactly two times.</li>
<li>What is the probability of event <span class="math inline">\(A_1\)</span> assuming a Laplace distribution?</li>
</ol>
</blockquote>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Ω = \big\{(i,j,k) : i,j,k\in\{1,2,\dots,6\}\big\}\)</span></li>
<li><span class="math inline">\(A_1 = \{(5,5,i)\}\cup\{(5,i,5)\}\cup\{(i,5,5)\}\)</span> where <span class="math inline">\(i\in\{1,2,3,4,6\}\)</span></li>
<li><span class="math display">\[\begin{aligned}Pr(A_1) &amp;= |A|\backslash |Ω| \\
&amp;= \frac{15}{216}\end{aligned}\]</span></li>
</ol>
<div id="expectation" class="section level3">
<h3>Expectation</h3>
<blockquote>
<p>Def 1.10. Discrete Continuous<br />
Random variable <span class="math inline">\(X\)</span> is</p>
<ol style="list-style-type: lower-roman">
<li>Discrete if its distribution is discrete,<br />
i.e., if there exists countably many points <span class="math inline">\(x_1&lt;x_2&lt;\dots\)</span> s,t, <span class="math inline">\(P(X=x_i)=p_i\)</span>, and <span class="math inline">\(\sum_i p_i=1\)</span></li>
<li>Continuous if its distribution is absolutely continuous,<br />
i.e., if its distribution function <span class="math inline">\(F\)</span> can be written as <span class="math display">\[F(x)=\int_{-\infty}^{\infty}f(y)\, dy\]</span>
with some bounded, integrable density function <span class="math inline">\(f\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>Def 1.11. Mean Expectation<br />
The mean of <span class="math inline">\(X\)</span> is defined as</p>
<ol style="list-style-type: lower-roman">
<li>Provided that <span class="math inline">\(\sum_{i=1}^{\infty} p_i\cdot x_i&lt;\infty\)</span>,
<span class="math display">\[\begin{aligned}E[X] &amp;= \textstyle{\sum}_{i=1}^{\infty} p_i\cdot x_i\\ &amp;= \textstyle{\sum}_{i=1}^{\infty} P(X=x_i)\cdot x_i \end{aligned}\]</span></li>
<li>Provided that <span class="math inline">\(\int |x|\cdot f(x)\, dx&lt;\infty\)</span>,
<span class="math display">\[E[X] = \int_{-\infty}^{\infty} x\cdot f(x)\, dx\]</span></li>
</ol>
</blockquote>
<p>Note: the finite bind is there s.t. <span class="math inline">\(E[X]\)</span> is well-defined and finite</p>
<blockquote>
<p>Example</p>
<ol style="list-style-type: lower-alpha">
<li>Expected gain of roulette: bet 1 on “odd”<br />
<span class="math inline">\(Ω = \{0,1,2,\dots, 36\}\)</span><br />
<span class="math inline">\(P\)</span> Laplace distribution<br />
<span class="math inline">\(X(ω)=\begin{cases}1 &amp; \text{odd} \\ -1 \end{cases}\)</span> gain<br />
<span class="math display">\[\begin{aligned}P(X=1) &amp;= \frac{18}{37} \\ P(X=-1) &amp;= \frac{19}{37} \\ E[X] &amp;= 1\cdot\frac{18}{37} + (-1)\cdot\frac{19}{37} &amp;= -\frac{1}{37} \end{aligned}\]</span></li>
<li>Normal distribution<br />
<span class="math display">\[\begin{aligned}f(x) &amp;=\frac{1}{\sqrt{2πσ^2}}\cdot\exp\big(-\frac{(x-μ)^2}{2σ^2}\big) \\
E[X] &amp;= \int_{-\infty}^{\infty} x\cdot f(x)\, dx\\
&amp;= \int_{-\infty}^{\infty} (y+μ)\cdot\frac{1}{\sqrt{2πσ^2}}\cdot\exp\big(-\frac{y^2}{2σ^2}\big)\, dy \\
&amp;= \underbrace{\int_{-\infty}^{\infty} y\cdot\frac{1}{\sqrt{2πσ^2}}\cdot\exp\big(-\frac{y^2}{2σ^2}\big)\, dy}_{=0} + μ\cdot\underbrace{\int_{-\infty}^{\infty}\frac{1}{\sqrt{2πσ^2}}\cdot\exp\big(-\frac{y^2}{2σ^2}\big)\, dy}_{=1} \\
&amp;= μ\end{aligned}\]</span></li>
</ol>
</blockquote>
<blockquote>
<p>Thm 1.12. Prepulis of Expectation<br />
Let <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> be real-valued random variables, <span class="math inline">\(a,b\in\mathbb{R}\)</span>, then</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(|E[X_1]|\leq\sup_{ω\in Ω} |X_(ω)|\)</span></li>
<li><span class="math inline">\(E[aX_1 + bX_2] = a\cdot E[X_1] + b\cdot E[X_2]\)</span></li>
<li><span class="math inline">\(E[X_1]\leq E[X_2]\)</span> if <span class="math inline">\(X_1(ω)\leq X_2(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li><span class="math inline">\(E[X_1X_2] = E[X_1]\cdot E[X_2]\)</span> if <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> are independent</li>
</ol>
</blockquote>
<blockquote>
<p>Prop 1.13.<br />
Let <span class="math inline">\(g:\mathbb{R}\to\mathbb{R}\)</span> be bounded integrable function, then<br />
<span class="math display">\[E[g(X)] = \int_{-\infty}^{\infty} g(x)\cdot f(x)\, dx\]</span></p>
</blockquote>
</div>
<div id="variance" class="section level3">
<h3>Variance</h3>
<blockquote>
<p>Def 1.14 Variance &amp; SD<br />
Let <span class="math inline">\(X\)</span> be <span class="math inline">\(E[X^2]&lt;\infty\)</span>, the variance and standard deviation of <span class="math inline">\(X\)</span> are defined as
<span class="math display">\[\begin{aligned}Var(X) &amp;= E\big[(X - E[X])^2\big] \\ σ&amp;= \sqrt{Var(X)} \end{aligned}\]</span></p>
</blockquote>
<p>Variance is a measure for the dispersion of the distribution of <span class="math inline">\(X\)</span> around its expected value</p>
<blockquote>
<p>Prop 1.15.<br />
Let <span class="math inline">\(X\)</span> be <span class="math inline">\(E[X^2]&lt;\infty\)</span> and <span class="math inline">\(a,b\in\mathbb{R}\)</span>, then</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Var(x)=E[X^2] - \big(E[X]\big)^2\)</span></li>
<li><span class="math inline">\(Var(aX+b)=a^2\cdot Var(X)\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>Prop 1.16.<br />
If <span class="math inline">\(X_1,X_2,\dots,X_n\)</span> are independent with finite variance, then
<span class="math display">\[Var(X_1+\dots X_n) = Var(X_1) +\dots + Var(X_n)\]</span></p>
</blockquote>
<p>Proof:<br />
Since <span class="math inline">\(Var(x)=E[X^2] - \big(E[X]\big)^2\)</span>, assume <span class="math inline">\(E[X_i]=0\)</span> for all <span class="math inline">\(i\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}Var\big(\textstyle{\sum}_{i=1} X_i\big) &amp;= E\Big[\big(\textstyle{\sum}_{i=1} X_i\big) \Big] \\
&amp;= \textstyle{\sum}_{i,j=1}^n E[X_iX_j] \\
&amp;= \textstyle{\sum}_{i=1}^n E[X_i^2] + \underbrace{\textstyle{\sum}_{i\neq j} E[X_iX_j]}_{=0} \\
&amp;= \textstyle{\sum}_{i=1}^n E[X_i^2] \\
&amp;= \textstyle{\sum}_{i=1}^n Var(X_i) \end{aligned}\]</span></p>
</div>
<div id="moments-of-rk" class="section level3">
<h3>Moments of R^k</h3>
<ul>
<li><span class="math inline">\(\mathbb{R}^k\)</span>-valued random variable:
<span class="math display">\[X = \begin{pmatrix}X_1\\\vdots\\X_k\end{pmatrix}\]</span></li>
<li>Expectation:
<span class="math display">\[E[X] = \begin{pmatrix}E[X_1]\\\vdots\\E[X_k]\end{pmatrix}\]</span></li>
<li>Covariance matrix:
<span class="math display">\[Σ=\begin{pmatrix}E\big[(X_1-μ_1)^2\big] &amp; E\big[(X_1-μ_1)(X_2-μ_2)\big] &amp; \cdots\\\vdots &amp;\ddots &amp;\vdots\\ E\big[(X_k-μ_k)(X_1-μ_1)\big] &amp; \cdots &amp; E\big[(X_k-μ_k)^2\big]\end{pmatrix}\]</span></li>
</ul>
<blockquote>
<p>Def 1.17. Moments<br />
The s-th moment of <span class="math inline">\(X\)</span> is defined as <span class="math inline">\(E[X^s]\)</span><br />
The absolute s-th moment is defined as <span class="math inline">\(E[|X|^s]\)</span><br />
The s-th central moment is defined as <span class="math inline">\(E\big[(X-E[X])^s\big]\)</span></p>
</blockquote>
</div>
</div>
<div id="stochastic-convergence" class="section level2">
<h2>Stochastic Convergence</h2>
<div class="figure">
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/46c94668666d71613847f157ca27de5b4e938023" alt="" />
<p class="caption">Source: <a href="https://zh.wikipedia.org/wiki/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B">wiki</a></p>
</div>
<p>a.s. =&gt; P =&gt; d =&gt; Op(1)<br />
p-th =&gt;</p>
<div id="convergence-in-expectations-almost-sure-convergence" class="section level3">
<h3>Convergence in Expectations &amp; Almost Sure Convergence</h3>
<ul>
<li><span class="math inline">\(X\)</span> is a real-valued random variable</li>
<li><span class="math inline">\(\{X_n\}\)</span> a sequence of real valued random variables s.t. <span class="math inline">\(X_n(ω)\to X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li>Probability space: <span class="math inline">\((Ω,\mathcal{A},P)\)</span>
<ul>
<li><span class="math inline">\(X\)</span>, <span class="math inline">\(X_n: Ω\to\mathbb{R}\)</span></li>
</ul></li>
<li>Sequence <span class="math inline">\(\{a_n\}\)</span> of real numbers converges to <span class="math inline">\(a\)</span><br />
<span class="math inline">\(\iff\)</span> for all <span class="math inline">\(ε&gt;0\)</span>, exists <span class="math inline">\(N=N(ε)\)</span> s.t. <span class="math inline">\(|a_n-a|\leq ε\)</span> for all <span class="math inline">\(n\geq N\)</span></li>
</ul>
<blockquote>
<p>Does <span class="math inline">\(X_n(ω)\to X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span> implies <span class="math inline">\(E[X_n]\to E[X]\)</span>?</p>
</blockquote>
<p>No.<br />
Proof: exercise</p>
<blockquote>
<p>Thm 1.18. Monotone Convergence Theorem<br />
<span class="math inline">\(E[X_n]\to E[X]\)</span> if for all <span class="math inline">\(ω\in Ω\)</span>,</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n(ω)\to X(ω)\)</span></li>
<li><span class="math inline">\(0\leq X_n(ω)\leq X_{n+1}(ω)\)</span> for all <span class="math inline">\(n\geq 1\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>Thm 1.19. Dominated Convergence Theorem<br />
<span class="math inline">\(E[X_n]\to E[X]\)</span> if for all <span class="math inline">\(ω\in Ω\)</span>,</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n(ω)\to X(ω)\)</span></li>
<li>For some random variable <span class="math inline">\(Y\)</span> s.t. <span class="math inline">\(E[Y]&lt;\infty\)</span>,<br />
<span class="math inline">\(|X_n(ω)|\leq Y(ω)\)</span> for all <span class="math inline">\(n\geq 1\)</span></li>
</ol>
</blockquote>
<p>Remarks: Conditions in Thm 1.18. and 1.19. can be weakened to hold only almost surely:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n\to X\)</span> almost surely<br />
<span class="math inline">\(\iff\)</span> <span class="math display">\[P(X_m\to X) = P\big(\{ω\in Ω : X_n(ω)\to X(ω)\}\big) = 1\]</span></li>
<li><span class="math inline">\(0\leq X_n(ω)\leq X_{n+1}(ω)\)</span> for <span class="math inline">\(n\geq 1\)</span> almost surely<br />
<span class="math inline">\(\iff\)</span>
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg}P(0\leq X_n(ω)\leq X_{n+1}(ω)\text{ for }n\geq 1)
\\&amp;= P\big(\{ω\in Ω : 0\leq X_n(ω)\leq X_{n+1}(ω)\text{ for }n\geq 1\}\big) = 1\end{aligned}\]</span></li>
<li><span class="math inline">\(|X_n(ω)|\leq Y(ω)\)</span> for <span class="math inline">\(n\geq 1\)</span> almost surely<br />
<span class="math inline">\(\iff\)</span>
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg}P(|X_n(ω)|\leq Y(ω)\text{ for }n\geq 1)
\\&amp;= P\big(\{ω\in Ω : |X_n(ω)|\leq Y(ω)\text{ for }n\geq 1\}\big) = 1\end{aligned}\]</span></li>
</ol>
<blockquote>
<p>Def<br />
Property <span class="math inline">\(Q\)</span> holds almost surely (a.s.)<br />
<span class="math inline">\(\iff Pr(Q)=1\)</span></p>
</blockquote>
<blockquote>
<p>Examples</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\{X_n\}\)</span> be a sequence of random variables s.t.<br />
<span class="math inline">\(|X_n|\leq 1\)</span> and <span class="math inline">\(X_n\to 0\)</span> almost surely<br />
Then Thm 1.19. <span class="math inline">\(\implies E[X_n]\to 0\)</span></li>
<li>Let <span class="math inline">\(X\geq 0\)</span> s.t. <span class="math inline">\(E[X]&lt;\infty\)</span><br />
Let <span class="math inline">\(\{A_n\}\)</span> be a sequence of sets with <span class="math inline">\(A_n\nearrow A\)</span><br />
Define <span class="math inline">\(1_A(x) =\begin{cases}1 &amp; x\in A\\ 0&amp;x\not\in A \end{cases}\)</span><br />
<span class="math inline">\(\implies 1_{A_n}(x)\to 1_A(x)\)</span> for all <span class="math inline">\(x\)</span><br />
Therefore, by Thm 1.18. or 1.19., <span class="math inline">\(E\big[1_{A_n}(X)\cdot X\big]\to E\big[1_{A}(X)\cdot X\big]\)</span> because:
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(1_{A_n}\big(X(ω)\big)\cdot X(ω)\to 1_{A}\big(X(ω)\big)\cdot X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li><span class="math inline">\(1_{A_n}\big(X(ω)\big)\cdot X(ω)\leq 1_{A_{n+1}}\big(X(ω)\big)\cdot X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li><span class="math inline">\(1_{A_n}\big(X(ω)\big)\cdot X(ω)\leq Y(ω):= X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
</ol></li>
<li>Let <span class="math inline">\(X: Ω\to\mathbb{R}\)</span>, and <span class="math inline">\(f:\mathbb{R}\to\mathbb{R}\)</span> continuously differentiable and
<span class="math display">\[f_n(x) = \frac{f(x+\frac{1}{n}) - f(x)}{\frac{1}{n}}\]</span>
Therefore, by Thm 1.19, <span class="math inline">\(E\big[f_n(X)\big]\to E\big[f(X)\big]\)</span> because:
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(f_n\big(X(ω)\big)\to f&#39;\big(X(ω)\big)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li>If <span class="math inline">\(|f&#39;|\leq C\)</span>, then <span class="math inline">\(|f_n\big(X(ω)\big)|\leq C=: Y(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
</ol></li>
</ol>
<table>
<thead>
<tr class="header">
<th>Theorem</th>
<th>Example 1</th>
<th>Example 2</th>
<th>Example 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(X\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(1_{A}(X)\cdot X\)</span></td>
<td><span class="math inline">\(f(X)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(X_n\)</span></td>
<td><span class="math inline">\(X_n\)</span></td>
<td><span class="math inline">\(1_{A_n}(X)\cdot X\)</span></td>
<td><span class="math inline">\(f_n(X)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Y\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(X\)</span></td>
<td><span class="math inline">\(C\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="convergence-in-probability" class="section level3">
<h3>Convergence in Probability</h3>
<ul>
<li><span class="math inline">\(\{X_n\}\)</span>: sequence of random variables valued in <span class="math inline">\((\mathbb{R}^k, \mathcal{B}^k)\)</span></li>
<li><span class="math inline">\(X\)</span>: random variable valued in <span class="math inline">\(\mathbb{R}^k\)</span></li>
<li><span class="math inline">\((Ω,\mathcal{A},P)\)</span>: probability space
<ul>
<li><span class="math inline">\(\|\cdot\|\)</span>: some norm on <span class="math inline">\(\mathbb{R}^k\)</span></li>
</ul></li>
</ul>
<blockquote>
<p>Def 2.1.</p>
<ol style="list-style-type: lower-roman">
<li>Convergence in probability: <span class="math inline">\(X_n\overset{P}{\to}X\)</span> if for any <span class="math inline">\(ε&gt;0\)</span>,
<span class="math display">\[\underbrace{P\big(\| X_n - X \|&gt;ε\big)}_{:= a_n(ε)}\to 0\]</span>
in other words,<br />
for all <span class="math inline">\(d&gt;0\)</span>, exists <span class="math inline">\(N\in\mathbb{N}\)</span> s.t. <span class="math inline">\(a_n(ε)\leq d\)</span> for all <span class="math inline">\(n\geq N\)</span></li>
<li>Almost sure convergence: <span class="math inline">\(X_n\overset{a.s.}{\to}X\)</span> if for any <span class="math inline">\(ε&gt;0\)</span>,
<span class="math display">\[P(X_n\to X)=1\]</span>
in other words, <span class="math inline">\(P\big(\{ω\in Ω: X_n(ω)\to X(ω)\}\big) = 1\)</span></li>
<li>Convergence in p-th mean: <span class="math inline">\(X_n\overset{L_p}{\to}X\)</span> if<br />
<span class="math display">\[E\big[\|X_n - X\|^p\big]\to 0\]</span></li>
</ol>
</blockquote>
</div>
<div id="convergence-in-probability-vs.-almost-convergence" class="section level3">
<h3>Convergence in Probability vs. Almost Convergence</h3>
<p>Reference: <a href="https://www.probabilitycourse.com/chapter7/7_2_7_almost_sure_convergence.php">Almost Sure Convergence</a></p>
<blockquote>
<p>Convergence in probability does not imply almost convergence</p>
</blockquote>
<p>Counterexample:<br />
Let <span class="math inline">\((Ω,\mathcal{A},P) = ([0,1], \mathcal{B}, \text{uniform distribution})\)</span><br />
Let <span class="math inline">\(ω\in[0,1]\)</span> and define
<span class="math display">\[\begin{aligned}X_1(ω) &amp;= 1 \\
X_2(ω) &amp;=\begin{cases}1 &amp; 0\leq ω\leq \frac{1}{2} \\ 0 \end{cases} \\
X_3(ω) &amp;=\begin{cases}1 &amp; \frac{1}{2}\leq ω\leq \frac{1}{2}+\frac{1}{3} \\ 0 \end{cases} \\
X_4(ω) &amp;=\begin{cases}1 &amp; \frac{5}{6}\leq ω\leq 1 \text{ or } 0\leq ω\leq \frac{1}{4} - \frac{1}{6} \\ 0 \end{cases} \\
X_5(ω) &amp;=\begin{cases}1 &amp; \frac{1}{12}\leq ω\leq \frac{1}{12}+\frac{1}{5} \\ 0 \end{cases} \\
&amp;\dots\end{aligned}\]</span></p>
<p>Conclusions:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n\overset{P}{\to} X \equiv 0\)</span><br />
Since for <span class="math inline">\(0&lt;ε&lt;1\)</span>,
<span class="math display">\[\begin{aligned}P(|X_n-X|&gt;ε) &amp;= P(|X_n|&gt;ε)\\ &amp;= P(X_n=1) \\ &amp;= \frac{1}{n} \to 0 \end{aligned}\]</span></li>
<li><span class="math inline">\(X_n\overset{a.s.}{\not\to} X \equiv 0\)</span><br />
Since <span class="math inline">\(\sum_{n=1}^{\infty} \frac{1}{n}=\infty\)</span>,<br />
<span class="math display">\[\begin{aligned}P(X_n\to X) &amp;= P(X_n\to 0)\\ &amp;= P\big(\{ω\in Ω : X_n(ω)\to 0\}\big) = 0 \end{aligned}\]</span>
in other words, if we find <span class="math inline">\(N\)</span> s.t. <span class="math inline">\(X_N(ω)=1\)</span>, there is always <span class="math inline">\(N&#39;&gt;N\)</span> s.t. <span class="math inline">\(X_{N&#39;}(ω)=1\)</span></li>
</ol>
<blockquote>
<p>Thm 2.2. Almost convergence implies convergence in probability<br />
<span class="math inline">\(X_n\overset{a.s.}{\to} X\implies X_n\overset{P}{\to} X\)</span></p>
</blockquote>
<p>Proof:<br />
Let <span class="math inline">\(ε&gt;0\)</span> and <span class="math inline">\(z_n = 1\big(\|X_n-X\|&gt;ε \big)\)</span>
Therefore, <span class="math inline">\(X_n(ω)\to X(ω)\implies\)</span> <span class="math inline">\(1\big(\|X_n(ω)-X(ω)\|&gt;ε \big)\to 0\)</span><br />
Therefore, <span class="math inline">\(X_n\overset{a.s.}{\to} X\implies z_n\overset{a.s.}{\to} 0\)</span><br />
Let <span class="math inline">\(Y:= 1\)</span>, <span class="math inline">\(|z_n|\leq 1 = Y\)</span><br />
By Thm 1.19., <span class="math inline">\(E[z_n]\to 0\)</span>
<span class="math inline">\(\implies\)</span>
<span class="math display">\[\begin{aligned}P\big(\|X_n-X\|&gt;ε \big) &amp;= E\Big[1\big(\|X_n-X\|&gt;ε \big) \Big]\\
&amp;= E[z_n]\to 0 \end{aligned}\]</span>
therefore, <span class="math inline">\(X_n\overset{P}{\to} X\)</span></p>
<blockquote>
<p>Exercise: find some random variable <span class="math inline">\(X\)</span> s.t.<br />
<span class="math inline">\(|E[X]&lt;\infty|\)</span> but <span class="math inline">\(E[|X|]=\infty\)</span></p>
</blockquote>
</div>
<div id="convergence-in-probability-vs.-covergence-in-p-th-mean" class="section level3">
<h3>Convergence in Probability vs. Covergence in p-th Mean</h3>
<blockquote>
<p>Thm 2.3. Markov Inequality<br />
Let <span class="math inline">\(X\)</span> be a random variable and <span class="math inline">\(g:[0,\infty)\to[0,\infty)\)</span> a monototnic increasing function<br />
Then for any <span class="math inline">\(ε&gt;0\)</span>,
<span class="math display">\[P\big(\|X\|\geq ε\big)\leq\frac{E\big[g(\|X\|)\big]}{g(ε)}\]</span></p>
</blockquote>
<p>Proof:<br />
Since <span class="math inline">\(1(\|X\|\geq ε)\leq \frac{g(\|X\|)}{g(ε)}\)</span>,
<span class="math display">\[\begin{aligned}P\big(\|X\|\geq ε\big) &amp;= E\big[1(\|X\|\geq ε)\big] \\
&amp;\leq E\Big[\frac{g(\|X\|)}{g(ε)}\Big] \\
&amp;= \frac{E\big[g(\|X\|)\big]}{g(ε)} \end{aligned}\]</span></p>
<blockquote>
<p>Corollary 2.4. Chebyshev Inequality<br />
For a real-valued random variabe <span class="math inline">\(X\)</span> and <span class="math inline">\(ε&gt;0\)</span>,
<span class="math display">\[P\big(|X|\geq ε\big)\leq\frac{E[X^2]}{ε^2}\]</span></p>
</blockquote>
<p>Proof: Apply Thm 2.3. with <span class="math inline">\(g(x)=x^2\)</span></p>
<blockquote>
<p>Thm 2.5. Covergence in p-th mean implies convergence in probability<br />
<span class="math inline">\(X_n\to X\)</span> in p-th mean <span class="math inline">\(\implies X_n\overset{P}{\to}X\)</span></p>
</blockquote>
<p>Proof:<br />
Apply Thm 2.3. with <span class="math inline">\(g(x)=x^P\)</span><br />
Since <span class="math inline">\(E[\|X_n-X\|^P]\to 0\)</span><br />
<span class="math display">\[P\big(\|X_n-X\|&gt; ε\big) \leq \frac{E\big[\|X_n-X\|^P\big]}{ε^p}\to 0\]</span>
<span class="math inline">\(\implies X_n\overset{P}{\to}X\)</span></p>
<blockquote>
<p>Thm. 2.6. Weak law of large numbers, WLLN<br />
Let <span class="math inline">\(X_1, X_2,\dots\)</span> be a sequence of random variables with the following properties, then
<span class="math display">\[\begin{aligned}\bar{X}_n &amp;:= \frac{1}{n}\sum_{i=1}^n X_i \\ &amp;\overset{P}{\to} μ \end{aligned}\]</span></p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_1, X_2,\dots\)</span> are uncorrelated, i.e., for <span class="math inline">\(i\neq j\)</span>,
<span class="math display">\[Cov(X_i,X_j)=E[X_iX_j] - E[X_i]\cdot E[X_j]=0\]</span></li>
<li><span class="math inline">\(E[X_1]=E[X_2]=\dots = μ\)</span></li>
<li><span class="math inline">\(Var(Χ_i)&lt;σ^2\)</span> for all <span class="math inline">\(i\)</span></li>
</ol>
</blockquote>
<p>Proof:<br />
:) <span class="math inline">\(\bar{X}_n\overset{P}{\to} μ\)</span><br />
Therefore, consider
<span class="math display">\[\begin{aligned}E\big[(\bar{X}_n-μ)^2\big] &amp;= Var(\bar{X}_n)\\
&amp;= Var\Big(\frac{1}{n}\textstyle{\sum}_{i=1}^n X_i \Big) \\
&amp;= \frac{1}{n^2}\cdot Var\big(\textstyle{\sum}_{i=1}^n X_i \big) \\
&amp;= \frac{1}{n^2}\cdot \textstyle{\sum}_{i=1}^n Var(X_i) \\
&amp;\leq \frac{σ^2}{n}\end{aligned}\]</span>
By Cor. 2.4.,
<span class="math display">\[\begin{aligned}P\big(\|\bar{X}_n-μ\|&gt; ε\big) &amp;\leq \frac{E\big[(\bar{X}_n-μ)^2\big]}{ε^2} \\
&amp;\leq \frac{σ^2}{n\cdot ε^2} \to 0 \end{aligned}\]</span></p>
</div>
<div id="convergence-in-distribution" class="section level3">
<h3>Convergence in Distribution</h3>
<ul>
<li><span class="math inline">\(X_n, X\)</span>: <span class="math inline">\(\mathbb{R}^k\)</span> valued random variables
<ul>
<li>not necessarily in the same prob. space</li>
</ul></li>
</ul>
<blockquote>
<p>Def 2.7. Convergence in distribution<br />
<span class="math inline">\(X_n\overset{d}{\to} X\)</span> or <span class="math inline">\(X_n\overset{\mathcal{L}}{\to} X\iff\)</span> for all <span class="math inline">\(f:\mathbb{R}^k\to\mathbb{R}\)</span> continuous and bounded
<span class="math display">\[E\big[f(X_n)\big]\to E\big[f(X)\big]\]</span></p>
</blockquote>
<blockquote>
<p>Thm 2.8.<br />
Let <span class="math inline">\(X_n, X\)</span> be real-valued with distribution functions <span class="math inline">\(F_n,F\)</span><br />
<span class="math inline">\(X_n\overset{d}{\to} X \iff F_n(x)\to F(x)\)</span> at all continuity points <span class="math inline">\(x\)</span> of <span class="math inline">\(F\)</span></p>
</blockquote>
<p>Example:<br />
<span class="math inline">\(X_n = \frac{1}{n}\to X=0\)</span> with <span class="math inline">\(F_n(x) = \begin{cases}1 &amp; x\geq\frac{1}{n} \\ 0 \end{cases}\)</span> and <span class="math inline">\(F(x) = \begin{cases}1 &amp; x\geq 0\\ 0 \end{cases}\)</span><br />
A reasonable definititon of convergence in distribution should include this case!<br />
We know <span class="math inline">\(F_n(x)\to F(x)\)</span> for all continuity points: <span class="math inline">\(x\neq 0\)</span><br />
Consider <span class="math inline">\(x=0\)</span>, <span class="math inline">\(F_n(0)=0\)</span> for all <span class="math inline">\(n\)</span>, and <span class="math inline">\(F(0)=1\)</span><br />
therefore, <span class="math inline">\(F_n(0)\not\to F(0)\)</span></p>
<p>Outline of Proof:<br />
(<span class="math inline">\(\implies\)</span>)<br />
Let <span class="math inline">\(g(y) = 1(y\leq x):=\begin{cases}1 &amp; y\leq x \\ 0 \end{cases}\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}F_n(x) &amp;= P(X_n\leq x)\\
&amp;= E\big[1(X_n\leq x)\big] \\
&amp;= E\big[g(X_n)\big]\end{aligned}\]</span>
Define continuous versions of <span class="math inline">\(g(y)\)</span>: <span class="math inline">\(g_δ^{-}\)</span> and <span class="math inline">\(g_δ^{+}\)</span><br />
Since <span class="math inline">\(g_δ^{-}\)</span> and <span class="math inline">\(g_δ^{+}\)</span> are continuous and bounded, <span class="math inline">\(X_n\overset{d}{\to} X\)</span>,<br />
By Def 2.7., we have
<span class="math display">\[\begin{aligned}E\big[g_δ^{-}(X_n)\big]&amp;\to E\big[g_δ^{-}(X)\big] \\
E\big[g_δ^{+}(X_n)\big]&amp;\to E\big[g_δ^{+}(X)\big]\end{aligned}\]</span>
therefore,
<span class="math display">\[\begin{aligned}E\big[g_δ^{-}(X_n)\big] \leq E\big[g(X_n)\big] &amp;= F_n(X) \leq E\big[g_δ^{+}(X_n)\big] \\
E\big[g_δ^{-}(X)\big] \leq E\big[g(X)\big] &amp;= F(X) \leq E\big[g_δ^{+}(X)\big] \end{aligned}\]</span>
We know
<span class="math display">\[\begin{aligned}F(x-δ) = E\big[1(X\leq x-δ) \big] &amp;\leq E\big[g_δ^{-}(X)\big]\\
&amp;\leq  E\big[g_δ^{+}(X)\big] \\
&amp;\leq E\big[1(X\leq x+δ) \big] = F(x+δ) \end{aligned}\]</span>
If <span class="math inline">\(F\)</span> is continuous at <span class="math inline">\(x\)</span>, then <span class="math inline">\(F(x+δ)-F(x-δ)\to 0\)</span> as <span class="math inline">\(δ\to 0\)</span><br />
therefore, we have <span class="math inline">\(E\big[g(X_n)\big]\to E\big[g(X)\big]\)</span><br />
<span class="math inline">\(\implies F_n(X)\to F(X)\)</span> at any continuity point of <span class="math inline">\(F\)</span></p>
<blockquote>
<p>Thm 2.9. Contunuous mapping theorem<br />
Let <span class="math inline">\(X_n, X\)</span> be <span class="math inline">\(\mathbb{R}^k\)</span> valued random variables, assume that <span class="math inline">\(X_n\overset{d}{\to}X\)</span><br />
Let <span class="math inline">\(f:\mathbb{R}^k\to\mathbb{R}^l\)</span> be continuous, then
<span class="math display">\[f(X_n)\overset{d}{\to} f(X)\]</span></p>
</blockquote>
<p>Proof:<br />
:) <span class="math inline">\(E\big[g(f(X_n)) \big]\to E\big[g(f(X)) \big]\)</span> for all <span class="math inline">\(g\)</span> continuous and bounded<br />
We know <span class="math inline">\(E\big[h(X_n) \big]\to E\big[h(X) \big]\)</span> for all <span class="math inline">\(h\)</span> continuous and bounded<br />
Since <span class="math inline">\(g\circ f\)</span> is bounded and continuous, <span class="math inline">\(h\implies g\circ f\)</span></p>
<blockquote>
<p>Thm 2.10. Cramér-Wold device<br />
Let <span class="math inline">\(X_n, X\)</span> be <span class="math inline">\(\mathbb{R}^k\)</span> valued random variables,<br />
<span class="math inline">\(X_n \overset{d}{\to}X \iff a^T X_n\overset{d}{\to} a^T X\)</span> for all <span class="math inline">\(a\in\mathbb{R}^k\)</span></p>
</blockquote>
<p>Proof: by characteristic functions</p>
</div>
<div id="convergence-in-distribution-vs.-convergence-in-probability" class="section level3">
<h3>Convergence in Distribution vs. Convergence in Probability</h3>
<ul>
<li>Convergence in distribution is weaker than convergence in probability</li>
</ul>
<blockquote>
<p>Example<br />
Let <span class="math inline">\(X\sim\mathcal{N}(0,1)\)</span>, define <span class="math inline">\(X_n=(-1)^n\cdot X\)</span>, <span class="math inline">\(X_n\sim\mathcal{N}(0,1)\)</span><br />
Therefore, <span class="math inline">\(X_n\overset{d}{\to} X\)</span> but <span class="math inline">\(X_n\overset{p}{\not\to} X\)</span></p>
</blockquote>
<p>Proof:<br />
Since <span class="math inline">\(X_{2n+1} = -X\)</span>,
<span class="math display">\[\begin{aligned}P\big(|X_{2n+1}-X|&gt;ε\big) &amp;= P\big(|-X-X|&gt;ε\big) \\
&amp;= P\Big(|X|&gt;\frac{ε}{2}\Big)\\
&amp;\not\to 0 \end{aligned}\]</span></p>
<p>Remark:<br />
By def, <span class="math inline">\(X_n\overset{d}{\to} X\)</span> if <span class="math inline">\(E[f(X_n)]\to E[f(X)]\)</span> for all bounded and continuous <span class="math inline">\(f\)</span><br />
Which <strong>does not</strong> imply that <span class="math inline">\(E[X_n]\to E[X]\)</span> since <span class="math inline">\(f(x)=x\)</span> is continuous but not bounded</p>
<blockquote>
<p>Thm 2.11.<br />
<span class="math inline">\(X_n\overset{p}{\to} X\implies X_n\overset{d}{\to} X\)</span></p>
</blockquote>
<p>Proof:<br />
Let <span class="math inline">\(ε&gt;0\)</span>, <span class="math inline">\(f\)</span> be bounded and continuous<br />
WLOG let <span class="math inline">\(|f|\leq 1\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg}\Big| E\big[f(X_n)\big] - E\big[f(X)\big]\Big| \\
&amp;\leq E\Big[\big|f(X_n) - f(X) \big| \Big] \\
&amp;= E\Big[\big|f(X_n) - f(X)\cdot 1(\|X\|&gt; c) \big| \Big] + E\Big[\big|f(X_n) - f(X)\cdot 1(\|X\|\leq c) \big| \Big] \\
&amp;= E\Big[\big|\underbrace{f(X_n) - f(X)}_{|\cdot|\leq 2}\cdot 1(\|X\|&gt; c) \big| \Big] \\
&amp;\phantom{\ggg}+ E\Big[\big|f(X_n) - f(X)\cdot 1(\|X\|\leq c,\ \|X_n-X\|\leq δ) \big| \Big] \\
&amp;\phantom{\ggg}+ E\Big[\big|\underbrace{f(X_n) - f(X)}_{|\cdot|\leq 2}\cdot 1(\|X\|\leq c,\ \|X_n-X\|&gt; δ) \big| \Big] \\
&amp;\leq 2\cdot P\big(\| X\|&gt; c\big) + 2\cdot P\big(\| X_n-X\|&gt; δ\big)\\
&amp;\phantom{\ggg}+ \sup\Big\{|f(v)-f(u)\big| : \|v\|\leq c,\ \|v-u\|\leq δ\Big\}\end{aligned}\]</span>
:) <span class="math inline">\(E[f(X_n)]\to E[f(X)]\)</span>, i.e.,<br />
:) <span class="math inline">\(\Big|E\big[f(X_n)\big]- E\big[f(X)\big]\Big|\leq ε\)</span> for all <span class="math inline">\(n\geq N\)</span><br />
Choose <span class="math inline">\(c&gt;0\)</span> s.t. <span class="math inline">\(P(\|X\|&gt;c)\leq \frac{ε}{6}\)</span>,<br />
choose <span class="math inline">\(δ&gt;0\)</span> s.t. <span class="math inline">\(\sup\Big\{|f(v)-f(u)\big| : \|v\|\leq c,\ \|v-u\|\leq δ\Big\}\leq \frac{ε}{3}\)</span><br />
choose <span class="math inline">\(N\in\mathbb{N}\)</span> s.t. <span class="math inline">\(P\big(\| X_n-X\|&gt; δ\big)\leq\frac{ε}{6}\)</span> for all <span class="math inline">\(n\geq N\)</span><br />
<span class="math inline">\(\implies\)</span> we have <span class="math inline">\(\Big| E\big[f(X_n)\big] - E\big[f(X)\big]\Big| \leq ε\)</span></p>
<blockquote>
<p>Thm 2.12.<br />
Let <span class="math inline">\(c\in\mathbb{R}^k\)</span>, then <span class="math inline">\(X_n\overset{p}{\to} c\iff X_n\overset{d}{\to}\)</span></p>
</blockquote>
<p>Proof: exercises</p>
</div>
<div id="central-limit-theorem" class="section level3">
<h3>Central Limit Theorem</h3>
<ul>
<li>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be real valued random variables, <span class="math inline">\(\bar{X}_n=\frac{1}{n}\sum X_i\)</span><br />
</li>
</ul>
<ol style="list-style-type: decimal">
<li>If for all <span class="math inline">\(i\)</span>, <span class="math inline">\(X_i\)</span> are independent, and <span class="math inline">\(X_i\sim\mathcal{N}(μ,σ^2)\)</span>, then
<ul>
<li><span class="math inline">\(\bar{X}_n\sim\mathcal{N}(μ,\frac{σ^2}{n})\)</span></li>
<li><span class="math inline">\(\sqrt{n}(\frac{\bar{X}_n-μ}{σ})\sim\mathcal{N}(0,1)\)</span></li>
</ul></li>
<li>If <span class="math inline">\(X_i\)</span> are iid, and <span class="math inline">\(E[X_i]=μ\)</span>, <span class="math inline">\(Var(X_i)=σ^2\)</span>, then
<ul>
<li><span class="math inline">\(\sqrt{n}(\frac{\bar{X}_n-μ}{σ})\overset{d}{\to}\mathcal{N}(0,1)\)</span></li>
</ul></li>
</ol>
<blockquote>
<p>Thm 2.13. Central Limit Theorem (real valued iid sequences)<br />
Let <span class="math inline">\(X_1,\dots,X_n\)</span> be iid real valued random variables,<br />
<span class="math inline">\(E[X_i]=μ\)</span>, <span class="math inline">\(Var(X_i)=σ^2\)</span>, then
<span class="math display">\[\sqrt{n}(\frac{\bar{X}_n-μ}{σ})\overset{d}{\to}\mathcal{N}(0,1)\]</span></p>
</blockquote>
<p>also written as:
<span class="math display">\[\begin{aligned}\sqrt{n}(\frac{\bar{X}_n-μ}{σ}) &amp;= \sqrt{n}(\frac{\frac{1}{n}\sum X_i-μ}{σ})\\
&amp;= \frac{\frac{1}{\sqrt{n}}\sum X_i-μ}{σ} \overset{d}{\to}\mathcal{N}(0,1) \end{aligned}\]</span>
by Thm 2.9. (CMT),
<span class="math display">\[\begin{aligned}\frac{1}{\sqrt{n}}\sum(X_i-μ) \overset{d}{\to}\mathcal{N}(0,σ^2) \end{aligned}\]</span></p>
<blockquote>
<p>Thm 2.14. Central Limit Theorem (multivariable iid sequences)<br />
Let <span class="math inline">\(X_1,\dots,X_n\)</span> be iid <span class="math inline">\(\mathbb{R}^k\)</span> valued random variables,<br />
<span class="math inline">\(E[X_i]=μ\)</span>, <span class="math inline">\(Var(X_i)=Σ\)</span>, then
<span class="math display">\[\frac{1}{\sqrt{n}}\sum(X_i-μ)\overset{d}{\to}\mathcal{N}(0,Σ)\]</span></p>
</blockquote>
<p>Proof:<br />
Apply Thm 2.13. and Thm 2.10.<br />
Let <span class="math inline">\(a\in\mathbb{R}^k\)</span>,<br />
<span class="math inline">\(\implies a^TX_1,\dots,a^TX_n\)</span> are k-demensional iid with mean <span class="math inline">\(a^Tμ\)</span> and variance <span class="math inline">\(a^TΣa\)</span><br />
therefore, by Thm 2.13.,
<span class="math display">\[\frac{1}{\sqrt{n}}\sum\Big(\frac{a^TX_i - a^Tμ}{\sqrt{a^TΣa}}\Big) \overset{d}{\to}\mathcal{N}(0,1)\]</span>
by Thm 2.9. (CMT),
<span class="math display">\[\begin{aligned}a^T \frac{1}{\sqrt{n}} (X_i-μ) &amp;\overset{d}{\to} \mathcal{N}(0,a^TΣa) \\
&amp;\sim a^T X &amp;&amp; X\sim\mathcal{N}(0,Σ) \\
a^T \frac{1}{\sqrt{n}} (X_i-μ) &amp;\overset{d}{\to} a^T X \end{aligned}\]</span>
by Thm 2.10.,
<span class="math display">\[\frac{1}{\sqrt{n}}\sum(X_i-μ)\overset{d}{\to} X\]</span></p>
</div>
<div id="stochastic-boundedness-tightness" class="section level3">
<h3>Stochastic Boundedness (Tightness)</h3>
<ul>
<li><span class="math inline">\(X_n\)</span>: sequence of random variables that are not necessarily defined on the same probability space</li>
</ul>
<blockquote>
<p>Def 2.15. Stochastic boundedness<br />
<span class="math inline">\(\{X_n\}\)</span> is stochastically bounded, <span class="math inline">\(X_n=O_p(1)\)</span> if<br />
for all <span class="math inline">\(ε&gt;0\)</span>, exists <span class="math inline">\(c&gt;0\)</span> and <span class="math inline">\(N\in\mathbb{N}\)</span> s.t. for all <span class="math inline">\(n\geq N\)</span>,
<span class="math display">\[P\big(\|X_n\|\leq c\big)\geq 1-ε\]</span></p>
</blockquote>
<p>Example:<br />
<span class="math inline">\(X_n=X\sim\mathcal{N}(0,1)\)</span><br />
for any <span class="math inline">\(c&gt;0\)</span>, always exists <span class="math inline">\(X_i\)</span> s.t. <span class="math inline">\(|X_i|&gt;c\)</span><br />
therefore, cannot use <span class="math inline">\(P\big(\|X_n\|\leq c\big)=1\)</span><br />
$P(|X_n|c)</p>
<blockquote>
<p>Thm 2.16.<br />
<span class="math inline">\(X_n\overset{d}{\to} X\implies X_n=O_p(1)\)</span></p>
</blockquote>
<p>Proof: assume</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n, X\)</span> are real valued</li>
<li>Distribution function <span class="math inline">\(F\)</span> of <span class="math inline">\(X\)</span> is continuous</li>
</ol>
<p>For <span class="math inline">\(ε&gt;0\)</span>, choose <span class="math inline">\(X_ε^-\)</span> and <span class="math inline">\(X_ε^+\)</span> s.t. <span class="math inline">\(F(X_ε^-)&lt;\frac{ε}{2}\)</span> and <span class="math inline">\(F(X_ε^+)&gt;1-\frac{ε}{2}\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}P(X_ε^-&lt;X\leq X_ε^+) &amp;= F_n(X_ε^+) - F_n(X_ε^-)\\
&amp;\to F(X_ε^+) - F(X_ε^-)\\
&amp;&gt; 1-ε\end{aligned}\]</span>
Choose <span class="math inline">\(c=\max\{|X_ε^-|, |X_ε^+|\}\)</span>, therefore,<br />
<span class="math display">\[\begin{aligned}P(|X|\leq c)&amp;\geq P(X_ε^-&lt;X\leq X_ε^+ )\\
&amp;\geq 1-ε \end{aligned}\]</span></p>
</div>
</div>

    </div>
  </article>
  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  (function() { 
  var d = document, s = d.createElement('script');
  s.src = 'https://loikein-github.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>

</main>
      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>

          <li>By loikein with love</li>
        </ul>
      </footer>
    </div>
    
    <script src="https://hypothes.is/embed.js" async></script>
    
    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-143089736-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>