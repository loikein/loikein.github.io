<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.61.0" />

<title>Notes on Econometric Theory - loikein&#39;s notes</title>
<meta property="og:title" content="Notes on Econometric Theory - loikein&#39;s notes">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-143089736-1', 'auto');
  
  ga('send', 'pageview');
}
</script>


  


<link rel="icon" type="image/x-icon" href="/favicon.ico" />


<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">
<link rel="stylesheet" href="/css/clumsy-toc.css" media="all">




  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/post/" class="nav-logo">
    <img src="/images/loikein-logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>
  <ul class="nav-links">
    
    <li><a href="/post/">Posts</a></li>
    
    <li><a href="/tags/">Tags</a></li>
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/loikein/loikein.github.io">GitHub</a></li>
    
  </ul>
</nav>
      </header>
<main class="content" role="main">
  <article class="article">
    
    <span class="article-duration">35 min read</span>
    
    <h1 class="article-title">Notes on Econometric Theory</h1>
    
    <span class="article-date">2019/10/08</span>
    
    
    
    <span class="tags">
    
    
    Tags:
    
    <a href='/tags/notes'>notes</a>
    
    <a href='/tags/stat'>stat</a>
    
    
    
    </span>
    
    <div class="article-content">
      

<div id="TOC">
<ul>
<li><a href="#literature">Literature</a></li>
<li><a href="#probability-measures">Probability Measures</a>
<ul>
<li><a href="#σ--borel-field">σ- &amp; Borel Field</a></li>
<li><a href="#distribution-function">Distribution Function</a></li>
<li><a href="#extension-to-rk">Extension to R^k</a></li>
</ul></li>
<li><a href="#random-variables">Random Variables</a>
<ul>
<li><a href="#expectation">Expectation</a></li>
<li><a href="#variance">Variance</a></li>
<li><a href="#moments-of-rk">Moments of R^k</a></li>
</ul></li>
<li><a href="#stochastic-convergence">Stochastic Convergence</a>
<ul>
<li><a href="#convergence-in-expectations-almost-sure-convergence">Convergence in Expectations &amp; Almost Sure Convergence</a></li>
<li><a href="#convergence-in-probability">Convergence in Probability</a></li>
<li><a href="#convergence-in-probability-vs.-almost-convergence">Convergence in Probability vs. Almost Convergence</a></li>
<li><a href="#convergence-in-probability-vs.-covergence-in-p-th-mean">Convergence in Probability vs. Covergence in p-th Mean</a></li>
<li><a href="#convergence-in-distribution">Convergence in Distribution</a></li>
<li><a href="#convergence-in-distribution-vs.-convergence-in-probability">Convergence in Distribution vs. Convergence in Probability</a></li>
<li><a href="#central-limit-theorem">Central Limit Theorem</a></li>
<li><a href="#stochastic-boundedness-tightness">Stochastic Boundedness (Tightness)</a></li>
<li><a href="#calculus-for-convergence-in-probability-tightness">Calculus for Convergence in Probability &amp; Tightness</a></li>
</ul></li>
<li><a href="#m-estimators">M-Estimators</a>
<ul>
<li><a href="#linear-regression-model">Linear Regression Model</a></li>
<li><a href="#parametric-model">Parametric Model</a></li>
<li><a href="#consistency">Consistency</a></li>
<li><a href="#uniform-convergence-in-probability">Uniform Convergence in Probability</a></li>
<li><a href="#asymphotic-normality">Asymphotic Normality</a></li>
</ul></li>
</ul>
</div>

<!-- parametric estimation -->
<ul>
<li>Uppercase variables: random variables</li>
<li>Lowercase variables: deterministic variables</li>
<li>(I missed the PS4 class)</li>
</ul>
<div id="literature" class="section level2">
<h2>Literature</h2>
<p>Asymptotic Theory:</p>
<ul>
<li>Casella, Berger: Statistical Inference</li>
<li>Bruce Hansen: Econometrics (Lecture Notes), Ch5</li>
<li>Davidson: Econometric Theory</li>
<li>White: Asymptotic Theory for Econometricians</li>
</ul>
<p>M-, Z-, Extremum Estimators:</p>
<ul>
<li>?</li>
</ul>
<!-- 
## Asymptotic Statistics

- Observations: $(x_n) = x_1, x_2,\dots$
    - Assumption: $x_n\to x$
- Linear model: $y_i = x_i^T\cdot β + ε_i$, $i=1,\dots,m$
- Estimator: $\hat{β} = \hat{β}_n = \hat{β}_n\big((y_1, x_1),\dots,(y_n,x_n)\big)$
    - $(\hat{β}_n) = \hat{β}_1, \hat{β}_2$
    - Consistency: $\hat{β}_n\to β$

## M-Estimators, Z-Estimators, Extremum Estimators

- Assume $\{z_i: i=1,\dots,m\}\overset{iid}{\sim}P_{θ_0}$
    - $P$ is some distribution
    - $θ_0 = (μ_0, σ_0^2)$
- Linear model: $y_i = x_i^T\cdot θ_0 + ε_i$
    - Assume $E[ε_i\ |\ x_i] = 0\implies E[y_i\ |\ x_i] = x_i^T\cdot θ_0$
    - Theoretically, $θ_0 = \text{argmin}_{θ\in Θ}\ Q(θ)$
    - In practice, let $\hat{θ}_0 = \text{argmin}_{θ\in Θ}\ \hat{Q}_n(θ)$
    - Consistency: $\hat{θ}_n\overset{p}{\to} θ_0$
    - $\sqrt{m}(\hat{θ}_n - θ_0)\overset{d}{\to} \mathcal{N}(0,Σ)$
 -->
</div>
<div id="probability-measures" class="section level2">
<h2>Probability Measures</h2>
<ul>
<li><span class="math inline">\(Ω\)</span>: sample space, a set of possible outcomes of an experiment
<ul>
<li>Countable set: <span class="math inline">\(\| Ω \|\leq \mathbb{N}\)</span></li>
</ul></li>
<li><span class="math inline">\(A\subset Ω\)</span>: event, a collection of possible outcomes
<ul>
<li>Disjoint sets: <span class="math inline">\(A_i\cap A_j = \emptyset\)</span> for <span class="math inline">\(i\neq j\)</span></li>
</ul></li>
<li><span class="math inline">\(\mathcal{A}\)</span>: family of <span class="math inline">\(A\)</span></li>
<li>Step functions: <span class="math inline">\(F\)</span> is a step function with finitely or countably many jumps <span class="math inline">\(x_1&lt;x_2&lt;\dots\)</span> and jump heights <span class="math inline">\(b_1,b_2,\dots &gt;0\)</span></li>
</ul>
<blockquote>
<p>Def 1.1. Axioms of probability / Kolmogorov Axioms<br />
A probability measure <span class="math inline">\(P\)</span> on <span class="math inline">\((Ω,\mathcal{A})\)</span> is a mapping <span class="math inline">\(P:\mathcal{A}\to[0,1]\)</span> with the following properties:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(P(\emptyset)= 0\)</span></li>
<li><span class="math inline">\(P(Ω) =1\)</span></li>
<li><span class="math inline">\(P\big(\cup_{i=1}^{\infty} A_i\big) = \sum_{i=1}^{\infty} P(A_i)\)</span> for all <span class="math inline">\(A_i\in\mathcal{A}\)</span> that are disjoint</li>
</ol>
</blockquote>
<p>Remarks:</p>
<ol style="list-style-type: lower-alpha">
<li>Def 1.1. formally defines the probability <span class="math inline">\(P(A)\)</span> that event <span class="math inline">\(A\)</span> occurs</li>
<li>By (i), the probability that nothing happens is 0<br />
By (ii), the probability that something happens is 1</li>
<li>By (iii) <span class="math inline">\(\implies\)</span> finite additivity (σ-additivity) for disjoint <span class="math inline">\(A_i\)</span>’s:
<span class="math display">\[P\big(\cup_{i=1}^{I} A_i\big) = \sum_{i=1}^{I} P(A_i)\]</span></li>
<li>If <span class="math inline">\(Ω\)</span> is countable, we can always take <span class="math inline">\(\mathcal{A} = \mathcal{P}(Ω)\)</span> (power set)</li>
<li>If <span class="math inline">\(Ω\)</span> is uncountable, it is in general impossible to define <span class="math inline">\(P\)</span> for all <span class="math inline">\(A\subset Ω\)</span> s.t. (i) - (iii) are satisfied<br />
thus, we cannot set <span class="math inline">\(\mathcal{A} = \mathcal{P}(Ω)\)</span> in general, but have to restrict the class of possible events <span class="math inline">\(A\)</span></li>
</ol>
<blockquote>
<p>PS1.Q3<br />
Let <span class="math inline">\(Ω\)</span> be a non-empty set, <span class="math inline">\(\mathcal{A}\)</span> a σ-field on <span class="math inline">\(Ω\)</span> and <span class="math inline">\(A_1,\dots,A_n\in \mathcal{A}\)</span>.<br />
Show that a probability measure <span class="math inline">\(P\)</span> on <span class="math inline">\((Ω, \mathcal{A})\)</span> has the following properties:</p>
<ol style="list-style-type: lower-roman">
<li>Let <span class="math inline">\(A_1,\dots,A_n\)</span> be pairwise disjoint, then <span class="math inline">\(P(\cup_{i=1}^n A_i) = \sum_{i=1}^n P(A_i)\)</span></li>
<li><span class="math inline">\(P(A^c)=1−P(A)\)</span></li>
<li><span class="math inline">\(A_1\subset A_2\implies P(A_2\backslash A_1)=P(A_2)−P(A_1)\)</span></li>
<li><span class="math inline">\(A_1\subset A_2\implies P(A_1)\leq P(A_2)\)</span></li>
<li><span class="math inline">\(P(\cup_{i=1}^n A_i)\leq \sum_{i=1}^n P (A_i)\)</span></li>
<li><span class="math inline">\(P(A_1\cup A_2)=P(A_1)+P(A_2)−P(A_1\cup A_2)\)</span></li>
</ol>
</blockquote>
<ol style="list-style-type: lower-roman">
<li>Construct <span class="math inline">\(A_i = \emptyset\)</span> for all <span class="math inline">\(i\geq n+1\)</span>,
<span class="math display">\[\begin{aligned}P(\cup_{i=1}^n A_i) &amp;= P(\cup_{i=1}^{\infty} A_i) \\
&amp;= \sum_{i=1}^{\infty} P(A_i) \\
&amp;= \sum_{i=1}^n P(A_i) + \sum_{i=n+1}^{\infty} \underbrace{P(A_i)}_{=0} \\
&amp;= \sum_{i=1}^n P(A_i)\end{aligned}\]</span></li>
<li>For all <span class="math inline">\(A\in Ω\)</span>,
<span class="math display">\[\begin{aligned}P(Ω) = 1 &amp;= P(A\cup A^C) \\
&amp;= P(A) + P(A^C)\end{aligned}\]</span></li>
<li><span class="math inline">\(A_1\subset A_2\implies\)</span>
<span class="math display">\[\begin{aligned}A_2 &amp;= A_1\cup (A_2\backslash A_1) \\
P(A_2) &amp;= P(A_1) + P(A_2\backslash A_1) \end{aligned}\]</span></li>
<li>Following (iii), <span class="math inline">\(A_1\subset A_2\implies\)</span>
<span class="math display">\[\begin{aligned}P(A_1) &amp;= P(A_2) - P(A_2\backslash A_1) \\
&amp;\leq P(A_2)\end{aligned}\]</span></li>
<li>Way 1: By induction using (vi)<br />
Way 2: Let <span class="math inline">\(A_1,\dots,A_n\in\mathcal{A}\)</span>,<br />
Define <span class="math inline">\(B_1 = A_1\)</span>, <span class="math inline">\(B_2 = A_2\backslash A_1\)</span>, <span class="math inline">\(B_3 = A_3\backslash(A_1\cup A_2)\)</span>…<br />
Therefore, following (iv),
<span class="math display">\[\begin{aligned}P\big( \cup_{i=1}^{\infty} A_i) &amp;= P\big( \cup_{i=1}^{\infty} B_i) \\
&amp;= \sum_{i=1}^{\infty} \underbrace{P(B_i)}_{\leq P(A_i)} \\
&amp;\leq \sum_{i=1}^{\infty} P(A_i) \end{aligned}\]</span></li>
<li>If <span class="math inline">\(A_1\)</span>, <span class="math inline">\(A_2\)</span> are not disjoint,
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg} P(A_1)+P(A_2)−2P(A_1\cup A_2) \\
&amp;= P\big(A_1\backslash (A_1\cup A_2)\big) + P\big(A_2\backslash (A_1\cup A_2)\big) \\
&amp;= P\big[ \big(A_1\backslash (A_1\cup A_2)\big)\cup\big(A_2\backslash (A_1\cup A_2)\big)\big] \\
&amp;= P\big((A_1\cup A_2)\backslash (A_1\cap A_2) \big) \\
&amp;= P(A_1\cup A_2) - P(A_1\cap A_2)\end{aligned}\]</span></li>
</ol>
<blockquote>
<p>Example: Laplace distribution<br />
Let <span class="math inline">\(Ω\)</span>, <span class="math inline">\(\mathcal{A} = \mathcal{P}(Ω)\)</span> the class of all subsets of <span class="math inline">\(Ω\)</span>, then<br />
<span class="math display">\[P(A) := \frac{|A|}{|Ω|}\]</span></p>
</blockquote>
<p>Consider tossing a fair dice once, <span class="math inline">\(Ω = \{1,\dots,6\}\)</span>, then <span class="math inline">\(P(A=\{6\}) = \frac{1}{6}\)</span><br />
Consider tossing a fair dice twive, <span class="math inline">\(Ω = \{(1,1),\dots,(6,6)\}\)</span>, then <span class="math inline">\(P(A=\{(2,6), (3,5),(4,4)\}) = \frac{3}{36}\)</span></p>
<blockquote>
<p>Prop 1.2., PS1.Q4<br />
Let <span class="math inline">\(Ω\)</span> be countable, mapping <span class="math inline">\(p:Ω\to[0,1]\)</span> is <span class="math inline">\(\sum_{ω\in Ω}p(ω)=1\)</span><br />
Then <span class="math inline">\(P(A):= \sum_{ω\in A}p(ω)\)</span> for all <span class="math inline">\(A\in\mathcal{P}(Ω)\)</span> defines a probability measure on <span class="math inline">\(\mathcal{P}(Ω)\)</span></p>
</blockquote>
<p>Proof: check Kolmogorov Axioms</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(P(\emptyset) = \sum_{ω\in\emptyset} p(ω)= 0\)</span></li>
<li><span class="math inline">\(P(Ω) = \sum_{ω\in Ω} p(ω)= 1\)</span></li>
<li>Let <span class="math inline">\(A_1,A_2,\dots\)</span> be disjoint<br />
<span class="math inline">\(\implies\)</span> for any <span class="math inline">\(ω\in A_i\)</span>, <span class="math inline">\(ω\not\in A_j\)</span> for any <span class="math inline">\(j\neq i\)</span><br />
<span class="math display">\[\begin{aligned}P\big(\cup_{i=1}^{\infty} A_i\big) &amp;= \sum_{ω\in \cup_{i=1}^{\infty} A_i} p(ω) \\
&amp;= \sum_{i=1}^{\infty}\underbrace{\sum_{ω\in A_i} p(ω)}_{P(A_i)} \\
&amp;= \sum_{i=1}^{\infty} P(A_i)\end{aligned}\]</span></li>
</ol>
<blockquote>
<p>Example: Probability measure</p>
<ol style="list-style-type: decimal">
<li>Laplace distribution<br />
</li>
<li>Bernoulli distribution<br />
<span class="math inline">\(Ω = \{0,1\}\)</span>, <span class="math inline">\(p(1)=q\)</span> for some <span class="math inline">\(q\in[0,1]\)</span>, <span class="math inline">\(p(0)=1-q\)</span><br />
</li>
<li>Binomial distribution<br />
<span class="math inline">\(\mathcal{B}(m,p)\)</span>, <span class="math inline">\(m\geq 1\)</span>, <span class="math inline">\(0\leq q\leq 1\)</span><br />
<span class="math inline">\(Ω = \{0,\dots,m\}\)</span>, <span class="math inline">\(p(k)=(m, k)^T\cdot q^k\cdot(1-q)^{m-k}\)</span> for some <span class="math inline">\(q\in[0,1]\)</span>, <span class="math inline">\(p(0)=1-q\)</span><br />
<span class="math inline">\(\sum_{k=0}^{\infty} p(k)= \sum_{k=0}^{\infty} (m, k)^T\cdot q^k\cdot(1-q)^{m-k}= \big(q + (1-q)\big)^m =1\)</span></li>
<li>Poisson distribution<br />
<span class="math inline">\(Ω = \mathbb{N}_0\)</span>, <span class="math inline">\(p(k)=\frac{λ^k}{k!}\cdot l^{-λ}\)</span><br />
<span class="math inline">\(\sum_{k=0}^{\infty} p(k)=\Big(\sum_{k=0}^{\infty}\frac{λ^k}{k!}\Big)\cdot l^{-λ} = l^{λ}\cdot l^{-λ} =1\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>PS1.Q5<br />
Suppose that for a probability measure <span class="math inline">\(P\)</span> and a sequence of sets <span class="math inline">\(A_1,A_2,\dots\)</span>, it holds that <span class="math inline">\(P(A_i) = 0\)</span>.<br />
Which value has <span class="math inline">\(P\big(\sum_{i=1}^{\infty} A_i\big)\)</span>?</p>
</blockquote>
<p>Way 1:</p>
<ol style="list-style-type: lower-roman">
<li>For finite union, prove <span class="math inline">\(P\big(\sum_{i=1}^n A_i\big) = 0\)</span><br />
Following PS1.Q3 (vi), for <span class="math inline">\(n=2\)</span>,
<span class="math display">\[\begin{aligned}P(A_1\cup A_2) &amp;= \underbrace{P(A_1)}_{\geq 0} + \underbrace{P(A_2)}_{\geq 0} - P(A_1\cap A_2) \\
&amp;\geq 0 \\
P(A_1\cap A_2) &amp;= 0 \end{aligned}\]</span>
By induction on <span class="math inline">\(n\)</span>, it follows that <span class="math inline">\(P\big(\sum_{i=1}^n A_i\big) = 0\)</span></li>
<li>For countably infinite union, prove <span class="math inline">\(P\big(\sum_{i=1}^{\infty} A_i\big) = 0\)</span><br />
Following (i),
<span class="math display">\[P\big(\cup_{i=1}^{\infty} A_i\big) = P\big(\cup_{i=1}^{\infty} A_i \backslash \cup_{i=n+1}^n A_i\big) + \underbrace{P\big(\cup_{i=n+1}^n A_i\big)}_{=0}\]</span>
Therefore, set <span class="math inline">\(B_n = \cup_{i=1}^{\infty} A_i \backslash \cup_{i=n+1}^n A_i\)</span> is monotonicly decreasing
<span class="math display">\[\begin{aligned}\lim_{n\to\infty} B_n &amp;= P\big(\cap_{i=1}^{\infty} B_1\big) \\
&amp;= P\big(\cup_{i=1}^{\infty} A_i \backslash \cup_{i=n+1}^{\infty} A_i \big) \\
&amp;= P(\emptyset) \\&amp;= 0 \end{aligned}\]</span></li>
</ol>
<p>Way 2: follow PS1.Q3 (v) way 2,<br />
Define <span class="math inline">\(B_1 = A_1\)</span>, <span class="math inline">\(B_2 = A_2\backslash A_1\)</span>, <span class="math inline">\(B_3 = A_3\backslash(A_1\cup A_2)\)</span>…<br />
<span class="math display">\[\begin{aligned}P\big(\cup_{i=1}^{\infty} A_i\big) &amp;= P\big(\cup_{i=1}^{\infty} B_i\big) \\
&amp;= \sum_{i=1}^{\infty} \underbrace{P(B_i)}_{\leq P(A_i)} \\
&amp;\leq \sum_{i=1}^{\infty} P(A_i) \\
&amp;= 0 \end{aligned}\]</span></p>
<blockquote>
<p>Remark 1.1.d.<br />
If <span class="math inline">\(Ω\)</span> is uncountable, it is in general impossible to define <span class="math inline">\(P\)</span> for all <span class="math inline">\(A\subset Ω\)</span> s.t. Kolmogorov Axioms are satisfied</p>
</blockquote>
<p>Solution: σ- &amp; Borel Field</p>
<div id="σ--borel-field" class="section level3">
<h3>σ- &amp; Borel Field</h3>
<blockquote>
<p>Def 1.3. σ-field (algebra)<br />
Define <span class="math inline">\(P\)</span> on <span class="math inline">\(\mathcal{A}\subset\mathcal{P}(Ω)\)</span><br />
If all <span class="math inline">\(A\in\mathcal{A}\)</span> have the following properties, then <span class="math inline">\(\mathcal{A}\)</span> is called a σ-algebra or a σ-field</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(\emptyset\in\mathcal{A}\)</span></li>
<li><span class="math inline">\(A\in\mathcal{A}\implies A^C\in\mathcal{A}\)</span></li>
<li><span class="math inline">\(A_1,A_2,\dots\in\mathcal{A}\implies \cup_{i=1}^{\infty}A_i\in\mathcal{A}\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>PS1.Q2<br />
Let <span class="math inline">\(Ω\)</span> be a non-empty set and <span class="math inline">\(\mathcal{A}\)</span> a σ-field on <span class="math inline">\(Ω\)</span>. Show the following two statements:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Ω\in\mathcal{A}\)</span></li>
<li>If <span class="math inline">\(A_1, A_2\in\mathcal{A}\)</span>, then <span class="math inline">\(A_1\cap A_2\in\mathcal{A}\)</span></li>
</ol>
</blockquote>
<ol style="list-style-type: lower-roman">
<li>Since the properties of <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(\emptyset\in\mathcal{A}\)</span><br />
So <span class="math inline">\(\emptyset ^C = Ω\in\mathcal{A}\)</span></li>
<li>Since <span class="math inline">\(A_1\in\mathcal{A}\implies A_1^C\in\mathcal{A}\)</span><br />
And <span class="math inline">\(A_2\in\mathcal{A}\implies A_2^C\in\mathcal{A}\)</span><br />
Therefore, <span class="math inline">\((A_1^C)\cup(A_2^C)\in\mathcal{A}\)</span><br />
<span class="math inline">\(\implies \big(A_1^C)\cup(A_2^C)\big)^C = A_1\cap A_2\in\mathcal{A}\)</span></li>
</ol>
<blockquote>
<p>Def 1.4. Borel σ-field<br />
Let <span class="math inline">\(Ω=\mathbb{R}\)</span><br />
The σ-field <span class="math inline">\(\mathcal{B}\)</span> that contains all open intervals <span class="math inline">\((a,b)\)</span> for <span class="math inline">\(-\infty\leq a&lt;b\leq\infty\)</span> is called Borel-σ-field<br />
A set <span class="math inline">\(A\in\mathcal{B}\)</span> is called a Borel set</p>
</blockquote>
<p>Remark: <span class="math inline">\(\mathcal{B}\neq\mathcal{P}(Ω)\)</span></p>
<!-- > $\mathcal{A}_1=\{(a,b]:-\infty\leq a<b<\infty\}$  
> $\mathcal{A}_2=\{[a,b):-\infty < a<b\leq\infty\}$  
> $\mathcal{A}_3=\{[a,b]:-\infty < a<b<\infty\}$  
> $\mathcal{A}_4=\{(-\infty,b]:-\infty <b<\infty\}$  
> For $j=1,\dots,4$, $\mathcal{B}$ is the smallest σ-field that contains $\mathcal{A}_j$
 -->
<blockquote>
<p>Remark 1.5., PS2.Q2<br />
Let <span class="math inline">\(\mathcal{B}\)</span> be the Borel σ-field on R, i.e., the smallest σ-field which contains all open intervals <span class="math inline">\((a,b)\)</span> with <span class="math inline">\(-\infty\leq a&lt;b\leq\infty\)</span>.<br />
Furthermore, define <span class="math inline">\(\mathcal{B}^*\)</span> to be the smallest σ-field on R that contains all half-open intervals <span class="math inline">\((a, b]\)</span> with <span class="math inline">\(-\infty\leq a&lt;b&lt;\infty\)</span>.<br />
Show <span class="math inline">\(\mathcal{B}=\mathcal{B}^*\)</span>.</p>
<p>Hint:<br />
Verify that <span class="math inline">\(\mathcal{B}\subset\mathcal{B}^*\)</span> and <span class="math inline">\(\mathcal{B}^*\subset\mathcal{B}\)</span>.<br />
To show that <span class="math inline">\(\mathcal{B}\subset\mathcal{B}^*\)</span>, proceed as follows:<br />
First argue that <span class="math inline">\(\mathcal{B}\subset\mathcal{B}^*\)</span> if all open intervals <span class="math inline">\((a, b)\)</span> are elements of <span class="math inline">\(\mathcal{B}^*\)</span>,<br />
then verify that indeed <span class="math inline">\((a, b)\in\mathcal{B}^*\)</span> for any <span class="math inline">\(a, b\)</span>.</p>
</blockquote>
<p>Proof:</p>
<ol style="list-style-type: decimal">
<li>(<span class="math inline">\(\subset\)</span>)<br />
Let <span class="math inline">\(-\infty\leq a&lt;b\leq\infty\)</span>
<ol style="list-style-type: lower-roman">
<li>When <span class="math inline">\(b=\infty\)</span>,
<span class="math display">\[\begin{aligned}(a,b) &amp;= (a,\infty) \\
 &amp;= \underbrace{\cup_{k\in\mathbb{N}} \overbrace{(a, a+k]}^{\in\mathcal{B}^*}}_{\in\mathcal{B}^*} \\
 &amp;\in \mathcal{B}^* \end{aligned}\]</span></li>
<li>When <span class="math inline">\(b&lt;\infty\)</span>,
<span class="math display">\[\begin{aligned}(a,b) &amp;= \cup_{k\in\mathbb{N}}\Big(a, b-\frac{1}{k}\Big] \\
 &amp;\in \mathcal{B}^* \end{aligned}\]</span></li>
<li>Therefore, <span class="math inline">\((a,b)\in\mathcal{B}^* \implies \mathcal{B}\in\mathcal{B}^*\)</span></li>
</ol></li>
<li>(<span class="math inline">\(\supset\)</span>)<br />
Let <span class="math inline">\(-\infty\leq a&lt;b&lt;\infty\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}(a,b] &amp;= \cap_{k\in\mathbb{N}}\Big(a,b+\frac{1}{k}\Big) \\
&amp;= \Bigg[ \underbrace{\cup_{k\in\mathbb{N}} \bigg(\overbrace{\Big(a, b+\frac{1}{k}\Big)}^{\in\mathcal{B}}\bigg)^C}_{\in\mathcal{B}}\Bigg]^C \\
&amp;\in \mathcal{B} \end{aligned}\]</span>
<span class="math inline">\(\implies \mathcal{B}^*\in\mathcal{B}\)</span></li>
</ol>
<blockquote>
<p>PS2.Q1.<br />
Let <span class="math inline">\(Ω\)</span> be a non-empty set.</p>
<ol style="list-style-type: lower-roman">
<li>Suppose that <span class="math inline">\(\mathcal{A}_i\)</span> are σ-fields on <span class="math inline">\(Ω\)</span>, <span class="math inline">\(i\in I\)</span>, <span class="math inline">\(I\)</span> is an arbitrary index set.<br />
Show that <span class="math inline">\(\cap_{i\in I}\mathcal{A}_i\)</span> is a σ-field on <span class="math inline">\(Ω\)</span>.</li>
<li>Let <span class="math inline">\(\mathcal{A}^*\)</span> be a class of subsets of <span class="math inline">\(Ω\)</span>.<br />
Apply (i) to show that the smallest σ-field <span class="math inline">\(\mathcal{A}\)</span> with <span class="math inline">\(\mathcal{A}^*\subset\mathcal{A}\)</span> is well defined.</li>
</ol>
</blockquote>
<ol style="list-style-type: lower-roman">
<li><ol style="list-style-type: decimal">
<li>Since <span class="math inline">\(\emptyset\in \mathcal{A}_i\)</span> for all <span class="math inline">\(i\)</span>,<br />
<span class="math inline">\(\emptyset\in\cap\mathcal{A}_i\)</span></li>
<li>Let <span class="math inline">\(A\in\cap\mathcal{A}_i\)</span><br />
<span class="math inline">\(\implies A\in\mathcal{A}_i\)</span> for all <span class="math inline">\(i\)</span><br />
<span class="math inline">\(\implies A^C\in\mathcal{A}_i\)</span> for all <span class="math inline">\(i\)</span><br />
<span class="math inline">\(\implies A^C\in\cap\mathcal{A}_i\)</span></li>
<li>Let <span class="math inline">\(A_1,A_2,\dots\in\in\cap\mathcal{A}_i\)</span><br />
<span class="math inline">\(\implies A_1,A_2\in\mathcal{A}_i\)</span> for all <span class="math inline">\(i\)</span><br />
<span class="math inline">\(\implies (\cup_{n\in\mathbb{N}}A_n)\in\mathcal{A}_i\)</span> for all <span class="math inline">\(i\)</span><br />
<span class="math inline">\(\implies (\cup_{n\in\mathbb{N}}A_n)\in\cap\mathcal{A}_i\)</span></li>
</ol></li>
<li><ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\mathbb{A}\)</span> be the collection of all σ-fields on <span class="math inline">\(Ω\)</span> s.t.<br />
for all <span class="math inline">\(\mathcal{A}\in \mathbb{A}\)</span>, <span class="math inline">\(\mathcal{A}^*\in \mathbb{A}\)</span><br />
Let <span class="math inline">\(σ(\mathcal{A}^*) := \cap_{\mathcal{A}\in \mathbb{A}} \mathcal{A}\)</span><br />
therefore, by (i), <span class="math inline">\(σ(\mathcal{A}^*)\)</span> is a σ-field on <span class="math inline">\(Ω\)</span>, <span class="math inline">\(\mathcal{A}^*\in σ(\mathcal{A}^*)\)</span></li>
<li>:) <span class="math inline">\(σ(\mathcal{A}^*)\)</span> is the smallest σ-field on <span class="math inline">\(Ω\)</span><br />
FSOC assume exsits <span class="math inline">\(\mathcal{B}\)</span> containing <span class="math inline">\(\mathcal{A}^*\)</span> s.t. <span class="math inline">\(σ(\mathcal{A}^*)\not\subset \mathcal{B}\)</span><br />
Since <span class="math inline">\(\mathcal{A}^*\in\mathcal{B}\)</span><br />
<span class="math inline">\(\implies \mathcal{B}\in\mathbb{A}\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}σ(\mathcal{A}^*) &amp;=\cap_{\mathcal{A}\in \mathbb{A}} \mathcal{A}\\
&amp;= \cap_{\mathcal{A}\in \mathbb{A}\backslash \mathcal{B}} \cap \{\mathcal{B}\}\\
&amp;\subset \mathcal{B} \end{aligned}\]</span>
Contradiction.</li>
</ol></li>
</ol>
</div>
<div id="distribution-function" class="section level3">
<h3>Distribution Function</h3>
<blockquote>
<p>Def 1.6. Distribution function<br />
Let <span class="math inline">\(P\)</span> be a probability measure on <span class="math inline">\(\mathbb{R},\mathcal{B}\)</span><br />
The distribution function <span class="math inline">\(F:\mathbb{R}\to[0,1]\)</span> is defined by setting<br />
<span class="math display">\[F(b) = P\big((-\infty,b]\big) \text{ for all } b\in\mathbb{R}\]</span></p>
</blockquote>
<p>Note: <span class="math inline">\(P\big((a,b]\big)=F(b)-F(a)\)</span></p>
<blockquote>
<p>Thm 1.7.<br />
Let <span class="math inline">\(F:\mathbb{R}\to\mathbb{R}\)</span> be a function with the following properties:</p>
<ol style="list-style-type: lower-roman">
<li>Non-decreasing: <span class="math inline">\(F(a)\leq F(b)\)</span> for <span class="math inline">\(a&lt;b\)</span></li>
<li>Continuous from above (the right): <span class="math inline">\(F(b_n)\to F(b)\)</span> for <span class="math inline">\(b_n\searrow b\)</span></li>
<li><span class="math inline">\(\lim_{x\to -\infty}F(x)=0\)</span>, <span class="math inline">\(\lim_{x\to\infty}F(x)=1\)</span></li>
</ol>
<p>Then there exists a unique probability measure <span class="math inline">\(P\)</span> on <span class="math inline">\((\mathbb{R},B)\)</span> s.t.
<span class="math display">\[F(b) = P\big((-\infty,b]\big) \text{ for all } b\in\mathbb{R}\]</span></p>
</blockquote>
<p>Remarks:</p>
<ol style="list-style-type: lower-alpha">
<li>This says that any function <span class="math inline">\(F\)</span> with the properties uniquely characterizes a probability measure on <span class="math inline">\((\mathbb{R},B)\)</span><br />
</li>
<li>The reverse holds true too, see Thm 1.8. below</li>
</ol>
<blockquote>
<p>Thm 1.8.<br />
Any distribution function <span class="math inline">\(F\)</span> satisfies properties (i)-(iii) in Thm 1.7.</p>
</blockquote>
<p>Remarks:</p>
<ol style="list-style-type: lower-alpha">
<li>A probability measure <span class="math inline">\(P\)</span> on <span class="math inline">\((\mathbb{R},B)\)</span> is uniquely determined by the corresponding distribution function <span class="math inline">\(F\)</span> with <span class="math inline">\(F(b) = P\big((-\infty,b]\big)\)</span></li>
<li>Class of distribution fuctions = class of functions with properties (i)-(iii)</li>
</ol>
<p>Proof: (i) and (iii): exercise<br />
(ii):<br />
FSOC assume <span class="math inline">\(F:\mathbb{R}\to\mathbb{R}\)</span> is a distribution function that is not continuous from the right<br />
<span class="math inline">\(\implies\)</span> exists a sequence <span class="math inline">\(\{b_n\}\)</span>, <span class="math inline">\(b_n\searrow b\)</span> monotonically for some <span class="math inline">\(b\in\mathbb{R}\)</span> s.t. <span class="math inline">\(F(b_n)\not\to F(b)\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}1-F(b_n) &amp;= 1-P\big((-\infty, b_n]\big)\\
&amp;= P\big((b_n,\infty)\big) \\
&amp;= P\Big[ (b_1,\infty)\cup\big(\cup_{k=2}^n (b_k,b_{k-1}]\big)\Big] \\
&amp;= P\big((b_1,\infty)\big) + \textstyle{\sum}_{k=2}^n P\big((b_k,b_{k-1}]\big) \\
:&amp;= p_n\end{aligned}\]</span>
It is obvious that <span class="math inline">\(p_n\)</span> is monotonically increasing and bounded,<br />
<span class="math inline">\(\implies\)</span>
<span class="math display">\[\begin{aligned}p_n &amp;\to p \\
&amp;= P\big((b_1,\infty)\big) + \sum_{k=2}^{\infty} P\big((b_k,b_{k-1}]\big)\\
&amp;= P\Big[(b_1,\infty)\cup\big(\cup_{k=2}^{\infty}(b_k,b_{k-1}] \big)\Big] \\
&amp;= P\big((b,\infty)\big) \\
&amp;= 1 - P\big((-\infty, b]\big) \\
&amp;= 1 - F(b) \end{aligned}\]</span>
therefore, <span class="math inline">\(1 - F(b_n) = p_n\to p = 1-F(b)\)</span><br />
<span class="math inline">\(\implies 1 - F(b_n)\to 1-F(b)\)</span>, contradiction</p>
<blockquote>
<p>PS2.Q3<br />
Prove that any distribution function <span class="math inline">\(F:\mathbb{R}\to [0, 1]\)</span> has the following properties:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(F\)</span> is non-decreasing.</li>
<li><span class="math inline">\(\lim_{x\searrow −\infty}F(x)=0\)</span> and <span class="math inline">\(\lim_{x\nearrow\infty}F(x)=1\)</span>.</li>
</ol>
</blockquote>
<ol style="list-style-type: lower-roman">
<li>Let <span class="math inline">\(l&gt;l&#39;\in\mathbb{R}\)</span>,<br />
<span class="math display">\[\begin{aligned}F(l) &amp;= P((-\infty, l]) \\
F(l&#39;) &amp;= P((-\infty, l&#39;]) \\
F(l) - F(l&#39;) &amp;= P((l&#39;,l]) \geq 0\\
F(l)&amp;\geq F(l&#39;)\end{aligned}\]</span></li>
<li><ol style="list-style-type: decimal">
<li>When <span class="math inline">\(x_n\searrow -\infty\)</span><br />
<span class="math display">\[\begin{aligned}1-F(x_n) &amp;= 1-P((-\infty,x_n]) \\
&amp;= P((x_n, \infty)) \\
&amp;= P\Big(\cup_{i=2}^n (x_i,x_{i-1}]\cup (x_1,\infty)\Big) \\
&amp;= P((x_1,\infty)) + \sum_{i=2}^n P((x_i, x_{i-1}]) := p_n \\
p_n &amp;\to p \\
:&amp;= P((x_1,\infty)) + \sum_{i=2}^{\infty} P((x_i, x_{i-1}]) \\
&amp;= P\Big(\cup_{i=2}^{\infty} (x_i,x_{i-1}]\cup (x_1,\infty) \Big) \\
&amp;= P(\mathbb{R}) =1 \\
1-F(x_n) &amp;\to 1\\
F(x_n) &amp;\to 0\end{aligned}\]</span></li>
<li>When <span class="math inline">\(x_n\nearrow\infty\)</span> (similarly)</li>
</ol></li>
</ol>
<blockquote>
<p>Example: Step function
Let <span class="math inline">\(F\)</span> be a step function, <span class="math inline">\(P\)</span> be a probability function corresponds to <span class="math inline">\(F\)</span>,<br />
<span class="math inline">\(P\big(\{x_i\}\big) = b_i\)</span> with <span class="math inline">\(\sum_{i=1}^{\infty}b_i = 1\)</span><br />
Hence we can regard the sample space as being complete</p>
</blockquote>
<p>In this case, <span class="math inline">\(Ω = \{x_1,x_2,\dots\}\)</span></p>
<blockquote>
<p>Example: Absolutely continuous functions<br />
Let <span class="math inline">\(F\)</span> have the representation <span class="math inline">\(F(x) = \int_{-\infty}^x f(y)\, dy\)</span><br />
where <span class="math inline">\(f\)</span> is bounded &amp; integrable on any interval <span class="math inline">\([a,b]\)</span> and <span class="math inline">\(\lim_{a,b\to\infty} \int_a^b f(y)\, dy =1\)</span><br />
<span class="math inline">\(f\)</span> is called (probability) density function.<br />
Let <span class="math inline">\(P\)</span> be the corresponding probability measure, it holds that <span class="math inline">\(P(\{x\})=0\)</span> for all <span class="math inline">\(x\in\mathbb{R}\)</span></p>
</blockquote>
<p>Example:</p>
<ol style="list-style-type: decimal">
<li>Uniform distribution on <span class="math inline">\([a,b]\)</span><br />
<span class="math inline">\(f(x) = \frac{1}{b-a}\cdot 1\)</span>, <span class="math inline">\(x\in[a,b]\)</span><br />
then <span class="math inline">\(F(x)=\begin{cases}0 &amp; x\leq a \\ \frac{x-a}{b-a} &amp; a\leq x\leq b \\ 1 &amp; x\geq b \end{cases}\)</span><br />
</li>
<li>Normal distribution <span class="math inline">\(\mathcal{N}(μ,σ)\)</span>, <span class="math inline">\(μ\in\mathbb{R}\)</span>, <span class="math inline">\(σ&gt;0\)</span><br />
<span class="math inline">\(f_{μ,σ}(x) = \frac{1}{\sqrt{2πσ^2}}\cdot \exp\big(-\frac{1}{2}\cdot\frac{(x-μ)^2}{σ^2}\big)\)</span><br />
then <span class="math inline">\(F(x) = \int_{-\infty}^x f_{μ,σ}(y)\, dy\)</span></li>
</ol>
<blockquote>
<p>PS2.Q4<br />
Consider a probability measure <span class="math inline">\(P\)</span> on the real line <span class="math inline">\(\mathbb{R}\)</span> endowed with the Borel σ-field <span class="math inline">\(\mathcal{B}\)</span>.</p>
<ol style="list-style-type: lower-roman">
<li>Let <span class="math inline">\(g:\mathbb{R}\to \mathbb{R}\)</span> be measurable,<br />
i.e., for each Borel set <span class="math inline">\(B\)</span>, the set <span class="math inline">\(g^{−1}(B) = \{x : g(x)\in B\}\)</span> is also a Borel set.<br />
Show that for <span class="math inline">\(B\in\mathcal{B}\)</span>, <span class="math inline">\(Q(B):=P(g^{-1}(B))\)</span> is a probability measure,<br />
i.e., check that <span class="math inline">\(Q\)</span> fulfills the axioms of probability.<br />
Hint: Derive a simple expression for <span class="math inline">\(g^{−1}(\cup_{i=1}^{\infty} B_i)\)</span></li>
<li>Suppose that the distribution function <span class="math inline">\(F\)</span> of <span class="math inline">\(P\)</span> is strictly increasing and continuous, set <span class="math inline">\(g = F\)</span>.<br />
Show that with this choice, <span class="math inline">\(Q\)</span> is the uniform distribution on <span class="math inline">\([0, 1]\)</span><br />
i.e. the density of <span class="math inline">\(Q\)</span> is equal to 1 on <span class="math inline">\([0, 1]\)</span> and vanishes outside this interval.</li>
</ol>
</blockquote>
<ol style="list-style-type: lower-roman">
<li><ol style="list-style-type: decimal">
<li>Since <span class="math inline">\(P(\emptyset)=0\)</span>,
<span class="math inline">\(Q(\emptyset) = P\big(g^{-1}(\emptyset)\big)= 0\)</span></li>
<li><span class="math inline">\(Q(\mathbb{R}) = P\big(g^{-1}(\mathbb{R})\big) = 1\)</span></li>
<li>Let disjoint <span class="math inline">\(B_1,B_2,\dots\in\mathcal{B}\)</span><br />
<span class="math display">\[\begin{aligned}Q(\cup B_i) &amp;= P\big(g^{-1}(\cup B_i)\big) \\
&amp;= P\big(\{x:g(x)\in\cup B_i\}\big) \\
&amp;= P(\cup\{x:g(x)\in B_i\}) \\
&amp;= P(\cup g^{-1}(B_i)) \\
&amp;= \textstyle{\sum} P( g^{-1}(B_i)) \\
&amp;= \textstyle{\sum} Q(B_i)\end{aligned}\]</span></li>
</ol></li>
<li>Since <span class="math inline">\(F\)</span> is strictly increasing,<br />
exists <span class="math inline">\(F^{-1}:[0,1]\to \mathbb{R}\)</span> s.t.
<span class="math display">\[\begin{aligned}F\circ F^{-1}(x) &amp;= x &amp;&amp; x\in[0,1] \\
F^{-1}\circ F(x) &amp;= x &amp;&amp; x\in\mathbb{R} \end{aligned}\]</span>
Let <span class="math inline">\(x\in\mathbb{R}\)</span>, let <span class="math inline">\(G:\mathbb{R}\to[0,1]\)</span> be the density function of <span class="math inline">\(Q\)</span>,
<span class="math display">\[\begin{aligned}G(x) &amp;= Q\big((-\infty, x]\big)\\
&amp;= P\big(F^{-1}(-\infty,x]\big) \end{aligned}\]</span>
If <span class="math inline">\(x&lt;0\)</span>,
<span class="math display">\[\begin{aligned}F^{-1}\big((-\infty,x]\big) &amp;= \emptyset \\
G(x) &amp;= P(\emptyset) = 0 \end{aligned}\]</span>
If <span class="math inline">\(x=0\)</span>,
<span class="math display">\[\begin{aligned}F^{-1}\big((-\infty,x]\big) &amp;= F^{-1}(0) \\
G(x) &amp;= P\big(F^{-1}(0)\big) = 0 \end{aligned}\]</span>
If <span class="math inline">\(0&lt;x\leq 1\)</span>,
<span class="math display">\[\begin{aligned}F^{-1}\big((-\infty,x]\big) &amp;= F^{-1}\big((-\infty,0)\big)\cup F^{-1}\big([0,x]\big) \\
&amp;= F^{-1}\big((-\infty,0)\big)\cup \big[F^{-1}(0), F^{-1}(x)\big] \\
P\Big(F^{-1}\big((-\infty,x]\big)\Big) &amp;= 0 + P\Big(F^{-1}\big([0,x]\big)\Big) \\
&amp;= P\Big(\big(F^{-1}(0), F^{-1}(x)\big]\Big) \\
&amp;= F\big(F^{-1}(x)\big) - F\big(F^{-1}(0)\big) \\
&amp;= x - 0 = x \end{aligned}\]</span>
If <span class="math inline">\(x&gt;1\)</span>,
<span class="math display">\[\begin{aligned}F^{-1}\big((-\infty,x]\big) &amp;\geq F^{-1}\big((-\infty,1]\big)\\
&amp;= \mathbb{R}\\
G(x) &amp;= 1\end{aligned}\]</span>
Let <span class="math inline">\(\tilde{g}(x) = 1(x)\)</span>,
<span class="math display">\[G(x) = \int_{-\infty}^x \tilde{g}(y)\, dy\]</span>
<span class="math inline">\(\implies \tilde{g}(x)\)</span> is the density function of <span class="math inline">\(G\)</span></li>
</ol>
</div>
<div id="extension-to-rk" class="section level3">
<h3>Extension to R^k</h3>
<ul>
<li>Borel σ-field <span class="math inline">\(\mathcal{B}^k = \mathcal{B}\)</span> is defined as the smallest σ-field generated by the open cubes <span class="math inline">\((a_1,b_1)\times\dots\times (a_k,b_k)\)</span></li>
<li>Distribution function <span class="math inline">\(F:\mathbb{R}^k\to\mathbb{R}\)</span> corresponding to the probability measure <span class="math inline">\(P\)</span> is defined as
<span class="math display">\[F(b_1,\dots,b_k)=P\big((x_1,\dots,x_k) : x_1\leq b_1,\dots,x_k\leq b_k\big)\]</span></li>
<li>Absolute continuity: <span class="math inline">\(F\)</span> s.t. exists <span class="math inline">\(f:\mathbb{R}^k\to\mathbb{R}\)</span> s.t.
<span class="math display">\[F(b_1,\dots,b_k) = \int_{-\infty}^{b_1}\cdots\int_{-\infty}^{b_k}f(x_1,\dots,x_k)\, dx_1\dots dx_k\]</span></li>
</ul>
</div>
</div>
<div id="random-variables" class="section level2">
<h2>Random Variables</h2>
<ul>
<li><span class="math inline">\(\mathbb{R}^k\)</span>-valued random variable: <span class="math inline">\(X\)</span> is a random element of <span class="math inline">\(\mathbb{R}^k\)</span></li>
</ul>
<blockquote>
<p>Def 1.9. Random variable<br />
<span class="math inline">\(\mathbb{R}^k\)</span>-valued random variable <span class="math inline">\(X\)</span> is a function <span class="math inline">\(X:Ω\to\mathbb{R}^k\)</span>,<br />
where <span class="math inline">\((Ω,\mathcal{A}, P)\)</span> is a probability space, and <span class="math inline">\(X\)</span> has measurability (M):<br />
<span class="math inline">\(X^{-1}(B) = \{ω\in Ω : X(ω)\in B\}\)</span>, <span class="math inline">\(X^{-1}(B)\in\mathcal{A}\)</span> for all <span class="math inline">\(B\in\mathcal{B}^k\)</span></p>
</blockquote>
<blockquote>
<p>Def. Distribution of <span class="math inline">\(X\)</span><br />
Measurability allows to define a probability measure on <span class="math inline">\(\mathbb{R}^k\)</span> as follows:<br />
Define the probability that <span class="math inline">\(X\in B\)</span> by <span class="math inline">\(P^X(B) = Pr(X\in B)\)</span>, then<br />
<span class="math display">\[\begin{aligned}P^X(B) &amp;:= P\big(X^{-1}(B) \big) \\ &amp;\phantom{:}= P\big(\{ω:X(ω)\in B\} \big) \\ &amp;:= P(X\in B)\end{aligned}\]</span></p>
</blockquote>
<p>Remarks:</p>
<ul>
<li>Random variable: <span class="math inline">\(X: Ω\to\mathbb{R}\)</span></li>
<li>One can show that <span class="math inline">\(P^X\)</span> is a probability measure on <span class="math inline">\((\mathbb{R}^k, \mathcal{B}^k)\)</span></li>
<li><span class="math inline">\(X\)</span> is a deterministic function with (M)<br />
<span class="math inline">\(\implies\)</span> the probability structure on <span class="math inline">\(Ω\)</span> is also mapped appropreately to <span class="math inline">\(\mathbb{R}^k\)</span></li>
<li>Def. of a random variable as a deterministic function is parsimonious in the sense that it avoids introducing a new probability structure on <span class="math inline">\(\mathbb{R}^k\)</span></li>
</ul>
<blockquote>
<p>Example: Tossing a coin 3 times<br />
<span class="math display">\[Ω=\big\{ω =(ω_1, ω_2, ω_3) : ω_j\in\{0,1\}\big\}\]</span>
P: Laplace distribution on <span class="math inline">\(Ω\)</span>:
<span class="math display">\[P(A) = |A|\backslash |Ω| = |A|\backslash 8\]</span>
Define the random variable <span class="math inline">\(X:Ω\to\mathbb{R}\)</span>:
<span class="math display">\[ω =(ω_1, ω_2, ω_3) \mapsto \sum_{j=1}^3 ω_j\]</span>
therefore, <span class="math inline">\(X\in\{0,1,2,3\}\)</span><br />
<span class="math display">\[\begin{aligned}P^X\big(\{0\}\big) &amp;= P\big(\{ω\in Ω : X(ω) = 0\}\big) \\&amp;= P\big(\{0,0,0\}\big) \\&amp;=\frac{1}{8}\\
P^X\big(\{1\}\big) &amp;= P\big(\{ω\in Ω : X(ω) = 1\}\big) \\&amp;= P\big(\{1,0,0\},\{0,1,0\},\{0,0,1\}\big) \\&amp;=\frac{3}{8}\end{aligned}\]</span>
and so on</p>
</blockquote>
<blockquote>
<p>PS1.Q1<br />
Consider the experiment of tossing a dice three times.</p>
<ol style="list-style-type: lower-roman">
<li>Write down the sample space <span class="math inline">\(Ω\)</span> for this experiment.</li>
<li>Write down the event <span class="math inline">\(A_1\)</span> that “5” occurs exactly two times.</li>
<li>What is the probability of event <span class="math inline">\(A_1\)</span> assuming a Laplace distribution?</li>
</ol>
</blockquote>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Ω = \big\{(i,j,k) : i,j,k\in\{1,2,\dots,6\}\big\}\)</span></li>
<li><span class="math inline">\(A_1 = \{(5,5,i)\}\cup\{(5,i,5)\}\cup\{(i,5,5)\}\)</span> where <span class="math inline">\(i\in\{1,2,3,4,6\}\)</span></li>
<li><span class="math display">\[\begin{aligned}Pr(A_1) &amp;= |A|\backslash |Ω| \\
&amp;= \frac{15}{216}\end{aligned}\]</span></li>
</ol>
<div id="expectation" class="section level3">
<h3>Expectation</h3>
<blockquote>
<p>Def 1.10. Discrete Continuous<br />
Random variable <span class="math inline">\(X\)</span> is</p>
<ol style="list-style-type: lower-roman">
<li>Discrete if its distribution is discrete,<br />
i.e., if there exists countably many points <span class="math inline">\(x_1&lt;x_2&lt;\dots\)</span> s,t, <span class="math inline">\(P(X=x_i)=p_i\)</span>, and <span class="math inline">\(\sum_i p_i=1\)</span></li>
<li>Continuous if its distribution is absolutely continuous,<br />
i.e., if its distribution function <span class="math inline">\(F\)</span> can be written as <span class="math display">\[F(x)=\int_{-\infty}^{\infty}f(y)\, dy\]</span>
with some bounded, integrable density function <span class="math inline">\(f\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>Def 1.11. Mean Expectation<br />
The mean of <span class="math inline">\(X\)</span> is defined as</p>
<ol style="list-style-type: lower-roman">
<li>Provided that <span class="math inline">\(\sum_{i=1}^{\infty} p_i\cdot x_i&lt;\infty\)</span>,
<span class="math display">\[\begin{aligned}E[X] &amp;= \textstyle{\sum}_{i=1}^{\infty} p_i\cdot x_i\\ &amp;= \textstyle{\sum}_{i=1}^{\infty} P(X=x_i)\cdot x_i \end{aligned}\]</span></li>
<li>Provided that <span class="math inline">\(\int |x|\cdot f(x)\, dx&lt;\infty\)</span>,
<span class="math display">\[E[X] = \int_{-\infty}^{\infty} x\cdot f(x)\, dx\]</span></li>
</ol>
</blockquote>
<p>Note: the finite bind is there s.t. <span class="math inline">\(E[X]\)</span> is well-defined and finite</p>
<blockquote>
<p>Example</p>
<ol style="list-style-type: lower-alpha">
<li>Expected gain of roulette: bet 1 on “odd”<br />
<span class="math inline">\(Ω = \{0,1,2,\dots, 36\}\)</span><br />
<span class="math inline">\(P\)</span> Laplace distribution<br />
<span class="math inline">\(X(ω)=\begin{cases}1 &amp; \text{odd} \\ -1 \end{cases}\)</span> gain<br />
<span class="math display">\[\begin{aligned}P(X=1) &amp;= \frac{18}{37} \\ P(X=-1) &amp;= \frac{19}{37} \\ E[X] &amp;= 1\cdot\frac{18}{37} + (-1)\cdot\frac{19}{37} &amp;= -\frac{1}{37} \end{aligned}\]</span></li>
<li>Normal distribution<br />
$$\begin{aligned}f(x) &amp;=(-) \
E[X] &amp;= <em>{-}^{} xf(x), dx\
&amp;= </em>{-}^{} (y+μ)(-), dy \
&amp;= _{=0}</li>
</ol>
<ul>
<li>μ_{=1} \
&amp;= μ\end{aligned}$$</li>
</ul>
</blockquote>
<blockquote>
<p>Thm 1.12. Prepulis of Expectation<br />
Let <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> be real-valued random variables, <span class="math inline">\(a,b\in\mathbb{R}\)</span>, then</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(|E[X_1]|\leq\sup_{ω\in Ω} |X_(ω)|\)</span></li>
<li><span class="math inline">\(E[aX_1 + bX_2] = a\cdot E[X_1] + b\cdot E[X_2]\)</span></li>
<li><span class="math inline">\(E[X_1]\leq E[X_2]\)</span> if <span class="math inline">\(X_1(ω)\leq X_2(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li><span class="math inline">\(E[X_1X_2] = E[X_1]\cdot E[X_2]\)</span> if <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span> are independent</li>
</ol>
</blockquote>
<blockquote>
<p>Prop 1.13.<br />
Let <span class="math inline">\(g:\mathbb{R}\to\mathbb{R}\)</span> be bounded integrable function, then<br />
<span class="math display">\[E[g(X)] = \int_{-\infty}^{\infty} g(x)\cdot f(x)\, dx\]</span></p>
</blockquote>
<blockquote>
<p>PS3.Q1<br />
A function <span class="math inline">\(f:\mathbb{R}\to[0,\infty)\)</span> is given by<br />
<span class="math display">\[f(x)=\begin{cases}c(x+2) &amp; -2\leq x&lt;0 \\ c(2-x) &amp; 0\leq x&lt;2 \\ 0 \end{cases}\]</span>
Chose <span class="math inline">\(c\)</span> s.t. <span class="math inline">\(\int f(x)\, dx = 1\)</span></p>
</blockquote>
<p><span class="math display">\[\begin{aligned}4c &amp;= \int f(x)\, dx \\
c&amp;=1/4 \end{aligned}\]</span></p>
</div>
<div id="variance" class="section level3">
<h3>Variance</h3>
<blockquote>
<p>Def 1.14 Variance &amp; SD<br />
Let <span class="math inline">\(X\)</span> be <span class="math inline">\(E[X^2]&lt;\infty\)</span>, the variance and standard deviation of <span class="math inline">\(X\)</span> are defined as
<span class="math display">\[\begin{aligned}Var(X) &amp;= E\big[(X - E[X])^2\big] \\ σ&amp;= \sqrt{Var(X)} \end{aligned}\]</span></p>
</blockquote>
<p>Variance is a measure for the dispersion of the distribution of <span class="math inline">\(X\)</span> around its expected value</p>
<blockquote>
<p>Prop 1.15.<br />
Let <span class="math inline">\(X\)</span> be <span class="math inline">\(E[X^2]&lt;\infty\)</span> and <span class="math inline">\(a,b\in\mathbb{R}\)</span>, then</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Var(x)=E[X^2] - \big(E[X]\big)^2\)</span></li>
<li><span class="math inline">\(Var(aX+b)=a^2\cdot Var(X)\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>Prop 1.16.<br />
If <span class="math inline">\(X_1,X_2,\dots,X_n\)</span> are independent with finite variance, then
<span class="math display">\[Var(X_1+\dots X_n) = Var(X_1) +\dots + Var(X_n)\]</span></p>
</blockquote>
<p>Proof:<br />
Since <span class="math inline">\(Var(x)=E[X^2] - \big(E[X]\big)^2\)</span>, assume <span class="math inline">\(E[X_i]=0\)</span> for all <span class="math inline">\(i\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}Var\big(\textstyle{\sum}_{i=1} X_i\big) &amp;= E\Big[\big(\textstyle{\sum}_{i=1} X_i\big) \Big] \\
&amp;= \textstyle{\sum}_{i,j=1}^n E[X_iX_j] \\
&amp;= \textstyle{\sum}_{i=1}^n E[X_i^2] + \underbrace{\textstyle{\sum}_{i\neq j} E[X_iX_j]}_{=0} \\
&amp;= \textstyle{\sum}_{i=1}^n E[X_i^2] \\
&amp;= \textstyle{\sum}_{i=1}^n Var(X_i) \end{aligned}\]</span></p>
<blockquote>
<p>PS3.Q2<br />
Let <span class="math inline">\(X\)</span> be a real valued random variable with the density
<span class="math display">\[f(x)=\frac{1}{\sqrt{2πσ^2}}\exp\big(-\frac{(x-μ)^2}{σ^2} \big)\]</span>
Show that <span class="math inline">\(μ = E[X]\)</span> and <span class="math inline">\(σ^2 = Var(X)\)</span></p>
</blockquote>
<p>We already know <span class="math inline">\(μ = E[X]\)</span>
<span class="math display">\[\begin{aligned}E[X] &amp;= \int x^2 f(x)\, dx \\
&amp;= \int x^2\cdot\frac{1}{\sqrt{2πσ^2}}\exp\big(-\frac{(x-μ)^2}{σ^2} \big)\, dx \\
&amp;= 2μ\cdot\underbrace{\int xf(x)\, dx}_{=μ}-μ^2\cdot\underbrace{\int f(x)\, dx}_{=1} + \int (x-μ)^2\cdot\frac{1}{\sqrt{2πσ^2}}\exp\big(-\frac{(x-μ)^2}{σ^2} \big)\, dx \\
&amp;= μ^2 + \int y^2\cdot\frac{1}{\sqrt{2πσ^2}}\exp\big(-\frac{y^2}{σ^2} \big)\, dy \\
&amp;= μ^2 + σ^2\end{aligned}\]</span>
Since <span class="math inline">\(Var(x) &amp;= E[X^2] - (E[X])^2\)</span>, …</p>
</div>
<div id="moments-of-rk" class="section level3">
<h3>Moments of R^k</h3>
<ul>
<li><span class="math inline">\(\mathbb{R}^k\)</span>-valued random variable:
<span class="math display">\[X = \begin{pmatrix}X_1\\\vdots\\X_k\end{pmatrix}\]</span></li>
<li>Expectation:
<span class="math display">\[E[X] = \begin{pmatrix}E[X_1]\\\vdots\\E[X_k]\end{pmatrix}\]</span></li>
<li>Covariance matrix:
<span class="math display">\[Σ=\begin{pmatrix}E\big[(X_1-μ_1)^2\big] &amp; E\big[(X_1-μ_1)(X_2-μ_2)\big] &amp; \cdots\\\vdots &amp;\ddots &amp;\vdots\\ E\big[(X_k-μ_k)(X_1-μ_1)\big] &amp; \cdots &amp; E\big[(X_k-μ_k)^2\big]\end{pmatrix}\]</span></li>
</ul>
<blockquote>
<p>Def 1.17. Moments<br />
The s-th moment of <span class="math inline">\(X\)</span> is defined as <span class="math inline">\(E[X^s]\)</span><br />
The absolute s-th moment is defined as <span class="math inline">\(E[|X|^s]\)</span><br />
The s-th central moment is defined as <span class="math inline">\(E\big[(X-E[X])^s\big]\)</span></p>
</blockquote>
</div>
</div>
<div id="stochastic-convergence" class="section level2">
<h2>Stochastic Convergence</h2>
<div class="figure">
<img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/46c94668666d71613847f157ca27de5b4e938023" alt="" />
<p class="caption">Source: <a href="https://zh.wikipedia.org/wiki/%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F%E7%9A%84%E6%94%B6%E6%95%9B">wiki</a></p>
</div>
<p>a.s. =&gt; P =&gt; d =&gt; Op(1)<br />
p-th =&gt;</p>
<div id="convergence-in-expectations-almost-sure-convergence" class="section level3">
<h3>Convergence in Expectations &amp; Almost Sure Convergence</h3>
<ul>
<li><span class="math inline">\(X\)</span> is a real-valued random variable</li>
<li><span class="math inline">\(\{X_n\}\)</span> a sequence of real valued random variables s.t. <span class="math inline">\(X_n(ω)\to X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li>Probability space: <span class="math inline">\((Ω,\mathcal{A},P)\)</span>
<ul>
<li><span class="math inline">\(X\)</span>, <span class="math inline">\(X_n: Ω\to\mathbb{R}\)</span></li>
</ul></li>
<li>Sequence <span class="math inline">\(\{a_n\}\)</span> of real numbers converges to <span class="math inline">\(a\)</span><br />
<span class="math inline">\(\iff\)</span> for all <span class="math inline">\(ε&gt;0\)</span>, exists <span class="math inline">\(N=N(ε)\)</span> s.t. <span class="math inline">\(|a_n-a|\leq ε\)</span> for all <span class="math inline">\(n\geq N\)</span></li>
</ul>
<blockquote>
<p>Does <span class="math inline">\(X_n(ω)\to X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span> implies <span class="math inline">\(E[X_n]\to E[X]\)</span>?</p>
</blockquote>
<p>No.<br />
Proof: exercise</p>
<blockquote>
<p>Thm 1.18. Monotone Convergence Theorem<br />
<span class="math inline">\(E[X_n]\to E[X]\)</span> if for all <span class="math inline">\(ω\in Ω\)</span>,</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n(ω)\to X(ω)\)</span></li>
<li><span class="math inline">\(0\leq X_n(ω)\leq X_{n+1}(ω)\)</span> for all <span class="math inline">\(n\geq 1\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>Thm 1.19. Dominated Convergence Theorem<br />
<span class="math inline">\(E[X_n]\to E[X]\)</span> if for all <span class="math inline">\(ω\in Ω\)</span>,</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n(ω)\to X(ω)\)</span></li>
<li>For some random variable <span class="math inline">\(Y\)</span> s.t. <span class="math inline">\(E[Y]&lt;\infty\)</span>,<br />
<span class="math inline">\(|X_n(ω)|\leq Y(ω)\)</span> for all <span class="math inline">\(n\geq 1\)</span></li>
</ol>
</blockquote>
<p>Remarks: Conditions in Thm 1.18. and 1.19. can be weakened to hold only almost surely:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n\to X\)</span> almost surely<br />
<span class="math inline">\(\iff\)</span> <span class="math display">\[P(X_m\to X) = P\big(\{ω\in Ω : X_n(ω)\to X(ω)\}\big) = 1\]</span></li>
<li><span class="math inline">\(0\leq X_n(ω)\leq X_{n+1}(ω)\)</span> for <span class="math inline">\(n\geq 1\)</span> almost surely<br />
<span class="math inline">\(\iff\)</span>
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg}P(0\leq X_n(ω)\leq X_{n+1}(ω)\text{ for }n\geq 1)
\\&amp;= P\big(\{ω\in Ω : 0\leq X_n(ω)\leq X_{n+1}(ω)\text{ for }n\geq 1\}\big) = 1\end{aligned}\]</span></li>
<li><span class="math inline">\(|X_n(ω)|\leq Y(ω)\)</span> for <span class="math inline">\(n\geq 1\)</span> almost surely<br />
<span class="math inline">\(\iff\)</span>
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg}P(|X_n(ω)|\leq Y(ω)\text{ for }n\geq 1)
\\&amp;= P\big(\{ω\in Ω : |X_n(ω)|\leq Y(ω)\text{ for }n\geq 1\}\big) = 1\end{aligned}\]</span></li>
</ol>
<blockquote>
<p>Def<br />
Property <span class="math inline">\(Q\)</span> holds almost surely (a.s.)<br />
<span class="math inline">\(\iff Pr(Q)=1\)</span></p>
</blockquote>
<blockquote>
<p>Examples</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(\{X_n\}\)</span> be a sequence of random variables s.t.<br />
<span class="math inline">\(|X_n|\leq 1\)</span> and <span class="math inline">\(X_n\to 0\)</span> almost surely<br />
Then Thm 1.19. <span class="math inline">\(\implies E[X_n]\to 0\)</span></li>
<li>Let <span class="math inline">\(X\geq 0\)</span> s.t. <span class="math inline">\(E[X]&lt;\infty\)</span><br />
Let <span class="math inline">\(\{A_n\}\)</span> be a sequence of sets with <span class="math inline">\(A_n\nearrow A\)</span><br />
Define <span class="math inline">\(1_A(x) =\begin{cases}1 &amp; x\in A\\ 0&amp;x\not\in A \end{cases}\)</span><br />
<span class="math inline">\(\implies 1_{A_n}(x)\to 1_A(x)\)</span> for all <span class="math inline">\(x\)</span><br />
Therefore, by Thm 1.18. or 1.19., <span class="math inline">\(E\big[1_{A_n}(X)\cdot X\big]\to E\big[1_{A}(X)\cdot X\big]\)</span> because:
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(1_{A_n}\big(X(ω)\big)\cdot X(ω)\to 1_{A}\big(X(ω)\big)\cdot X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li><span class="math inline">\(1_{A_n}\big(X(ω)\big)\cdot X(ω)\leq 1_{A_{n+1}}\big(X(ω)\big)\cdot X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li><span class="math inline">\(1_{A_n}\big(X(ω)\big)\cdot X(ω)\leq Y(ω):= X(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
</ol></li>
<li>Let <span class="math inline">\(X: Ω\to\mathbb{R}\)</span>, and <span class="math inline">\(f:\mathbb{R}\to\mathbb{R}\)</span> continuously differentiable and
<span class="math display">\[f_n(x) = \frac{f(x+\frac{1}{n}) - f(x)}{\frac{1}{n}}\]</span>
Therefore, by Thm 1.19, <span class="math inline">\(E\big[f_n(X)\big]\to E\big[f(X)\big]\)</span> because:
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(f_n\big(X(ω)\big)\to f&#39;\big(X(ω)\big)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
<li>If <span class="math inline">\(|f&#39;|\leq C\)</span>, then <span class="math inline">\(|f_n\big(X(ω)\big)|\leq C=: Y(ω)\)</span> for all <span class="math inline">\(ω\in Ω\)</span></li>
</ol></li>
</ol>
<table>
<thead>
<tr class="header">
<th>Theorem</th>
<th>Example 1</th>
<th>Example 2</th>
<th>Example 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(X\)</span></td>
<td><span class="math inline">\(0\)</span></td>
<td><span class="math inline">\(1_{A}(X)\cdot X\)</span></td>
<td><span class="math inline">\(f(X)\)</span></td>
</tr>
<tr class="even">
<td><span class="math inline">\(X_n\)</span></td>
<td><span class="math inline">\(X_n\)</span></td>
<td><span class="math inline">\(1_{A_n}(X)\cdot X\)</span></td>
<td><span class="math inline">\(f_n(X)\)</span></td>
</tr>
<tr class="odd">
<td><span class="math inline">\(Y\)</span></td>
<td><span class="math inline">\(1\)</span></td>
<td><span class="math inline">\(X\)</span></td>
<td><span class="math inline">\(C\)</span></td>
</tr>
</tbody>
</table>
<blockquote>
<p>PS3.Q3<br />
Construct a probability space <span class="math inline">\((Ω, \mathcal{A}, P)\)</span> and a sequence of random variables <span class="math inline">\(\{X_n : n\in N\}\)</span> with the following property:<br />
<span class="math inline">\(X_n(ω)\to 0\)</span> for all <span class="math inline">\(ω\in Ω\)</span> but <span class="math inline">\(E[X_n]\not\to 0\)</span></p>
</blockquote>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\((Ω, \mathcal{A}, P)=(\mathbb{N}, P(\mathbb{N}),P)\)</span><br />
<span class="math inline">\(P(A)=\sum p(n)\)</span> for all <span class="math inline">\(A\in\mathcal{A}\)</span>, <span class="math inline">\(p(n)=2^{-n}\)</span><br />
<span class="math display">\[X_n(ω)=\begin{cases}2^n &amp; ω\in\mathbb{N}\backslash\{1,\dots,n\}\\ \frac{1}{n} &amp; ω\in\{1,\dots,n\} \end{cases}\]</span>
Therefore, <span class="math inline">\(X_n\to 0\)</span> pointwise, but
<span class="math display">\[\begin{aligned}E[X_n] &amp;= \textstyle{\sum}^{\infty} p(i)X_n(i)\\
&amp;= \textstyle{\sum}^n_{i=1} p(i)X_n(i) + \textstyle{\sum}_{i=n+1}^{\infty} p(i)X_n(i)\\
&amp;= \frac{1}{n}\cdot\textstyle{\sum}^n_{i=1}2^{-i} + 2^n\cdot\textstyle{\sum}_{i=n+1}^{\infty}2^{-i} \\
&amp;= \underbrace{\frac{1}{n}\cdot\textstyle{\sum}^n_{i=1}2^{-n}}_{\to 0} + 1 \to 1 \end{aligned}\]</span></li>
<li>Let <span class="math inline">\((Ω, \mathcal{A}, P)=([0,1], \mathcal{B},\text{uniform dist.})\)</span><br />
<span class="math display">\[X_n(ω)=\begin{cases}0 &amp; ω=0 \\ n^2 &amp; 0&lt;ω&lt;\frac{1}{n} \\ 0 &amp; \frac{1}{n}\leq ω\leq 1 \end{cases}\]</span>
Therefore, <span class="math inline">\(X_n(ω)\to X(ω)= 0\)</span> pointwise for all <span class="math inline">\(ω\in[0,1]\)</span>, but
<span class="math display">\[\begin{aligned}E[X_n] &amp;= \frac{1}{n} \cdot n^2 \\ &amp;= n\to\infty \end{aligned}\]</span></li>
</ol>
</div>
<div id="convergence-in-probability" class="section level3">
<h3>Convergence in Probability</h3>
<ul>
<li><span class="math inline">\(\{X_n\}\)</span>: sequence of random variables valued in <span class="math inline">\((\mathbb{R}^k, \mathcal{B}^k)\)</span></li>
<li><span class="math inline">\(X\)</span>: random variable valued in <span class="math inline">\(\mathbb{R}^k\)</span></li>
<li><span class="math inline">\((Ω,\mathcal{A},P)\)</span>: probability space
<ul>
<li><span class="math inline">\(\|\cdot\|\)</span>: some norm on <span class="math inline">\(\mathbb{R}^k\)</span></li>
</ul></li>
</ul>
<blockquote>
<p>Def 2.1.</p>
<ol style="list-style-type: lower-roman">
<li>Convergence in probability: <span class="math inline">\(X_n\overset{P}{\to}X\)</span>, or <span class="math inline">\(X_n=X+o_p(1)\)</span> if for any <span class="math inline">\(ε&gt;0\)</span>,
<span class="math display">\[\underbrace{P\big(\| X_n - X \|&gt;ε\big)}_{:= a_n(ε)}\to 0\]</span>
in other words,<br />
for all <span class="math inline">\(d&gt;0\)</span>, exists <span class="math inline">\(N\in\mathbb{N}\)</span> s.t. <span class="math inline">\(a_n(ε)\leq d\)</span> for all <span class="math inline">\(n\geq N\)</span></li>
<li>Almost sure convergence: <span class="math inline">\(X_n\overset{a.s.}{\to}X\)</span> if for any <span class="math inline">\(ε&gt;0\)</span>,
<span class="math display">\[P(X_n\to X)=1\]</span>
in other words, <span class="math inline">\(P\big(\{ω\in Ω: X_n(ω)\to X(ω)\}\big) = 1\)</span></li>
<li>Convergence in p-th mean: <span class="math inline">\(X_n\overset{L_p}{\to}X\)</span> if<br />
<span class="math display">\[E\big[\|X_n - X\|^p\big]\to 0\]</span></li>
</ol>
</blockquote>
</div>
<div id="convergence-in-probability-vs.-almost-convergence" class="section level3">
<h3>Convergence in Probability vs. Almost Convergence</h3>
<p>Reference: <a href="https://www.probabilitycourse.com/chapter7/7_2_7_almost_sure_convergence.php">Almost Sure Convergence</a></p>
<blockquote>
<p>Convergence in probability does not imply almost convergence</p>
</blockquote>
<p>Counterexample:<br />
Let <span class="math inline">\((Ω,\mathcal{A},P) = ([0,1], \mathcal{B}, \text{uniform distribution})\)</span><br />
Let <span class="math inline">\(ω\in[0,1]\)</span> and define
<span class="math display">\[\begin{aligned}X_1(ω) &amp;= 1 \\
X_2(ω) &amp;=\begin{cases}1 &amp; 0\leq ω\leq \frac{1}{2} \\ 0 \end{cases} \\
X_3(ω) &amp;=\begin{cases}1 &amp; \frac{1}{2}\leq ω\leq \frac{1}{2}+\frac{1}{3} \\ 0 \end{cases} \\
X_4(ω) &amp;=\begin{cases}1 &amp; \frac{5}{6}\leq ω\leq 1 \text{ or } 0\leq ω\leq \frac{1}{4} - \frac{1}{6} \\ 0 \end{cases} \\
X_5(ω) &amp;=\begin{cases}1 &amp; \frac{1}{12}\leq ω\leq \frac{1}{12}+\frac{1}{5} \\ 0 \end{cases} \\
&amp;\dots\end{aligned}\]</span></p>
<p>Conclusions:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n\overset{P}{\to} X \equiv 0\)</span><br />
Since for <span class="math inline">\(0&lt;ε&lt;1\)</span>,
<span class="math display">\[\begin{aligned}P(|X_n-X|&gt;ε) &amp;= P(|X_n|&gt;ε)\\ &amp;= P(X_n=1) \\ &amp;= \frac{1}{n} \to 0 \end{aligned}\]</span></li>
<li><span class="math inline">\(X_n\overset{a.s.}{\not\to} X \equiv 0\)</span><br />
Since <span class="math inline">\(\sum_{n=1}^{\infty} \frac{1}{n}=\infty\)</span>,<br />
<span class="math display">\[\begin{aligned}P(X_n\to X) &amp;= P(X_n\to 0)\\ &amp;= P\big(\{ω\in Ω : X_n(ω)\to 0\}\big) = 0 \end{aligned}\]</span>
in other words, if we find <span class="math inline">\(N\)</span> s.t. <span class="math inline">\(X_N(ω)=1\)</span>, there is always <span class="math inline">\(N&#39;&gt;N\)</span> s.t. <span class="math inline">\(X_{N&#39;}(ω)=1\)</span></li>
</ol>
<blockquote>
<p>Thm 2.2. Almost convergence implies convergence in probability<br />
<span class="math inline">\(X_n\overset{a.s.}{\to} X\implies X_n\overset{P}{\to} X\)</span></p>
</blockquote>
<p>Proof:<br />
Let <span class="math inline">\(ε&gt;0\)</span> and <span class="math inline">\(z_n = 1\big(\|X_n-X\|&gt;ε \big)\)</span>
Therefore, <span class="math inline">\(X_n(ω)\to X(ω)\implies\)</span> <span class="math inline">\(1\big(\|X_n(ω)-X(ω)\|&gt;ε \big)\to 0\)</span><br />
Therefore, <span class="math inline">\(X_n\overset{a.s.}{\to} X\implies z_n\overset{a.s.}{\to} 0\)</span><br />
Let <span class="math inline">\(Y:= 1\)</span>, <span class="math inline">\(|z_n|\leq 1 = Y\)</span><br />
By Thm 1.19., <span class="math inline">\(E[z_n]\to 0\)</span>
<span class="math inline">\(\implies\)</span>
<span class="math display">\[\begin{aligned}P\big(\|X_n-X\|&gt;ε \big) &amp;= E\Big[1\big(\|X_n-X\|&gt;ε \big) \Big]\\
&amp;= E[z_n]\to 0 \end{aligned}\]</span>
therefore, <span class="math inline">\(X_n\overset{P}{\to} X\)</span></p>
<blockquote>
<p>PS2.Q5<br />
Are there real valued random variable <span class="math inline">\(X\)</span> s.t. <span class="math inline">\(\big|E[X]\big|&lt;\infty\)</span> but <span class="math inline">\(E\big[|X|\big]=\infty\)</span>?</p>
</blockquote>
<p><span class="math display">\[\begin{aligned}X &amp;= \underbrace{X\cdot 1(X&gt; 0)}_{X^+} + \underbrace{X\cdot 1(X\leq 0)}_{X^-}\\
E[X] &amp;= E[X^+ + X^-] \\
&amp;= E[X^+] + E[X^-] \end{aligned}\]</span>
<span class="math inline">\(\big|E[X]\big|&lt;\infty\implies \big|E[X^+]\big|&lt;\infty\)</span> and <span class="math inline">\(\big|E[X^-]\big|&lt;\infty\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned} E\big[|X|\big] &amp;= E[X^+] - E[X^-] \\
&amp;&lt; \infty\end{aligned}\]</span></p>
</div>
<div id="convergence-in-probability-vs.-covergence-in-p-th-mean" class="section level3">
<h3>Convergence in Probability vs. Covergence in p-th Mean</h3>
<blockquote>
<p>Thm 2.3. Markov Inequality<br />
Let <span class="math inline">\(X\)</span> be a random variable and <span class="math inline">\(g:[0,\infty)\to[0,\infty)\)</span> a monototnic increasing function<br />
Then for any <span class="math inline">\(ε&gt;0\)</span>,
<span class="math display">\[P\big(\|X\|\geq ε\big)\leq\frac{E\big[g(\|X\|)\big]}{g(ε)}\]</span></p>
</blockquote>
<p>Proof:<br />
Since <span class="math inline">\(1(\|X\|\geq ε)\leq \frac{g(\|X\|)}{g(ε)}\)</span>,
<span class="math display">\[\begin{aligned}P\big(\|X\|\geq ε\big) &amp;= E\big[1(\|X\|\geq ε)\big] \\
&amp;\leq E\Big[\frac{g(\|X\|)}{g(ε)}\Big] \\
&amp;= \frac{E\big[g(\|X\|)\big]}{g(ε)} \end{aligned}\]</span></p>
<blockquote>
<p>Corollary 2.4. Chebyshev Inequality<br />
For a real-valued random variabe <span class="math inline">\(X\)</span> and <span class="math inline">\(ε&gt;0\)</span>,
<span class="math display">\[P\big(|X|\geq ε\big)\leq\frac{E[X^2]}{ε^2}\]</span></p>
</blockquote>
<p>Proof: Apply Thm 2.3. with <span class="math inline">\(g(x)=x^2\)</span></p>
<blockquote>
<p>Thm 2.5. Covergence in p-th mean implies convergence in probability<br />
<span class="math inline">\(X_n\to X\)</span> in p-th mean <span class="math inline">\(\implies X_n\overset{P}{\to}X\)</span></p>
</blockquote>
<p>Proof:<br />
Apply Thm 2.3. with <span class="math inline">\(g(x)=x^P\)</span><br />
Since <span class="math inline">\(E[\|X_n-X\|^P]\to 0\)</span><br />
<span class="math display">\[P\big(\|X_n-X\|&gt; ε\big) \leq \frac{E\big[\|X_n-X\|^P\big]}{ε^p}\to 0\]</span>
<span class="math inline">\(\implies X_n\overset{P}{\to}X\)</span></p>
<blockquote>
<p>Thm. 2.6. Weak law of large numbers, WLLN<br />
Let <span class="math inline">\(X_1, X_2,\dots\)</span> be a sequence of random variables with the following properties, then
<span class="math display">\[\begin{aligned}\bar{X}_n &amp;:= \frac{1}{n}\sum_{i=1}^n X_i \\ &amp;\overset{P}{\to} μ \end{aligned}\]</span></p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_1, X_2,\dots\)</span> are uncorrelated, i.e., for <span class="math inline">\(i\neq j\)</span>,
<span class="math display">\[Cov(X_i,X_j)=E[X_iX_j] - E[X_i]\cdot E[X_j]=0\]</span></li>
<li><span class="math inline">\(E[X_1]=E[X_2]=\dots = μ\)</span></li>
<li><span class="math inline">\(Var(Χ_i)&lt;σ^2\)</span> for all <span class="math inline">\(i\)</span></li>
</ol>
</blockquote>
<p>Proof:<br />
:) <span class="math inline">\(\bar{X}_n\overset{P}{\to} μ\)</span><br />
Therefore, consider
<span class="math display">\[\begin{aligned}E\big[(\bar{X}_n-μ)^2\big] &amp;= Var(\bar{X}_n)\\
&amp;= Var\Big(\frac{1}{n}\textstyle{\sum}_{i=1}^n X_i \Big) \\
&amp;= \frac{1}{n^2}\cdot Var\big(\textstyle{\sum}_{i=1}^n X_i \big) \\
&amp;= \frac{1}{n^2}\cdot \textstyle{\sum}_{i=1}^n Var(X_i) \\
&amp;\leq \frac{σ^2}{n}\end{aligned}\]</span>
By Cor. 2.4.,
<span class="math display">\[\begin{aligned}P\big(\|\bar{X}_n-μ\|&gt; ε\big) &amp;\leq \frac{E\big[(\bar{X}_n-μ)^2\big]}{ε^2} \\
&amp;\leq \frac{σ^2}{n\cdot ε^2} \to 0 \end{aligned}\]</span></p>
</div>
<div id="convergence-in-distribution" class="section level3">
<h3>Convergence in Distribution</h3>
<ul>
<li><span class="math inline">\(X_n, X\)</span>: <span class="math inline">\(\mathbb{R}^k\)</span> valued random variables
<ul>
<li>not necessarily in the same prob. space</li>
</ul></li>
</ul>
<blockquote>
<p>Def 2.7. Convergence in distribution<br />
<span class="math inline">\(X_n\overset{d}{\to} X\)</span> or <span class="math inline">\(X_n\overset{\mathcal{L}}{\to} X\iff\)</span> for all <span class="math inline">\(f:\mathbb{R}^k\to\mathbb{R}\)</span> continuous and bounded
<span class="math display">\[E\big[f(X_n)\big]\to E\big[f(X)\big]\]</span></p>
</blockquote>
<blockquote>
<p>Thm 2.8.<br />
Let <span class="math inline">\(X_n, X\)</span> be real-valued with distribution functions <span class="math inline">\(F_n,F\)</span><br />
<span class="math inline">\(X_n\overset{d}{\to} X \iff F_n(x)\to F(x)\)</span> at all continuity points <span class="math inline">\(x\)</span> of <span class="math inline">\(F\)</span></p>
</blockquote>
<p>Example:<br />
<span class="math inline">\(X_n = \frac{1}{n}\to X=0\)</span> with <span class="math inline">\(F_n(x) = \begin{cases}1 &amp; x\geq\frac{1}{n} \\ 0 \end{cases}\)</span> and <span class="math inline">\(F(x) = \begin{cases}1 &amp; x\geq 0\\ 0 \end{cases}\)</span><br />
A reasonable definititon of convergence in distribution should include this case!<br />
We know <span class="math inline">\(F_n(x)\to F(x)\)</span> for all continuity points: <span class="math inline">\(x\neq 0\)</span><br />
Consider <span class="math inline">\(x=0\)</span>, <span class="math inline">\(F_n(0)=0\)</span> for all <span class="math inline">\(n\)</span>, and <span class="math inline">\(F(0)=1\)</span><br />
therefore, <span class="math inline">\(F_n(0)\not\to F(0)\)</span></p>
<p>Outline of Proof:<br />
(<span class="math inline">\(\implies\)</span>)<br />
Let <span class="math inline">\(g(y) = 1(y\leq x):=\begin{cases}1 &amp; y\leq x \\ 0 \end{cases}\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}F_n(x) &amp;= P(X_n\leq x)\\
&amp;= E\big[1(X_n\leq x)\big] \\
&amp;= E\big[g(X_n)\big]\end{aligned}\]</span>
Define continuous versions of <span class="math inline">\(g(y)\)</span>: <span class="math inline">\(g_δ^{-}\)</span> and <span class="math inline">\(g_δ^{+}\)</span><br />
Since <span class="math inline">\(g_δ^{-}\)</span> and <span class="math inline">\(g_δ^{+}\)</span> are continuous and bounded, <span class="math inline">\(X_n\overset{d}{\to} X\)</span>,<br />
By Def 2.7., we have
<span class="math display">\[\begin{aligned}E\big[g_δ^{-}(X_n)\big]&amp;\to E\big[g_δ^{-}(X)\big] \\
E\big[g_δ^{+}(X_n)\big]&amp;\to E\big[g_δ^{+}(X)\big]\end{aligned}\]</span>
therefore,
<span class="math display">\[\begin{aligned}E\big[g_δ^{-}(X_n)\big] \leq E\big[g(X_n)\big] &amp;= F_n(X) \leq E\big[g_δ^{+}(X_n)\big] \\
E\big[g_δ^{-}(X)\big] \leq E\big[g(X)\big] &amp;= F(X) \leq E\big[g_δ^{+}(X)\big] \end{aligned}\]</span>
We know
<span class="math display">\[\begin{aligned}F(x-δ) = E\big[1(X\leq x-δ) \big] &amp;\leq E\big[g_δ^{-}(X)\big]\\
&amp;\leq  E\big[g_δ^{+}(X)\big] \\
&amp;\leq E\big[1(X\leq x+δ) \big] = F(x+δ) \end{aligned}\]</span>
If <span class="math inline">\(F\)</span> is continuous at <span class="math inline">\(x\)</span>, then <span class="math inline">\(F(x+δ)-F(x-δ)\to 0\)</span> as <span class="math inline">\(δ\to 0\)</span><br />
therefore, we have <span class="math inline">\(E\big[g(X_n)\big]\to E\big[g(X)\big]\)</span><br />
<span class="math inline">\(\implies F_n(X)\to F(X)\)</span> at any continuity point of <span class="math inline">\(F\)</span></p>
<blockquote>
<p>Thm 2.9. Contunuous mapping theorem<br />
Let <span class="math inline">\(X_n, X\)</span> be <span class="math inline">\(\mathbb{R}^k\)</span> valued random variables, assume that <span class="math inline">\(X_n\overset{d}{\to}X\)</span><br />
Let <span class="math inline">\(f:\mathbb{R}^k\to\mathbb{R}^l\)</span> be continuous, then
<span class="math display">\[f(X_n)\overset{d}{\to} f(X)\]</span></p>
</blockquote>
<p>Proof:<br />
:) <span class="math inline">\(E\big[g(f(X_n)) \big]\to E\big[g(f(X)) \big]\)</span> for all <span class="math inline">\(g\)</span> continuous and bounded<br />
We know <span class="math inline">\(E\big[h(X_n) \big]\to E\big[h(X) \big]\)</span> for all <span class="math inline">\(h\)</span> continuous and bounded<br />
Since <span class="math inline">\(g\circ f\)</span> is bounded and continuous, <span class="math inline">\(h\implies g\circ f\)</span></p>
<blockquote>
<p>Thm 2.10. Cramér-Wold device<br />
Let <span class="math inline">\(X_n, X\)</span> be <span class="math inline">\(\mathbb{R}^k\)</span> valued random variables,<br />
<span class="math inline">\(X_n \overset{d}{\to}X \iff a^T X_n\overset{d}{\to} a^T X\)</span> for all <span class="math inline">\(a\in\mathbb{R}^k\)</span></p>
</blockquote>
<p>Proof: by characteristic functions</p>
</div>
<div id="convergence-in-distribution-vs.-convergence-in-probability" class="section level3">
<h3>Convergence in Distribution vs. Convergence in Probability</h3>
<ul>
<li>Convergence in distribution is weaker than convergence in probability</li>
</ul>
<blockquote>
<p>Example<br />
Let <span class="math inline">\(X\sim\mathcal{N}(0,1)\)</span>, define <span class="math inline">\(X_n=(-1)^n\cdot X\)</span>, <span class="math inline">\(X_n\sim\mathcal{N}(0,1)\)</span><br />
Therefore, <span class="math inline">\(X_n\overset{d}{\to} X\)</span> but <span class="math inline">\(X_n\overset{p}{\not\to} X\)</span></p>
</blockquote>
<p>Proof:<br />
Since <span class="math inline">\(X_{2n+1} = -X\)</span>,
<span class="math display">\[\begin{aligned}P\big(|X_{2n+1}-X|&gt;ε\big) &amp;= P\big(|-X-X|&gt;ε\big) \\
&amp;= P\Big(|X|&gt;\frac{ε}{2}\Big)\\
&amp;\not\to 0 \end{aligned}\]</span></p>
<p>Remark:<br />
By def, <span class="math inline">\(X_n\overset{d}{\to} X\)</span> if <span class="math inline">\(E[f(X_n)]\to E[f(X)]\)</span> for all bounded and continuous <span class="math inline">\(f\)</span><br />
Which <strong>does not</strong> imply that <span class="math inline">\(E[X_n]\to E[X]\)</span> since <span class="math inline">\(f(x)=x\)</span> is continuous but not bounded</p>
<blockquote>
<p>Thm 2.11.<br />
<span class="math inline">\(X_n\overset{p}{\to} X\implies X_n\overset{d}{\to} X\)</span></p>
</blockquote>
<p>Proof:<br />
Let <span class="math inline">\(ε&gt;0\)</span>, <span class="math inline">\(f\)</span> be bounded and continuous<br />
WLOG let <span class="math inline">\(|f|\leq 1\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg}\Big| E\big[f(X_n)\big] - E\big[f(X)\big]\Big| \\
&amp;\leq E\Big[\big|f(X_n) - f(X) \big| \Big] \\
&amp;= E\Big[\big|f(X_n) - f(X)\cdot 1(\|X\|&gt; c) \big| \Big] + E\Big[\big|f(X_n) - f(X)\cdot 1(\|X\|\leq c) \big| \Big] \\
&amp;= E\Big[\big|\underbrace{f(X_n) - f(X)}_{|\cdot|\leq 2}\cdot 1(\|X\|&gt; c) \big| \Big] \\
&amp;\phantom{\ggg}+ E\Big[\big|f(X_n) - f(X)\cdot 1(\|X\|\leq c,\ \|X_n-X\|\leq δ) \big| \Big] \\
&amp;\phantom{\ggg}+ E\Big[\big|\underbrace{f(X_n) - f(X)}_{|\cdot|\leq 2}\cdot 1(\|X\|\leq c,\ \|X_n-X\|&gt; δ) \big| \Big] \\
&amp;\leq 2\cdot P\big(\| X\|&gt; c\big) + 2\cdot P\big(\| X_n-X\|&gt; δ\big)\\
&amp;\phantom{\ggg}+ \sup\Big\{|f(v)-f(u)\big| : \|v\|\leq c,\ \|v-u\|\leq δ\Big\}\end{aligned}\]</span>
:) <span class="math inline">\(E[f(X_n)]\to E[f(X)]\)</span>, i.e.,<br />
:) <span class="math inline">\(\Big|E\big[f(X_n)\big]- E\big[f(X)\big]\Big|\leq ε\)</span> for all <span class="math inline">\(n\geq N\)</span><br />
Choose <span class="math inline">\(c&gt;0\)</span> s.t. <span class="math inline">\(P(\|X\|&gt;c)\leq \frac{ε}{6}\)</span>,<br />
choose <span class="math inline">\(δ&gt;0\)</span> s.t. <span class="math inline">\(\sup\Big\{|f(v)-f(u)\big| : \|v\|\leq c,\ \|v-u\|\leq δ\Big\}\leq \frac{ε}{3}\)</span><br />
choose <span class="math inline">\(N\in\mathbb{N}\)</span> s.t. <span class="math inline">\(P\big(\| X_n-X\|&gt; δ\big)\leq\frac{ε}{6}\)</span> for all <span class="math inline">\(n\geq N\)</span><br />
<span class="math inline">\(\implies\)</span> we have <span class="math inline">\(\Big| E\big[f(X_n)\big] - E\big[f(X)\big]\Big| \leq ε\)</span></p>
<blockquote>
<p>Thm 2.12.<br />
Let <span class="math inline">\(c\in\mathbb{R}^k\)</span>, then <span class="math inline">\(X_n\overset{p}{\to} c\iff X_n\overset{d}{\to}c\)</span></p>
</blockquote>
<p>Proof: exercises</p>
</div>
<div id="central-limit-theorem" class="section level3">
<h3>Central Limit Theorem</h3>
<ul>
<li>Let <span class="math inline">\(X_1,\dots,X_n\)</span> be real valued random variables, <span class="math inline">\(\bar{X}_n=\frac{1}{n}\sum X_i\)</span><br />
</li>
</ul>
<ol style="list-style-type: decimal">
<li>If for all <span class="math inline">\(i\)</span>, <span class="math inline">\(X_i\)</span> are independent, and <span class="math inline">\(X_i\sim\mathcal{N}(μ,σ^2)\)</span>, then
<ul>
<li><span class="math inline">\(\bar{X}_n\sim\mathcal{N}(μ,\frac{σ^2}{n})\)</span></li>
<li><span class="math inline">\(\sqrt{n}(\frac{\bar{X}_n-μ}{σ})\sim\mathcal{N}(0,1)\)</span></li>
</ul></li>
<li>If <span class="math inline">\(X_i\)</span> are iid, and <span class="math inline">\(E[X_i]=μ\)</span>, <span class="math inline">\(Var(X_i)=σ^2\)</span>, then
<ul>
<li><span class="math inline">\(\sqrt{n}(\frac{\bar{X}_n-μ}{σ})\overset{d}{\to}\mathcal{N}(0,1)\)</span></li>
</ul></li>
</ol>
<blockquote>
<p>Thm 2.13. Central Limit Theorem (real valued iid sequences)<br />
Let <span class="math inline">\(X_1,\dots,X_n\)</span> be iid real valued random variables,<br />
<span class="math inline">\(E[X_i]=μ\)</span>, <span class="math inline">\(Var(X_i)=σ^2\)</span>, then
<span class="math display">\[\sqrt{n}(\frac{\bar{X}_n-μ}{σ})\overset{d}{\to}\mathcal{N}(0,1)\]</span></p>
</blockquote>
<p>also written as:
<span class="math display">\[\begin{aligned}\sqrt{n}(\frac{\bar{X}_n-μ}{σ}) &amp;= \sqrt{n}(\frac{\frac{1}{n}\sum X_i-μ}{σ})\\
&amp;= \frac{\frac{1}{\sqrt{n}}\sum X_i-μ}{σ} \overset{d}{\to}\mathcal{N}(0,1) \end{aligned}\]</span>
by Thm 2.9. (CMT),
<span class="math display">\[\begin{aligned}\frac{1}{\sqrt{n}}\sum(X_i-μ) \overset{d}{\to}\mathcal{N}(0,σ^2) \end{aligned}\]</span></p>
<blockquote>
<p>Thm 2.14. Central Limit Theorem (multivariable iid sequences)<br />
Let <span class="math inline">\(X_1,\dots,X_n\)</span> be iid <span class="math inline">\(\mathbb{R}^k\)</span> valued random variables,<br />
<span class="math inline">\(E[X_i]=μ\)</span>, <span class="math inline">\(Var(X_i)=Σ\)</span>, then
<span class="math display">\[\frac{1}{\sqrt{n}}\sum(X_i-μ)\overset{d}{\to}\mathcal{N}(0,Σ)\]</span></p>
</blockquote>
<p>Proof:<br />
Apply Thm 2.13. and Thm 2.10.<br />
Let <span class="math inline">\(a\in\mathbb{R}^k\)</span>,<br />
<span class="math inline">\(\implies a^TX_1,\dots,a^TX_n\)</span> are k-demensional iid with mean <span class="math inline">\(a^Tμ\)</span> and variance <span class="math inline">\(a^TΣa\)</span><br />
therefore, by Thm 2.13.,
<span class="math display">\[\frac{1}{\sqrt{n}}\sum\Big(\frac{a^TX_i - a^Tμ}{\sqrt{a^TΣa}}\Big) \overset{d}{\to}\mathcal{N}(0,1)\]</span>
by Thm 2.9. (CMT),
<span class="math display">\[\begin{aligned}a^T \frac{1}{\sqrt{n}} (X_i-μ) &amp;\overset{d}{\to} \mathcal{N}(0,a^TΣa) \\
&amp;\sim a^T X &amp;&amp; X\sim\mathcal{N}(0,Σ) \\
a^T \frac{1}{\sqrt{n}} (X_i-μ) &amp;\overset{d}{\to} a^T X \end{aligned}\]</span>
by Thm 2.10.,
<span class="math display">\[\frac{1}{\sqrt{n}}\sum(X_i-μ)\overset{d}{\to} X\]</span></p>
</div>
<div id="stochastic-boundedness-tightness" class="section level3">
<h3>Stochastic Boundedness (Tightness)</h3>
<ul>
<li><span class="math inline">\(X_n\)</span>: sequence of random variables that are not necessarily defined on the same probability space</li>
</ul>
<blockquote>
<p>Def 2.15. Stochastic boundedness<br />
<span class="math inline">\(\{X_n\}\)</span> is stochastically bounded, <span class="math inline">\(X_n=O_p(1)\)</span> if<br />
for all <span class="math inline">\(ε&gt;0\)</span>, exists <span class="math inline">\(c&gt;0\)</span> and <span class="math inline">\(N\in\mathbb{N}\)</span> s.t. for all <span class="math inline">\(n\geq N\)</span>,
<span class="math display">\[P\big(\|X_n\|\leq c\big)\geq 1-ε\]</span></p>
</blockquote>
<p>Example:<br />
<span class="math inline">\(X_n=X\sim\mathcal{N}(0,1)\)</span><br />
for any <span class="math inline">\(c&gt;0\)</span>, always exists <span class="math inline">\(X_i\)</span> s.t. <span class="math inline">\(|X_i|&gt;c\)</span><br />
therefore, cannot use <span class="math inline">\(P\big(\|X_n\|\leq c\big)=1\)</span><br />
<span class="math inline">\(\implies P\big(\|X_n\|\leq c\big)\)</span></p>
<blockquote>
<p>Thm 2.16.<br />
<span class="math inline">\(X_n\overset{d}{\to} X\implies X_n=O_p(1)\)</span></p>
</blockquote>
<p>Proof: assume</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X_n, X\)</span> are real valued</li>
<li>Distribution function <span class="math inline">\(F\)</span> of <span class="math inline">\(X\)</span> is continuous</li>
</ol>
<p>For <span class="math inline">\(ε&gt;0\)</span>, choose <span class="math inline">\(X_ε^-\)</span> and <span class="math inline">\(X_ε^+\)</span> s.t. <span class="math inline">\(F(X_ε^-)&lt;\frac{ε}{2}\)</span> and <span class="math inline">\(F(X_ε^+)&gt;1-\frac{ε}{2}\)</span><br />
Therefore,
<span class="math display">\[\begin{aligned}P(X_ε^-&lt;X\leq X_ε^+) &amp;= F_n(X_ε^+) - F_n(X_ε^-)\\
&amp;\to F(X_ε^+) - F(X_ε^-)\\
&amp;&gt; 1-ε\end{aligned}\]</span>
Choose <span class="math inline">\(c=\max\{|X_ε^-|, |X_ε^+|\}\)</span>, therefore,<br />
<span class="math display">\[\begin{aligned}P(|X|\leq c)&amp;\geq P(X_ε^-&lt;X\leq X_ε^+ )\\
&amp;\geq 1-ε \end{aligned}\]</span></p>
</div>
<div id="calculus-for-convergence-in-probability-tightness" class="section level3">
<h3>Calculus for Convergence in Probability &amp; Tightness</h3>
<ul>
<li>Convergence in Probability: <span class="math inline">\(X_n=X+o_p(1)\)</span> means <span class="math inline">\(X_n\overset{P}{\to} X\)</span></li>
<li>Tightness: <span class="math inline">\(X_n=O_p(1)\)</span></li>
<li>Let <span class="math inline">\(\{r_n\}\)</span> be a real sequence
<ul>
<li>If <span class="math inline">\(r_n^{-1}\cdot X_n = O_p(1)\)</span>, then <span class="math inline">\(X_n\)</span> is of order <span class="math inline">\(r_n\)</span>: <span class="math display">\[X_n=O_p(\underline{r_n})\]</span></li>
<li>If <span class="math inline">\(r_n^{-1}\cdot X_n = o_p(1)\)</span>, then <span class="math inline">\(X_n\)</span> is of smaller order than <span class="math inline">\(r_n\)</span>: <span class="math display">\[X_n=o_p(\underline{r_n})\]</span></li>
</ul></li>
</ul>
<blockquote>
<p>Thm 2.17.</p>
<ol style="list-style-type: lower-roman">
<li>Let <span class="math inline">\(c\in\mathbb{R}\)</span>,<br />
then <span class="math inline">\(X_n=c+o_p(1)\implies X_n=O_p(1)\)</span></li>
<li>For <span class="math inline">\(X_n,Y_n=o_p(1)\)</span>, <span class="math inline">\(U_n,W_n=O_p(1)\)</span>,
<span class="math display">\[\begin{aligned}(a) &amp;&amp; X_n+Y_n&amp;=o_p(1)\\ (b) &amp;&amp; U_n+W_n&amp;=O_p(1) \\(c) &amp;&amp; U_n\cdot W_n&amp;=O_p(1)\\(d) &amp;&amp; X_n\cdot U_n&amp;=o_p(1)\end{aligned}\]</span></li>
<li>Slutsky’s Lemma<br />
Let <span class="math inline">\(g:\mathbb{R}^k\to\mathbb{R}^l\)</span> continuous at <span class="math inline">\(x_0\)</span>, <span class="math inline">\(X_n = x_0+o_p(1)\)</span>,<br />
then <span class="math inline">\(g(X_n)=g(x_0) + o_p(1)\)</span></li>
<li>Let <span class="math inline">\(A_n\)</span> random matrix, <span class="math inline">\(A\)</span> deterministic invertible matrix, <span class="math inline">\(A_n=A+o_p(1)\)</span>,<br />
then <span class="math inline">\(A_n^{-1}\)</span> exists with probability approaching 1, <span class="math inline">\(A_n^{-1}=A^{-1}+o_p(1)\)</span></li>
</ol>
</blockquote>
<p>Proof:</p>
<ol style="list-style-type: lower-roman">
<li>Thm 2.12</li>
<li>Exercise</li>
<li></li>
<li><ol style="list-style-type: decimal">
<li>If <span class="math inline">\(A\)</span> is invertible, then all <span class="math inline">\(B\)</span> with <span class="math inline">\(\|B-A\|\leq ε\)</span> are invertible,<br />
therefore, since <span class="math inline">\(A_n=A+o_p(1)\)</span>,
<span class="math display">\[\begin{aligned}P(A_n^{-1}\text{ exists})&amp;\geq P(\|A_n-A\|\leq ε)\\&amp;\to 1 \end{aligned}\]</span></li>
<li><span class="math inline">\(A_n^{-1}=A^{-1}+o_p(1)\)</span> since matrix inversion is a continuous mapping<br />
Let <span class="math inline">\(g(B)=B^{-1}\)</span> for invertible matrix <span class="math inline">\(B\)</span>,<br />
<span class="math inline">\(\implies g\)</span> is continuous at <span class="math inline">\(B=A\)</span><br />
therefore, by (iii),
<span class="math display">\[g(A_n) = g(A) + o_p(1)\]</span></li>
</ol></li>
</ol>
<blockquote>
<p>Examples</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><span class="math display">\[\begin{aligned} &amp;&amp; \sqrt{n}\cdot X &amp;= O_p(1) \\
\iff &amp;&amp; X_n &amp;= O_p(\frac{1}{\sqrt{n}})\\
\iff &amp;&amp; X_n &amp;= \underbrace{\frac{1}{\sqrt{n}}}_{=o_p(1)} \cdot O_p(1) = o_p(1) \end{aligned}\]</span>
Set <span class="math inline">\(X_n=(\hat{θ}_n-θ)\)</span>, <span class="math inline">\(\hat{θ}_n\)</span> is an estimation of <span class="math inline">\(θ\)</span>, then
<span class="math display">\[\begin{aligned}&amp;&amp;\sqrt{n}\cdot (\hat{θ}_n-θ)&amp;= O_p(1) \\
\iff &amp;&amp; \hat{θ}_n&amp;= θ+O_p(\frac{1}{\sqrt{n}}) \end{aligned}\]</span></li>
<li>For <span class="math inline">\(X_n = O_p(r_n)\)</span>,
<span class="math display">\[P(|X_n|\leq c\cdot r_n)\geq 1-ε\text{ for all }n\geq N\]</span></li>
<li>For <span class="math inline">\(Z_n=o_p(r_n)\)</span>,
<span class="math display">\[P(|Z_n|\leq ε\cdot r_n)\to 1\]</span></li>
</ol>
</div>
</div>
<div id="m-estimators" class="section level2">
<h2>M-Estimators</h2>
<ul>
<li>Assume random variables <span class="math inline">\(Z\sim P_{θ_0}\)</span>, <span class="math inline">\(P_{θ_0}\in\{P_{θ} : θ\in Θ \}\)</span>
<ul>
<li><span class="math inline">\(θ_0 = \text{argmax}_{θ\in Θ} \ Q(θ)\)</span></li>
</ul></li>
<li>Let <span class="math inline">\(\{Z_i : i=1,\dots, n\}\)</span> be iid data with <span class="math inline">\(Z_i\sim P_{θ_0}\)</span>
<ul>
<li>Approximate <span class="math inline">\(Q(θ)\)</span> by <span class="math inline">\(Q_n(θ) = Q_n(θ, Z_1,\dots, Z_n)\)</span></li>
<li>M-Estimator: <span class="math inline">\(\hat{θ} =\text{argmax}_{θ\in Θ} \ Q_n(θ)\)</span></li>
</ul></li>
</ul>
<div id="linear-regression-model" class="section level3">
<h3>Linear Regression Model</h3>
<ul>
<li>Standard model: <span class="math display">\[\begin{aligned}Y &amp;= X^T \cdot θ_0 + ε \\ &amp;= θ_{0,1}X_1 + \dots + θ_{0,d}X_d + ε \end{aligned}\]</span>
<ul>
<li><span class="math inline">\(E[ε\ |\ X] = 0\)</span>, <span class="math inline">\(E[ε^2\ |\ X]=σ^2\)</span></li>
<li><span class="math inline">\(E[Y\ |\ X] = E[X^T \cdot θ_0 + ε\ |\ X] = X^T \cdot θ_0\)</span></li>
</ul></li>
<li><span class="math inline">\(θ_0\)</span> is the minimizer: <span class="math display">\[θ_0 \in\text{argmin}_{θ\in\mathbb{R}^d}\ Q(θ)= E[(Y-X^T \cdot θ)^2]\]</span>
since
<span class="math display">\[\begin{aligned}E[(Y-X^T \cdot θ)^2] &amp;= E[(X^T \cdot θ_0 + ε - X^T \cdot θ)^2]\\
&amp;= E[ε^2] + 2\underbrace{E[εX^T(θ_0-θ)]}_{=0}+ E[(X^T(θ_0-θ))^2] \\
&amp;= σ^2 + E[(θ_0-θ)^TXX^T(θ_0-θ)] \\
&amp;= σ^2 + (θ_0-θ)^TE[XX^T](θ_0-θ)\end{aligned}\]</span></li>
<li>Estimator <span class="math inline">\(\hat{θ}\)</span><br />
Suppose have iid data, replace <span class="math inline">\(Q(θ)\)</span> by empirical data (sample mean):
<span class="math display">\[\begin{aligned} \min &amp;&amp; Q_n(θ) &amp;= \frac{1}{n}\sum (Y_i-X_i^T \cdot θ)^2 \\
&amp;&amp; \hat{θ} &amp;= \text{argmin}_{θ\in\mathbb{R}^d} \ Q_n(θ) \end{aligned}\]</span>
the problem has a closed form solution:
<span class="math display">\[\begin{aligned} \hat{θ} &amp;= \Big(\frac{1}{n}\sum X_iX_i^T\Big)^{-1}\Big(\frac{1}{n}\sum X_iY_i\Big)\\
&amp;= θ_0 + \Big(\frac{1}{n}\sum X_iX_i^T\Big)^{-1}\Big(\frac{1}{n}\sum X_iε_i\Big)\end{aligned}\]</span></li>
<li>Asymptotics of <span class="math inline">\(\hat{θ}\)</span>:<br />
by applying LLN and CLT, under standard conditions,
<span class="math display">\[\begin{aligned}\frac{1}{n}\sum X_iX_i^T &amp;\overset{P}{\to} E[X_iX_i^T] \\
\frac{1}{\sqrt{n}}\sum X_iε_i &amp;\overset{d}{\to} \mathcal{N}(0,σ^2E[X_iX_i^T]) \\
\sqrt{n}(\hat{θ}-θ_0) &amp;\overset{d}{\to} \mathcal{N}(0,\frac{1}{σ^2E[X_iX_i^T]}) \end{aligned}\]</span></li>
</ul>
</div>
<div id="parametric-model" class="section level3">
<h3>Parametric Model</h3>
<ul>
<li><span class="math inline">\(Z\)</span>: <span class="math inline">\(\mathbb{R}^d\)</span> value random variable
<ul>
<li>Distribution function: <span class="math inline">\(Z\sim P\)</span></li>
</ul></li>
<li>Model for <span class="math inline">\(Z\)</span>: known up to finite dimensional parameter <span class="math inline">\(θ_0\in Θ\subset \mathbb{R}^k\)</span></li>
<li><span class="math inline">\(θ_0\)</span> is the minimizer for criterian function <span class="math inline">\(Q(θ)=E[q(Z,θ)]\)</span>
<span class="math display">\[\begin{aligned}Z &amp;= (Y,X)\\ Q(θ) &amp;= E[(Y-X^Tθ)^2] \\ θ_0 &amp;=\text{argmax}_{θ\in\mathbb{R}^k}\ Q(θ) \end{aligned}\]</span></li>
<li>Sample: (iid) <span class="math inline">\(\{Z_i: i=1,\dots, n\}\)</span>, <span class="math inline">\(Z_i\sim P\)</span>
<ul>
<li>Sample criterian function:
<span class="math display">\[Q_n(θ) = \frac{1}{n}\sum q(Z_i, θ)\]</span>
by LLN, <span class="math inline">\(Q_n(θ)\overset{P}{\to}Q(θ)\)</span> pointwise as <span class="math inline">\(n\to\infty\)</span></li>
<li>M-estimator: <span class="math display">\[\hat{θ}=\hat{θ}_n=\text{argmax} \ Q_n(θ)\]</span></li>
</ul></li>
</ul>
<blockquote>
<p>Examples<br />
Do we get consistency: <span class="math inline">\(\hat{θ}_n\to θ\)</span><br />
or asymptotic normality: <span class="math inline">\(\sqrt{n}(\hat{θ}_n-θ)\overset{d}{\to}\mathcal{N}(0,Σ)\)</span>?</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>OLS<br />
<span class="math inline">\(Z=(Y,X)\)</span>, <span class="math inline">\(q(Z,θ)=(Y-X^Tθ)^2\)</span>,
<span class="math display">\[\begin{aligned}Q(θ) &amp;= E[q(Z,θ)]\\&amp;=E[(Y-X^Tθ)^2] \\ Q_n(θ) &amp;= \frac{1}{n}\sum q(Z_i,θ) \\ &amp;= \frac{1}{n}\sum (Y_i-X_i^Tθ)^2 \end{aligned}\]</span>
OLS estimator: <span class="math inline">\(\hat{θ}=\text{argmax}_{θ\in\mathbb{R}^k}\ Q_n(θ)\)</span></li>
<li>Non-linear Least Squares<br />
<span class="math inline">\(Z=(Y,X)\)</span>, <span class="math inline">\(Y=g(X,θ_0)+ε\)</span>, <span class="math inline">\(E[ε\ |\ X] = 0\)</span> and <span class="math inline">\(g\)</span> is a function known up to <span class="math inline">\(θ_0\)</span><br />
<span class="math inline">\(E[Y\ |\ X] = g(X,θ_0)\)</span>, <span class="math inline">\(θ\in\text{argmax}_{θ\in Θ}\ E[(Y-g(X,θ_0))^2]\)</span><br />
Let <span class="math inline">\(q(Z,θ)=(Y-g(X,θ_0))^2\)</span>,
<span class="math display">\[\begin{aligned}Q(θ) &amp;= E[q(Z,θ)]\\&amp;=E[(Y-g(X,θ_0))^2]\\
Q_n(θ) &amp;= \frac{1}{n}\sum q(Z_i,θ) \\
&amp;= \frac{1}{n}\sum (Y_i - g(X_i,θ))^2 \end{aligned}\]</span></li>
<li>Maximum Likelihood Estimator<br />
<span class="math inline">\(Z\sim f_Z(\cdot,θ)\)</span>, joint density of <span class="math inline">\((Z_1,\dots, Z_n)\)</span>: <span class="math inline">\(f_{Z_1,\dots,Z_n}(Z_1,\dots,Z_n) = \prod f_Z(Z_i, θ_0)\)</span><br />
Estimator:
<span class="math display">\[\begin{aligned}\hat{θ}&amp;=\text{argmax}_{θ\in Θ}\ \prod f_Z(Z_i, θ_0)\\
&amp;= \text{argmax}_{θ\in Θ}\ \sum \log\big(f_Z(Z_i, θ_0)\big) \\
&amp;= \text{argmin} \ -\sum \log\big(f_Z(Z_i, θ_0)\big)\end{aligned}\]</span>
Let <span class="math inline">\(q(Z,θ) = -\log\big(f_Z(Z_i, θ_0)\big)\)</span>
<span class="math display">\[\begin{aligned}Q(θ) &amp;= E[q(Z,θ)] \\
&amp;= -E[\log\big(f_Z(Z_i, θ_0)\big)] \\
Q_n(θ) &amp;= \frac{1}{n}\sum q(Z_i,θ) \\
&amp;= -\frac{1}{n}\sum \log\big(f_Z(Z_i, θ_0)\big) \\
\hat{θ}&amp;= \text{argmin}_{θ\in Θ}\ Q_n(θ) \end{aligned}\]</span></li>
<li>General Method of Moments (GMM)<br />
Suppose for some function <span class="math inline">\(h:\mathbb{R}^d\times Θ\to\mathbb{R}^L\)</span><br />
that <span class="math inline">\(E[h_l(Z,θ_0)] = 0\)</span> for <span class="math inline">\(l=1,dots, L\)</span><br />
Choose <span class="math inline">\(Q(θ) = E[h(Z,θ)^T]\cdot W\cdot E[h(Z,θ)]\)</span> for some weighting matrix <span class="math inline">\(W(L\times L)\)</span><br />
<span class="math inline">\(Q_n(θ) = \big(\frac{1}{n}\sum h(Z_i,θ)^T\big)\cdot W_n\cdot\big(\frac{1}{n}\sum h(Z_i,θ)\big)\)</span> for some sequence of weighting matrices <span class="math inline">\(W(L\times L)\)</span><br />
If <span class="math inline">\(W_n\overset{P}{\to}W\)</span>, then bt LLN,
<span class="math display">\[\begin{aligned}Q_n(θ) &amp;\overset{P}{\to} Q(θ) \text{ pointwise} \\
\hat{θ} &amp;= \text{argmin}_{θ\in Θ} \ Q_n(θ) \end{aligned}\]</span></li>
</ol>
<blockquote>
<p>Example: Quantile function</p>
</blockquote>
<p>Let <span class="math inline">\(Z\)</span> be a real valued random variable with <span class="math inline">\(F\)</span>,<br />
Define τ-quantile: for any <span class="math inline">\(0&lt;τ&lt;1\)</span>, any <span class="math inline">\(θ_τ\in\mathbb{R}\)</span> satisfying
<span class="math display">\[\begin{cases}P(Z\leq θ_τ)\geq τ \\ P(Z\geq θ_τ)\geq 1-τ \end{cases}\]</span>
Define the check function: <span class="math inline">\(f_τ(n) = n\cdot (τ-1(n&lt;0))\)</span> for <span class="math inline">\(n\in\mathbb{R}\)</span>,<br />
Then <span class="math inline">\(θ(τ)\)</span> minimises the check function:
<span class="math display">\[\begin{aligned}Q_τ(θ) &amp;= E[f_τ(Z-θ)] \\
&amp;= τ\cdot E[(Z-θ)\cdot 1(Z\geq θ)] + (τ+1)\cdot E[(Z-θ)\cdot 1(Z&lt; θ)] \\
\frac{\partial Q_τ(θ)}{θ_+} &amp;= \lim_{δ\to 0}\ \frac{1}{|δ|}\big(Q_τ(θ+|δ|) - Q_τ(θ)\big) \\
Q_τ(θ+|δ|) - Q_τ(θ) &amp;= τ\cdot\bigg(E\Big[\big(Z-(θ+|δ|)\big)\cdot 1(Z\geq θ+|δ|)\Big] - E\big[(Z-θ)\cdot 1(Z\geq θ)\big]\bigg) \\
&amp;\phantom{\ggg} (1-τ)\cdot\bigg(E\Big[\big(Z-(θ+|δ|)\big)\cdot 1(Z&lt; θ+|δ|)\Big] - E\big[(Z-θ)\cdot 1(Z&lt; θ)\big]\bigg) \\
&amp;= τ\Big(-|δ|\cdot P(Z\geq θ+|δ|) - \underbrace{E\big[(Z-θ)\cdot 1(θ\leq Z&lt; θ+|δ|)\big]}_{\leq |δ|\cdot P(θ&lt;Z\leq θ+|δ|)}\Big) \\
&amp;\phantom{\ggg} (1-τ)\cdot\Big(-|δ|\cdot P(Z&lt; θ+|δ|) - \underbrace{E\big[(Z-θ)\cdot 1(θ\leq Z&lt; θ+|δ|)\big]}\Big) \\
|δ|\cdot P(θ&lt;Z\leq θ+|δ|) &amp;= |δ|\cdot\big( F(θ+|δ|) - F(θ)\big) \\
&amp;\to 0 \text{ as }|δ|\to 0 \\
Q_τ(θ+|δ|) - Q_τ(θ) &amp;= -τ|δ| P(Z\geq θ+|δ|) - (τ-1)|δ|P(Z&lt; θ+|δ|) + o(|δ|) \\
&amp;= -τ|δ| +|δ|\underbrace{P((Z&lt; θ+|δ|)}_{=P(Z\leq θ) + P(θ&lt;Z&lt;θ+|δ|)} + o(|δ|) \\
&amp;= -τ|δ| +|δ|\cdot P(Z\leq θ)+ o(|δ|) \\
\frac{\partial Q_τ(θ)}{θ_+} &amp;= \lim_{δ\to 0}\ -τ|δ| +|δ|\cdot P(Z\leq θ) + o(|δ|) \\
&amp;= -τ +\cdot P(Z\leq θ)\\
\frac{\partial Q_τ(θ)}{θ_-} &amp;= -τ + P(Z&lt; θ)\\\end{aligned}\]</span>
For any <span class="math inline">\(θ\in\mathbb{R}\)</span> with <span class="math inline">\(F(θ)=P(Z\leq θ)&gt;F(θ(τ))\geq τ\)</span>,<br />
we have <span class="math inline">\(=P(Z&lt; θ) = F(θ) - P(Z-θ)\geq τ\)</span>,<br />
therefore, for any <span class="math inline">\(θ\in\mathbb{R}\)</span> with <span class="math inline">\(F(θ)&gt;F(θ(τ))\)</span>,
<span class="math display">\[\begin{cases}\frac{\partial Q_τ(θ)}{θ_+} &gt;0 \\ \frac{\partial Q_τ(θ)}{θ_-} \geq 0 \end{cases}\]</span>
moreover, for any <span class="math inline">\(θ\in\mathbb{R}\)</span> with <span class="math inline">\(F(θ)&lt;F(θ(τ))\)</span>,
<span class="math display">\[\begin{cases}\frac{\partial Q_τ(θ)}{θ_+} &lt;0 \\ \frac{\partial Q_τ(θ)}{θ_-} &lt; 0 \end{cases}\]</span>
Therefore, <span class="math inline">\(θ(τ)\)</span> minimises <span class="math inline">\(Q_τ(θ)\)</span></p>
<p>Define <span class="math inline">\(Q_{n,τ}(θ) = \frac{1}{n}\sum f_τ (Z_i-θ)\)</span>,<br />
<span class="math inline">\(\hat{θ}(τ) \in\text{argmin} \ Q_{n,τ}(θ)\)</span></p>
<p>Remarks:</p>
<ul>
<li>For any <span class="math inline">\(0&lt;τ&lt;1\)</span>, value <span class="math inline">\(θ_τ = θ(τ)\)</span> is a τ-quantile of <span class="math inline">\(Z\)</span></li>
<li>If <span class="math inline">\(F\)</span> is strictly increasing, <span class="math inline">\(θ(τ) = F^{-1}(τ)\)</span></li>
<li>If <span class="math inline">\(F\)</span> is continuous at <span class="math inline">\(θ\)</span>, then <span class="math inline">\(\frac{\partial Q_τ(θ)}{θ_+}=\frac{\partial Q_τ(θ)}{θ_-}=-τ+F(θ)\)</span></li>
<li>If <span class="math inline">\(F\)</span> has a jump at <span class="math inline">\(θ\)</span>, i.e. <span class="math inline">\(P(X=θ)&gt;0\)</span>, then <span class="math inline">\(Q_τ\)</span> has a kink at <span class="math inline">\(θ\)</span></li>
<li><span class="math inline">\(Q_τ\)</span> is continuous of <span class="math inline">\(θ\)</span> in any case</li>
</ul>
<blockquote>
<p>PS3.Q4: Lemma of Fatou<br />
Consider a sequence of non-negative random variables <span class="math inline">\(X_n\)</span> (defined on the same probability space <span class="math inline">\((Ω, \mathcal{A}, P)\)</span>)<br />
Define <span class="math inline">\(X(ω)= \text{lim inf}_{n\to\infty} X_n(ω) = \lim_{n\to\infty}\inf_{k\geq n} X_k(ω)\)</span><br />
Apply the monotone convergence theorem to show that
<span class="math display">\[E[X] = E[\text{lim inf}_{n\to\infty} X_n]\leq \text{lim inf}_{n\to\infty} E[X_n]\]</span></p>
</blockquote>
<p>Since <span class="math inline">\(X_n\)</span> is non-negative, <span class="math inline">\((\inf_{k\geq n} X_k(ω))_{k\in\mathbb{N}}\)</span> is monotonic increasing,
<span class="math display">\[\begin{aligned}E[X] &amp;= E[\text{lim inf}_{n\to\infty} X_n] \\
&amp;= \lim_{n\to\infty} E[\inf_{k\geq n} X_k]\end{aligned}\]</span>
Let <span class="math inline">\(Y=\inf_{k\geq n} X_k\)</span><br />
<span class="math inline">\(\implies Y\leq X_k\)</span> for all <span class="math inline">\(k\geq n\)</span><br />
<span class="math inline">\(\implies E[Y]\leq E[X_k]\)</span> for all <span class="math inline">\(k\geq n\)</span><br />
Take inf of RHS,
<span class="math display">\[\begin{aligned}E[Y] &amp;\leq \inf_{k\geq n}E[X_k] \\
E[\inf_{k\geq n} X_k] &amp;\leq \inf_{k\geq n}E[X_k] \\
E[\text{lim inf}_{n\to\infty} X_n] &amp;\leq \text{lim inf}_{n\to\infty} E[X_n]\end{aligned}\]</span></p>
<blockquote>
<p>PS3.Q5: Cauchy-Schwarz Inequality<br />
Let <span class="math inline">\(X, Y\)</span> be real valued random variables with <span class="math inline">\(E[X^2],E[Y^2]&lt;\infty\)</span>.<br />
Hint:</p>
<ol style="list-style-type: decimal">
<li>Show that <span class="math inline">\((E[XY])^2\leq E[X^2]E[Y^2]\)</span></li>
<li>What does hold if <span class="math inline">\((E[XY])^2= E[X^2]E[Y^2]\)</span>?</li>
</ol>
</blockquote>
<ol style="list-style-type: decimal">
<li>Since <span class="math display">\[\begin{aligned}|X\cdot Y|&amp;\leq X^2 + Y^2 \\
E[|X\cdot Y|] &amp;\leq E[X^2 + Y^2] \\
&amp;= E[X^2] + E[Y^2] &lt;\infty \end{aligned}\]</span>
Pick arbitrary <span class="math inline">\(α,β\in\mathbb{R}\)</span>,
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(E[X\cdot Y]\geq 0\)</span>
<span class="math display">\[\begin{aligned}0&amp;\leq E[(αX-βY)^2] \\&amp;= α^2E[X^2] - 2αβE[XY] + β^2E[Y^2] \\
 2E[XY] &amp;\leq \frac{α}{β}E[X^2] + \frac{β}{α}E[Y^2] \\
 2E[XY] &amp;\leq \frac{\sqrt{E[Y^2]}}{\sqrt{E[X^2]}}E[X^2] + \frac{\sqrt{E[X^2]}}{\sqrt{E[Y^2]}}E[Y^2] \\
 &amp;= 2\sqrt{E[X^2]}\sqrt{E[Y^2]} \\
 E[(XY)^2] &amp;\leq E[X^2]E[Y^2]\end{aligned}\]</span></li>
<li><span class="math inline">\(E[X\cdot Y]&lt; 0\)</span>, use <span class="math inline">\(0\leq E[(αX+βY)^2]\)</span></li>
</ol></li>
<li>Assume <span class="math inline">\(E[X^2],E[Y^2]\neq 0\)</span>,<br />
<span class="math display">\[\begin{aligned}(E[XY])^2 &amp;= E[X^2]E[Y^2] \\
E[(\frac{X}{\sqrt{E[X^2]}}\pm\frac{Y}{\sqrt{E[Y^2]}} )^2] &amp;= 0 &amp;&amp;\pm\text{ holds or} \\
\frac{X}{\sqrt{E[X^2]}}\pm\frac{Y}{\sqrt{E[Y^2]}} &amp;= 0 &amp;&amp; \text{almost surely} \\
X &amp;= C\dots Y &amp;&amp;\text{ almost surely for }c\neq 0 \end{aligned}\]</span></li>
</ol>
</div>
<div id="consistency" class="section level3">
<h3>Consistency</h3>
<ul>
<li>Consistent estimator: <span class="math inline">\(\hat{θ}\overset{P}{\to} θ_0\)</span> under appropriate conditions</li>
<li>Standard consistency result
<ul>
<li>Not sufficient: <span class="math inline">\(Q_n(θ)\overset{P}{\to} Q(θ)\)</span> pointwise for each <span class="math inline">\(θ\in Θ\)</span></li>
</ul></li>
</ul>
<blockquote>
<p>Def 2.1.<br />
<span class="math inline">\(Q_n\)</span> converges uniformly to <span class="math inline">\(Q\)</span> in probability if
<span class="math display">\[\sup_{θ\in Θ} \ \big| Q_n(θ) - Q(θ)\big| \overset{P}{\to} 0\]</span>
meanwhile, the following does not give uniform convergence:
<span class="math display">\[\sup_{θ\in [0,1]} \ \big| Q_n(θ) - Q(θ)\big| = \bar{y} - \underline{y}&gt; 0\]</span></p>
</blockquote>
<blockquote>
<p>Thm 2.2. Consistency of M-Estimator<br />
<span class="math inline">\(\hat{θ}\overset{P}{\to} θ_0\)</span> if:</p>
<ol style="list-style-type: decimal">
<li>(A1) Identification (unique minimizer &amp; enough separation)<br />
For all <span class="math inline">\(ε&gt;0\)</span>, exists <span class="math inline">\(δ&gt;0\)</span> s.t.
<span class="math display">\[\inf_{θ:\|θ-θ_0\|&gt;ε} \ Q(θ)\geq Q_{θ_0}+δ\]</span></li>
<li>(A2) Uniform convergence<br />
<span class="math inline">\(\sup_{θ\in Θ} \ \big| Q_n(θ) - Q(θ)\big| \overset{P}{\to} 0\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>Example<br />
Proof: show that <span class="math inline">\(\hat{θ}\overset{P}{\to} θ_0\)</span>, that is, <span class="math inline">\(P(\|\hat{θ}-θ_0\|&gt;ε)\to 0\)</span></p>
</blockquote>
<p>Let <span class="math inline">\(ε&gt;0\)</span><br />
By (A1), exists <span class="math inline">\(δ&gt;0\)</span> s.t.
<span class="math display">\[\begin{aligned}\|θ-θ_0\| &amp;&gt; ε \\
\implies Q(θ) &amp;\geq Q(θ_0) + δ \\
Q(θ) - Q(θ_0) &amp;\geq δ \\
|Q(θ) - Q(θ_0)| &amp;\geq δ \end{aligned}\]</span>
hence, <span class="math inline">\(\implies\)</span> (*)
<span class="math display">\[\begin{aligned}P(\|\hat{θ}-θ_0\|&gt;ε) &amp;\leq P\big(|Q(\hat{θ}) - Q(θ_0)| \geq δ\big) \\
&amp;\leq P\big(|Q(\hat{θ}) - Q(θ_0)| \geq δ\big) \\
&amp;\leq P\big(|Q(\hat{θ}) - Q_n(\hat{θ})| + |Q_n(\hat{θ}) - Q(θ_0)| \geq δ\big) \\
&amp;\leq P\big(|Q(\hat{θ}) - Q_n(\hat{θ})|\geq\frac{δ}{2}\text{ or }|Q_n(\hat{θ})- Q(θ_0)|\geq\frac{δ}{2}\big) \\
&amp;\leq P\big(|Q(\hat{θ}) - Q_n(\hat{θ})|\geq\frac{δ}{2}\big) + P\big(Q_n(\hat{θ})- Q(θ_0)|\geq\frac{δ}{2}\big)\end{aligned}\]</span>
Assumption (A2), <span class="math inline">\(\implies\)</span> (+)
<span class="math display">\[\begin{aligned}P\big(|Q(\hat{θ}) - Q_n(\hat{θ})|\geq\frac{δ}{2}\big) &amp;\leq P\big(\sup_{θ\in Θ} |Q(θ) - Q_n(θ)|\geq\frac{δ}{2}\big) \\
&amp;\to 0 \end{aligned}\]</span>
Moreover,
<span class="math display">\[\begin{aligned}Q_n(\hat{θ}) &amp;\geq Q(θ_0) \\
\implies 0 &amp;\leq Q_n(\hat{θ}) - Q(θ_0) \\
&amp;\leq Q_n(θ) - Q(θ_0) \\
&amp;\leq \sup_{θ\in Θ} |Q_n(θ) - Q(θ)|\end{aligned}\]</span>
and
<span class="math display">\[\begin{aligned}Q_n(\hat{θ}) &amp;\leq Q(θ_0) \\
\implies 0 &amp;\leq Q(θ_0) -Q_n(\hat{θ}) \\
&amp;\leq Q(\hat{θ}) - Q_n(\hat{θ}) \\
&amp;\leq \sup_{θ\in Θ} |Q(θ) - Q_n(θ)|\end{aligned}\]</span>
therefore,
<span class="math display">\[|Q_n(\hat{θ}) - Q(θ_0)| \leq \sup_{θ\in Θ} |Q_n(θ) - Q(θ)|\]</span>
Assumption (A2) <span class="math inline">\(\implies\)</span> (++)
<span class="math display">\[\begin{aligned}P\big(|Q_n(\hat{θ}) - Q(θ_0)|\geq\frac{δ}{2}\big) &amp;\leq P\big(\sup_{θ\in Θ} |Q_n(θ) - Q(θ)|\geq\frac{δ}{2}\big) \\
&amp;\overset{P}{\to} 0\end{aligned}\]</span>
Plugging (+) and (++) into (*), we have
<span class="math display">\[P(\|\hat{θ}-θ_0\|&gt;ε)\to 0\]</span></p>
<blockquote>
<p>Lemma 2.3.<br />
(A1) in Thm 2.2 is satisfied if</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(Θ\subset \mathbb{R}^k\)</span> is compact</li>
<li>(!) <span class="math inline">\(Q(θ)&gt; Q(θ_0)\)</span> for all <span class="math inline">\(θ\neq θ_0\)</span></li>
<li><span class="math inline">\(Q\)</span> is continuous</li>
</ol>
</blockquote>
<p>Proof: exercise</p>
<blockquote>
<p>Example: OLS<br />
Let <span class="math inline">\(Y = g(X,θ_0)+ε\)</span>, <span class="math inline">\(E[ε|X] = 0\)</span><br />
Show that <span class="math inline">\(Q(θ) = E[\big(Y-g(X,θ)\big)^2]\)</span> has a unique minimizer <span class="math inline">\(θ_0\)</span>, given assumptions:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Q(θ_0) = E[\big(Y-g(X,θ_0)\big)^2]&lt;\infty\)</span> for all <span class="math inline">\(θ\in Θ\)</span></li>
<li>For all <span class="math inline">\(θ\neq θ_0\)</span>, <span class="math inline">\(g(X,θ)\neq g(X,θ_0)\)</span> with non-zero probability</li>
</ol>
</blockquote>
<p>Given (b) <span class="math inline">\(\implies\)</span>
<span class="math display">\[\begin{aligned}E[\big(g(θ) - g(θ_0)\big)^2] &amp;&gt; 0 \\
Q(θ) &amp;= E[\big(Y-g(X,θ)\big)^2] \\
&amp;= E[\big(\underbrace{Y- g(θ_0)}_{=ε} +g(θ_0) -g(θ)\big)^2] \\
&amp;= E[ε^2] + 2\underbrace{E[ε\cdot \big(g(θ_0) - g(θ)\big)]}_{=0} + \underbrace{E[\big(g(θ_0) - g(θ)\big)^2]}_{&gt;0} \\
&amp;&gt; E[ε^2] \\
&amp;= E[\big(Y- g(θ_0)\big)^2] \\
&amp;= Q(θ_0) \end{aligned}\]</span></p>
<blockquote>
<p>Example: Maximum likelihood<br />
Let <span class="math inline">\(Z\sim f_Z(\cdot, θ_0)\)</span>, define the likelihood function by <span class="math inline">\(Q_n(θ) = \prop^n f_Z(Z_i, θ)\)</span><br />
The ML-estimator of <span class="math inline">\(θ_0\)</span> is
<span class="math display">\[\begin{aligned}\hat{θ}&amp;=\text{argmax}_{θ\in Θ} Q_n(θ)\\
&amp;=\text{argmax}_{θ\in Θ}\log Q_n(θ)\\
&amp;=\text{argmax}_{θ\in Θ}\frac{1}{n}\sum\log f_Z(Z_i, θ)\\
&amp;=\text{argmin}_{θ\in Θ}\underbrace{-\frac{1}{n}\sum\log f_Z(Z_i, θ)}_{\overset{P}{\to}\log f_Z(Z, θ)}\\
Q(θ) &amp;= -E[\log f_Z(Z,θ)]\end{aligned}\]</span>
Show that <span class="math inline">\(Q(θ)\)</span> has a unique minimum <span class="math inline">\(θ_0\)</span>, given assumptions:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(E[\big|\log f_Z(Z,θ)\big|]&lt;\infty\)</span> for all <span class="math inline">\(θ\in Θ\)</span></li>
<li>For all <span class="math inline">\(θ\neq θ_0\)</span>, <span class="math inline">\(f_Z(Z,θ)\neq f_Z(Z,θ_0)\)</span> with non-zero probability</li>
</ol>
</blockquote>
<p>:) <span class="math inline">\(E\big[g(X)\big] &lt; g\big(E[X]\big)\)</span><br />
Let</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(X\)</span> be non-degenerate real valued random variable</li>
<li><span class="math inline">\(g\)</span> be strictly concave</li>
<li><span class="math inline">\(E\big[g(X)\big]\)</span> and <span class="math inline">\(g\big(E[X]\big)\)</span> exist</li>
</ol>
<p>By Jensen’s Inequality, for all <span class="math inline">\(θ\in Θ\)</span>,
<span class="math display">\[\begin{aligned}Q(θ) - Q(θ_0) &amp;= -E\big[\log f_Z(θ)] + E[\log f_Z(θ_0)\big] \\
&amp;= -E\big[\log f_Z(θ) - \log f_Z(θ_0)\big] \\
&amp;= -E\Big[\log\frac{f_Z(θ)}{f_Z(θ_0)}\Big] \\
&amp;&gt; -\log E\Big[\frac{f_Z(θ)}{f_Z(θ_0)}\Big] \\
&amp;= -\log\int \frac{f_Z(θ)}{f_Z(θ_0)}\cdot f_Z(θ_0)\, dZ \\
&amp;\geq -\log\underbrace{\int f_Z(θ)\, dZ}_{=1} \\
&amp;= 0 \end{aligned}\]</span></p>
</div>
<div id="uniform-convergence-in-probability" class="section level3">
<h3>Uniform Convergence in Probability</h3>
<ul>
<li>Ordinary contiuity: <span class="math inline">\(f:Θ\to \mathbb{R}^m\)</span> is continuous at <span class="math inline">\(θ\)</span><br />
if for all <span class="math inline">\(ε&gt;0\)</span>, exists <span class="math inline">\(δ(ε,θ)&gt;0\)</span> s.t. for <span class="math inline">\(\|θ-θ&#39;\|&lt;δ\)</span>,
<span class="math display">\[\sup_{θ&#39;}\| f(θ) -f(θ&#39;)\| &lt; ε \]</span></li>
<li>Uniform contiuity: <span class="math inline">\(f:Θ\to \mathbb{R}^m\)</span> is uniformly continuous<br />
if for all <span class="math inline">\(ε&gt;0\)</span>, exists <span class="math inline">\(δ(ε)&gt;0\)</span> s.t. for <span class="math inline">\(\|θ-θ&#39;\|&lt;δ\)</span>,
<span class="math display">\[\sup_{(θ,θ&#39;)}\| f(θ) -f(θ&#39;)\| &lt; ε \]</span></li>
<li>Equicontinuity</li>
<li>Stochastic contiuity</li>
</ul>
<blockquote>
<p>Def 2.4. Equicontinuity<br />
A sequence <span class="math inline">\(\{f_n\}\)</span> of functions <span class="math inline">\(f_n:Θ\to \mathbb{R}^m\)</span> is equicontinuous<br />
if for all <span class="math inline">\(ε&gt;0\)</span>, exists <span class="math inline">\(δ&gt;0\)</span> s.t. for all <span class="math inline">\(n\)</span>, for <span class="math inline">\(\|θ-θ&#39;\|&lt;δ\)</span>,
<span class="math display">\[\sup_{(θ,θ&#39;)}\| f_n(θ) -f_n(θ&#39;)\| &lt; ε \]</span></p>
</blockquote>
<p>Remark:<br />
Equicontinuity <span class="math inline">\(\approx\)</span> uniform contiuity of all <span class="math inline">\(f_n\)</span>, <span class="math inline">\(δ(n)\)</span></p>
<blockquote>
<p>Def 2.5. Stochastic equicontinuity<br />
A sequence <span class="math inline">\(\{Q_n\}\)</span> of random functions <span class="math inline">\(Q_n:Θ\to \mathbb{R}^m\)</span> is stochastically equicontinuous<br />
if for all <span class="math inline">\(ε,η&gt;0\)</span>, exists <span class="math inline">\(δ&gt;0\)</span> s.t. for sufficiently large <span class="math inline">\(n\)</span>, for <span class="math inline">\(\|θ-θ&#39;\|&lt;δ\)</span>,
<span class="math display">\[P\big(\sup_{(θ,θ&#39;)}\| Q_n(θ) -Q_n(θ&#39;)\|&gt;η\big) &lt; ε \]</span></p>
</blockquote>
<blockquote>
<p>Thm 2.6. Uniform law of large numbers<br />
Let <span class="math inline">\(\{Q_n\}\)</span> be a sequence of random functions,<br />
<span class="math display">\[\sup_{θ\in Θ}\| Q_n(θ) -Q(θ)\|\overset{P}{\to} 0\]</span>
If:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Θ\)</span> is compact</li>
<li><span class="math inline">\(Q_n(θ)\overset{P}{\to}Q(θ)\)</span> pointwise for any <span class="math inline">\(θ\in Θ\)</span></li>
<li><span class="math inline">\(\{Q_n\}\)</span> is stochastically equicontinuous, <span class="math inline">\(Q\)</span> is continuous</li>
</ol>
</blockquote>
<p>Proof:<br />
Let <span class="math inline">\(ε,η&gt;0\)</span>, open ball <span class="math inline">\(B(θ,δ)=\{s\in Θ:\|s-θ\|&lt;δ\}\)</span><br />
Since <span class="math inline">\(Θ\)</span> is compact, <span class="math inline">\(Θ\)</span> can be covered by finitely many balls,<br />
<span class="math inline">\(\implies\)</span> exists <span class="math inline">\(θ_1,\dots,θ_K\in Θ\)</span> s.t. <span class="math inline">\(Θ=\cup_{k=1}^K B(θ_k,δ)\)</span><br />
<span class="math display">\[\|Q_n(θ) -Q(θ)\|\leq\|Q_n(θ) -Q_n(θ_k)\|+\|Q_n(θ_k) -Q(θ_k)\|+\|Q(θ_k) -Q(θ)\|\]</span>
Therefore,
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg}P\big(\sup_{θ\in Θ}\| Q_n(θ) -Q_n(θ)\|&gt;η\big)\\
&amp;= P\big(\max_{k}\sup_{(θ\in B(θ_k))}\ \| Q_n(θ) -Q_n(θ&#39;)\|&gt;η\big)\\
&amp;= P\big(\max_{k}\sup_{(θ\in B(θ_k))}\ \| Q_n(θ) -Q_n(θ_k)\|&gt;\frac{η}{3}\big) &amp;&amp; \text{(I)}\\
&amp;\phantom{\ggg}+ P\big(\max_{k}\sup_{(θ\in B(θ_k))}\ \| Q_n(θ_k) -Q(θ_k)\|&gt;\frac{η}{3}\big) &amp;&amp; \text{(II)}\\
&amp;\phantom{\ggg}+ P\big(\max_{k}\sup_{(θ\in B(θ_k))}\ \| Q(θ_k) -Q(θ)\|&gt;\frac{η}{3}\big) &amp;&amp; \text{(III)}\end{aligned}\]</span></p>
<p>For (III):<br />
As <span class="math inline">\(Q\)</span> is continuous and <span class="math inline">\(Θ\)</span> is compact, <span class="math inline">\(Q\)</span> is uniformly continuous<br />
Therefore,
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg} \max_k\sup_{θ\in B(θ_k)}\ \| Q(θ_k) -Q(θ)\| \\
&amp;\leq \sup_{(θ,θ&#39;:\|θ-θ&#39;\|&lt;δ)}\ \| Q(θ) -Q(θ&#39;)\|\\
&amp;\leq \frac{η}{3}\end{aligned}\]</span>
therefore, (III) <span class="math inline">\(\to 0\)</span> for <span class="math inline">\(δ\)</span> small enough</p>
<p>For (I):<br />
For <span class="math inline">\(δ\)</span> small enough, <span class="math inline">\(n\)</span> sufficiently large,<br />
since <span class="math inline">\(\{Q_n\}\)</span> is stochastically equicontinuous,
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg}P\big(\max_{k}\sup_{θ\in B(θ_k)}\ \| Q_n(θ_k) -Q(θ_k)\|&gt;\frac{η}{3}\big) \\
&amp;\leq P\big(\sup_{(θ,θ&#39;:\|θ-θ&#39;\|&lt;δ)}\ \| Q_n(θ) -Q(θ&#39;)\|&gt;\frac{η}{3}\big)\\
&amp;\leq \frac{ε}{2} \end{aligned}\]</span></p>
<p>For (II):<br />
For <span class="math inline">\(n\)</span> sufficiently large, by (A2),<br />
<span class="math display">\[\begin{aligned}&amp;\phantom{\ggg}P\big(\max_{k}\sup_{(θ\in B)}\ \| Q_n(θ_k) -Q(θ_k)\|&gt;\frac{η}{3}\big) \\
&amp;= P\big(\max_{k}\ \| Q_n(θ_k) -Q(θ_k)\|&gt;\frac{η}{3}\big) \\
&amp;\leq \textstyle{\sum}_{k=1}^K \underbrace{P\big(\| Q_n(θ_k) -Q(θ_k)\|&gt;\frac{η}{3}\big)}_{\to 0 \text{as} n\to\infty} \\
&amp;\leq \frac{ε}{2}\end{aligned}\]</span></p>
<p>Summing up, for <span class="math inline">\(n\)</span> sufficiently large,
<span class="math display">\[\begin{aligned}P\big(\sup_{θ\in Θ}\| Q_n(θ) -Q_n(θ)\|&gt;η\big) &amp;\leq \text{(I)} + \text{(II)} + \text{(III)}\\
&amp;\leq \frac{ε}{2} + \frac{ε}{2} =ε \end{aligned}\]</span></p>
<p>Remark:<br />
Thm 2.6 can be strengthened as:<br />
Uniform convergence <span class="math inline">\(\iff \{Q_n\}\)</span> is stochastically equicontinuous and <span class="math inline">\(Q\)</span> is continuous<br />
If:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Θ\)</span> is compact<br />
</li>
<li><span class="math inline">\(Q_n(θ)\overset{P}{\to}Q(θ)\)</span> pointwise for any <span class="math inline">\(θ\in Θ\)</span></li>
</ol>
<blockquote>
<p>Thm 2.7.<br />
Suppose <span class="math inline">\(B_n=O_p(1)\)</span> is stochastic sequence depending on <span class="math inline">\(θ\)</span>,<br />
Function <span class="math inline">\(h:\mathbb{R}_+\to\mathbb{R}_+\)</span> s.t. <span class="math inline">\(h(δ)\searrow 0\)</span> as <span class="math inline">\(δ\searrow 0\)</span><br />
And for all <span class="math inline">\(θ,θ&#39;\in Θ\)</span>,
<span class="math display">\[\begin{aligned}\|Q_n(θ)-Q_n(θ&#39;)\leq B_n\cdot h(\|θ-θ&#39;\|)\| &amp;&amp; \text{(*)}\end{aligned}\]</span>
Then <span class="math inline">\(\{Q_n\}\)</span> is stochastically equicontinuous on set <span class="math inline">\(Θ\)</span></p>
</blockquote>
<p>Proof:<br />
Let <span class="math inline">\(ε,η&gt;0\)</span>, by (*), for <span class="math inline">\(δ\)</span> small enough,
<span class="math display">\[P\big(B_n\cdot \sup_{\|θ-θ&#39;\|&lt;δ}\ h(\|θ-θ&#39;\|) &gt; η\big) \leq P(B_n\cdot h(δ)&gt;η)\]</span>
Since <span class="math inline">\(B_n=O_p(1)\)</span>, for <span class="math inline">\(n\)</span> large enough, exists <span class="math inline">\(C&gt;0\)</span> s.t. <span class="math inline">\(P(B_n&gt;C\cdot η)&lt;ε\)</span><br />
For <span class="math inline">\(δ\)</span> small enough, <span class="math inline">\(h(δ)&lt;\frac{1}{C}\implies\)</span><br />
<span class="math display">\[\begin{aligned}P(B_n\cdot h(δ)&gt;η) &amp;\leq P(B_n\cdot\frac{1}{C} &gt;η)\\
&amp;= P(B_n &gt; C\cdot η) &lt; ε \end{aligned}\]</span>
therefore, for <span class="math inline">\(δ\)</span> small enough, for <span class="math inline">\(n\)</span> large enough,
<span class="math display">\[P(\sup_{\|θ-θ&#39;\|&lt;δ}\ \|Q_n(θ)-Q_n(θ&#39;)\|&gt;η)&lt;ε\]</span>
<span class="math inline">\(\implies \{Q_n\}\)</span> is stochastically equicontinuous</p>
<blockquote>
<p>Col 2.8.<br />
<span class="math inline">\(\{Q_n\}\)</span> is stochastically equicontinuous and <span class="math inline">\(Q\)</span> is continuous if</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Θ\)</span> is compact</li>
<li><span class="math inline">\(E[\|q(Z,θ)\|]&lt;\infty\)</span> for all <span class="math inline">\(θ\in Θ\)</span></li>
<li>Exists function <span class="math inline">\(b\)</span> not depending on <span class="math inline">\(θ\)</span>, <span class="math inline">\(E[b(Z)]&lt;\infty\)</span>,
<span class="math display">\[\|q(Z,θ)-q(Z,θ&#39;)\| \leq b(Z)\cdot h(\|θ-θ&#39;\|)\]</span></li>
</ol>
</blockquote>
<p>Proof: exercise</p>
<blockquote>
<p>Thm 2.9.<br />
<span class="math inline">\(\{Q_n\}\)</span> is stochastically equicontinuous, and <span class="math inline">\(Q(θ)=E\big[q(θ,z)\big]\)</span> is continuous<br />
If:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(Θ\)</span> is compact</li>
<li><span class="math inline">\(θ\mapsto q(θ,z)\)</span> is continuous in <span class="math inline">\(θ\)</span> for all <span class="math inline">\(z\)</span>, except with probability <span class="math inline">\(0\)</span></li>
<li>Exists <span class="math inline">\(\bar{q}\)</span> s.t. <span class="math inline">\(\|q(θ,z)\|\leq \bar{q}(z)\)</span> for all <span class="math inline">\(θ\)</span>,<br />
and <span class="math inline">\(E\big[\bar{q}(Z)\big]&lt;\infty\)</span></li>
</ol>
</blockquote>
<p>Proof:</p>
<ol style="list-style-type: decimal">
<li>Let <span class="math inline">\(ε,η&gt;0\)</span>, for any <span class="math inline">\(δ&gt;0\)</span>, define:
<span class="math display">\[u(z,δ)=\sup_{\|θ-θ&#39;\|\leq δ}\|q(z,θ)-q(z,θ&#39;)\|\]</span>
By assumptions (i) and (ii),<br />
<span class="math inline">\(\implies\)</span> for all <span class="math inline">\(z\)</span> except with probability <span class="math inline">\(0\)</span>, <span class="math inline">\(u(z,δ)\to 0\)</span> as <span class="math inline">\(δ\to 0\)</span><br />
<span class="math inline">\(\iff u(Z,δ)\to 0\)</span> as <span class="math inline">\(δ\to 0\)</span> almost surely<br />
By assumption (iii),<br />
<span class="math inline">\(\implies\)</span> <span class="math display">\[u(Z,δ)\leq 2\cdot\sup_{θ\in Θ} \|q(Z,θ)\| \leq 2\cdot\bar{q}(Z)\]</span>
Therefore, by dominated convergense,<br />
<span class="math inline">\(\implies E\big[u(Z,δ)\big]\to 0\)</span> as <span class="math inline">\(δ\to 0\)</span><br />
Since
<span class="math display">\[\sup_{\|θ-θ&#39;\|\leq δ}\|Q_n(θ)-Q_n(θ&#39;)\|\leq\frac{1}{n}\sum_{i=1}\underbrace{\sup_{\|θ-θ&#39;\|\leq δ} \|q(Z_i, θ)-q(Z_i, θ&#39;)\|}_{=u(Z_i,δ)}\]</span>
as <span class="math inline">\(δ\to 0\)</span>, it holds that
<span class="math display">\[\begin{aligned}P(\sup_{\|θ-θ&#39;\|\leq δ}\|Q_n(θ)-Q_n(θ&#39;)\|&gt;η)&amp;\leq P(\frac{1}{n}\sum_{i=1} u(Z_i,δ))&gt;η)\\
&amp;\leq \frac{1}{η}E[u(Z,δ)] \to 0 \text{ by Markov}\end{aligned}\]</span>
Therefore, <span class="math inline">\(\{Q_n\}\)</span> is stochastically equicontinuous</li>
<li>For any <span class="math inline">\(θ\in Θ\)</span>, <span class="math inline">\(\|θ-θ&#39;\|&lt;δ\)</span> as <span class="math inline">\(δ\to 0\)</span>,<br />
<span class="math display">\[\begin{aligned}\|Q(θ)-Q(θ&#39;)\| &amp;\leq E[\|q(Z,θ)-q(Z,θ&#39;)\|]\\
&amp;\leq E[u(Z,δ)] \to 0 \end{aligned}\]</span></li>
</ol>
<p>Remark:<br />
Let <span class="math inline">\(δ_n\to 0\)</span> and <span class="math inline">\(U_n=u(Z,δ_n)\)</span>,<br />
<span class="math inline">\(\implies U_n\to 0\)</span> almost surely, <span class="math inline">\(U_n\leq 2\cdot \bar{q}(Z)\)</span> with <span class="math inline">\(E[\bar{q}(Z)]&lt;\infty\)</span><br />
By dominated convergence, <span class="math inline">\(\implies E[U_n]\to 0\)</span></p>
<blockquote>
<p>Example<br />
Let <span class="math inline">\(Y=g(X,θ_0)+ε\)</span>, <span class="math inline">\(E[ε\ |\ X]=0\)</span>, show that <span class="math inline">\(\{Q_n\}\)</span> is stochastically equicontinuous, assuming:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Θ\)</span> is compact</li>
<li><span class="math inline">\(θ\mapsto g(x,θ)\)</span> is continuous for all <span class="math inline">\(x\)</span></li>
<li><span class="math inline">\(|g(x,θ)|\leq\bar{g}(x)\)</span>, with <span class="math inline">\(E[\bar{g}^2(X)]&lt;\infty\)</span></li>
</ol>
</blockquote>
<p>Check Thm 2.9.:</p>
<ol style="list-style-type: lower-roman">
<li>Yes</li>
<li><span class="math inline">\(q(\underbrace{z,θ}_{(x,y)})=\big(y-g(x,θ)\big)^2\)</span> is continuous</li>
<li><span class="math display">\[\begin{aligned}|q(z,θ)|&amp;=(y-g(x,θ))^2\\
&amp;\leq \big(2\cdot \max\{|y|,|g(x,θ)|\}\big)^2 \\
&amp;\leq 4y^2 + 4\cdot g(x,θ)^2 \\
&amp;\leq 4y^2 + 4\cdot\bar{g}(x)^2  \\
:&amp;= \bar{q}(z)\end{aligned}\]</span></li>
</ol>
<blockquote>
<p>Example: maximun likelihood<br />
Let <span class="math inline">\(Z\sim f_Z(\cdot, θ_0)\)</span>, define the likelihood function by
<span class="math display">\[Q_n(θ) = \prod_{i=1} f_Z(Z_i, θ)\]</span>
the ML-estimator of <span class="math inline">\(θ_0\)</span> is defined as
<span class="math display">\[\begin{aligned}\hat{θ}&amp;= \text{argmax}_{θ\in Θ} Q_n(θ)\\&amp;=\text{argmax}_{θ\in Θ}-\frac{1}{n}\sum \log f_Z(Z_i,θ)\end{aligned}\]</span>
<span class="math inline">\(\{Q_n\}\)</span> is stochastically equicontinuous assuming:</p>
<ol style="list-style-type: lower-alpha">
<li><span class="math inline">\(Θ\)</span> is compact</li>
<li><span class="math inline">\(θ\mapsto \log f_Z(z,θ)\)</span> is continuous for all <span class="math inline">\(z\)</span>, except with probability 0</li>
<li>Exists <span class="math inline">\(\bar{q}\)</span> s.t. <span class="math inline">\(|\log f_Z(z,θ)|\leq \bar{q}(z)\)</span> for all <span class="math inline">\(θ\in Θ\)</span> and <span class="math inline">\(E[\bar{q}(Z)]&lt;\infty\)</span></li>
</ol>
</blockquote>
<p>To put it explicitly,
<span class="math display">\[\begin{aligned}f_Z(z,θ_0) &amp;= f_Z(z, μ_0, σ_0^2) \\
&amp;= \frac{1}{\sqrt{2πσ_0^2}}\exp\Big(-\frac{(z-μ_0)^2}{2σ_0^2}\Big) \end{aligned}\]</span>
Let <span class="math inline">\(Θ=[\underline{μ},\bar{μ}]\times[\underline{σ}^2,\bar{σ}^2]\)</span>, with <span class="math inline">\(-\infty&lt;\underline{μ}\leq \bar{μ}&lt;\infty\)</span> and <span class="math inline">\(0&lt;\underline{σ}^2\leq\bar{σ}^2&lt;\infty\)</span><br />
Note that
<span class="math display">\[-\log f_Z(z,μ,σ^2)=\log(\sqrt{2πσ_0^2})+\frac{(z-μ_0)^2}{2σ_0^2}\]</span>
and
<span class="math display">\[\begin{aligned}\log f_Z(z,μ,σ^2) &amp;\leq \max\bigg\{\Big|\log\big(\sqrt{2π\underline{σ}^2}\big)|, \Big|\log\big(\sqrt{2π\bar{σ}^2}\big)\Big|\bigg\} \\
&amp;\phantom{\ggg} + \frac{(|z| + \max\{|\underline{μ}|,|\bar{μ}|\})^2}{2\underline{σ}^2}\end{aligned}\]</span></p>
</div>
<div id="asymphotic-normality" class="section level3">
<h3>Asymphotic Normality</h3>
<ul>
<li>Have shown: <span class="math inline">\(\hat{θ}\overset{P}{\to}θ_0\)</span></li>
<li>Want to show: <span class="math inline">\(\sqrt{n}(\hat{θ}-θ_0)\overset{d}{\to}\mathcal{N}(0,V)\)</span>
<ul>
<li><span class="math inline">\(\implies \sqrt{n}(\hat{θ}-θ_0)=O_p(1)\)</span><br />
<span class="math inline">\(\iff (\hat{θ}-θ_0)=O_p(\frac{1}{\sqrt{n}})\)</span><br />
<span class="math inline">\(\implies (\hat{θ}-θ_0)=o_p(1)\)</span></li>
<li>Stronger than <span class="math inline">\(\hat{θ}\overset{P}{\to}θ\)</span>!</li>
</ul></li>
</ul>
<blockquote>
<p>Thm 3.1. Standard normality<br />
It holds that
<span class="math display">\[\sqrt{n}(\hat{θ}-θ_0)\overset{d}{\to}\mathcal{N}(0,H_0^{-1}Ω_0H_0)\]</span>
assuming <span class="math inline">\(\hat{θ}\overset{P}{\to}θ_0\)</span> and:</p>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(θ_0\in \text{int}(Θ)\)</span></li>
<li><span class="math inline">\(Q_n(θ)\)</span> is twice continuous differentiable in some open neighbourhood <span class="math inline">\(\mathcal{N}\)</span> of <span class="math inline">\(θ_0\)</span></li>
<li>For <span class="math inline">\(S_n(θ_0)=δ_θQ_n(θ_0)\)</span>,
<span class="math display">\[\sqrt{n} S_n(θ_0)\overset{d}{\to}\mathcal{N}(0,Ω_0)\]</span></li>
<li>For <span class="math inline">\(H_n(θ)=δ_{θθ}Q_n(θ)\)</span>, <span class="math inline">\(H(θ)=δ_{θθ}Q(θ)\)</span>,
<span class="math display">\[\sup_{θ\in\mathcal{N}} \|H_n(θ)-H(θ)\|\overset{P}{\to}0\]</span>
and <span class="math inline">\(θ\mapsto H(θ)\)</span> is continuous and <span class="math inline">\(H_0=H(θ_0)\)</span> is non-singular (invertible)</li>
</ol>
</blockquote>
<p>Since <span class="math inline">\(\hat{θ}\)</span> is the minimizer of <span class="math inline">\(Q_n(θ)\)</span>,<br />
By Taylor expansion:
<span class="math display">\[\begin{aligned}0 &amp;= S_n(\hat{θ}) \\
&amp;= δ_θ Q_n(\hat{θ}) \\
&amp;= S_n(θ_0) + \mathcal{H}_n(θ_0,\hat{θ})(\hat{θ}-θ_0) \\
\sqrt{n}(\hat{θ}-θ_0) &amp;= \underbrace{\mathcal{H}_n(θ_0,\hat{θ})^{-1}}_{\overset{P}{\to} H_0^{-1}}\cdot \underbrace{\sqrt{n}S_n(θ_0)}_{\overset{d}{\to}\mathcal{N}(0,Ω_0)}\\
&amp;\overset{d}{\to}\mathcal{N}(0,H_0^{-1}Ω_0H_0)\end{aligned}\]</span></p>
</div>
</div>

    </div>
  </article>
  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  (function() { 
  var d = document, s = d.createElement('script');
  s.src = 'https://loikein-github.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>

</main>
      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>

          <li>By loikein with love</li>
        </ul>
      </footer>
    </div>
    
    <script src="https://hypothes.is/embed.js" async></script>
    
    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-143089736-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>