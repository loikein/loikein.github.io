---
title: "Notes on Mathematics - Optimization and Dynamics"
author: "loikein"
date: "2019-01-30"
slug: "notes-math-optimization-dynamics"
tags: ["notes","math"]
output:
  blogdown::html_page:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = FALSE, tidy = FALSE, echo = TRUE, eval = FALSE)
```

## Optimization

### Lagrange Method

> PS11.Q1  
> Solve $\max_{(x,y)\in \mathbb{R}^2} x+y$ &emsp; s.t. $x^2+y^2=2$ 

Lagrangian: 
$$\mathcal{L}(x,y,λ) = x+y+λ(x^2+y^2-2)$$ 
FOC 
$$\begin{aligned} \frac{\partial\mathcal{L}}{\partial x} = 1 + 2λx = 0 && x &= -\frac{1}{2λ} \\
\frac{\partial\mathcal{L}}{\partial y} = 1 + 2λy = 0 && y &= -\frac{1}{2λ} \\
x^2+y^2=2 && \frac{1}{4λ^2} &= 1 \implies λ = \pm \frac{1}{2}\end{aligned}$$ 
1. When $λ = \frac{1}{2}$  
$$\begin{aligned} x &= -1 \\ y &= -1 \\ x+y &= -2\end{aligned}$$ 
2. When $λ = -\frac{1}{2}$  
$$\begin{aligned} x &= 1 \\ y &= 1 \\ x+y &= 2\end{aligned}$$ 
Therefore, $\max x+y = 2$ 

> PS11.Q4  
> Solve $\max_{x,y} xy$ &emsp; s.t. $5x+2y=20$ 

Lagrangian: 
$$\mathcal{L}(x,y, λ) = xy + λ(5x+2y-20)$$ 
FOC 
$$\begin{aligned} \frac{\partial\mathcal{L}}{\partial x} = y + 5λ = 0 && y = -5λ \\
\frac{\partial\mathcal{L}}{\partial y} = x + 2λ = 0 && x = -2 λ \\
5x+2y-20 = 0 && \end{aligned}$$ 
Solve 
$$\begin{aligned} λ &= -1 \\ x &= 2 \\ y &= 5\end{aligned}$$ 
Therefore, $\max xy = 10$ 

### Kuhn-Tucker Method

> PS11.Q2  
> Solve $\max_{(x,y)\in \mathbb{R}^2} -(x_1-4)^2 - (x_2-4)^2$ &emsp; s.t. $\begin{cases} x_1+x_2\leq 4 \\ 3x_1+x_2\leq 9\end{cases}$ 

Kuhn-Tucker conditions: 
$$\begin{aligned} x_1+x_2\leq 4 &\implies λ_1(4 - x_1 - x_2) \geq 0 \\
3x_1+x_2\leq 9 &\implies λ_2(9 - 3x_1 - x_2) \geq 0\end{aligned}$$ 
Lagrangian: 
$$\begin{aligned} \mathcal{L}(x_1,x_2,λ_1, λ_2) = &-(x_1-4)^2 - (x_2-4)^2 \\ &+ λ_1(4 - x_1 - x_2) + λ_2(9 - 3x_1 - x_2) \end{aligned}$$ 
FOC 
$$\begin{aligned} \frac{\partial\mathcal{L}}{\partial x_1} = -2(x_1-4) - λ_1 - 3λ_2 &= 0 \\
\frac{\partial\mathcal{L}}{\partial x_2} = -2(x_2-4) - λ_1 - λ_2 &= 0 \\
λ_1(4 - x_1 - x_2) &= 0 \\
λ_2(9 - 3x_1 - x_2) &= 0\end{aligned}$$ 

1. $λ_1 = 0$ and $λ_2 = 0$ 
$$\begin{cases} -2(x_1-4) =0 \\ -2(x_2-4) =0\end{cases}$$ 
$\implies x_1 = x_2 = 4$  
$\implies x_1+x_2 \not\leq 4$ (omitted)  
2. $λ_1 > 0$ and $λ_2 = 0$ 
$$\begin{cases} x_1+x_2 = 4 \\ -2(x_1-4) - λ_1 = 0 \\ -2(x_2-4) - λ_1 = 0\end{cases}$$ 
$\implies \begin{cases} x_1 = x_2 = 2 \\ λ_1 = 4 > 0\\ \end{cases}$  
$\implies \begin{cases} x_1+x_2 = 4\leq 4 \\ 3x_1+x_2 = 8 \leq 9\\ \end{cases}$  
All constraints are satisfied, $-(x_1-4)^2 - (x_2-4)^2 = -8$  
3. $λ_1 = 0$ and $λ_2 > 0$ 
$$\begin{cases} 9 - 3x_1 - x_2 = 0 \\ -2(x_1-4) - 3λ_2 = 0 \\ -2(x_2-4)- λ_2 = 0\end{cases}$$ 
$\implies \begin{cases} x_1 = -\frac{3λ_2}{2} + 4 \\ x_2 = -\frac{λ_2}{2} + 4 \\ λ_2 = \frac{7}{5} > 0\end{cases}$  
$\implies x_1+x_2 \not\leq 4$ (omitted)  
4. $λ_1 > 0$ and $λ_2 > 0$ 
$$\begin{cases} x_1+x_2 = 4\\ 3x_1+x_2= 9 \\ -2(x_1-4) - λ_1 - 3λ_2 = 0 \\ -2(x_2-4) - λ_1 - λ_2 = 0\end{cases}$$ 
$\implies \begin{cases} x_1 = \frac{5}{2} \\ x_2 = \frac{3}{2} \\ λ_1 = 6 > 0 \\ λ_2 = -1 < 0\end{cases}$ (omitted)

Therefore, $\max -(x_1-4)^2 - (x_2-4)^2 = -8$  

> PS11.Q3  
> Solve $\max_{(x,y)\in \mathbb{R}^2} x_1^2 + x_2^2$ &emsp; s.t. $\begin{cases} x_1 \geq 0 \\ x_2\geq 0 \\ 3x_1+x_2\leq 9\end{cases}$ 

> PS11.Q5  
> Solve $\max_{x,y} xy$ &emsp; s.t. $\begin{cases} 5x+2y\leq 20 \\ x\geq 0\end{cases}$

### Dual Problem

- Primal Problem: 
$$\begin{aligned} \max_{x\in\mathcal{A}} c^Tx && \mathcal{A} = \{x\in \mathbb{R}^n \ | \ Ax \leq b, x\geq 0\} \end{aligned}$$ 
- Dual Problem: 
$$\begin{aligned} \min_{y\in\mathcal{B}} b^Ty && \mathcal{B} = \{y\in \mathbb{R}^m \ | \ A^Ty \geq c, y\geq 0\} \end{aligned}$$ 
- Weak Duality: $x$ is feasible $\implies c^T x\leq b^T y$ for any feasible $y$  
- Strong Duality: $x^*$ is optimal $\implies y^*$ is optimal, $c^Tx^*=b^Ty^*$ 
- Equilibrium Theorem: feasible $x^*$ and $y^*$ are optimal  
$\iff$ $\begin{cases} y_i^* \cdot(b_i - \sum_j^n a_{ij}x_j^*)= 0 && \text{for all }i \\ x_j^* \cdot ( \sum_i^m a_{ij}y_i^* - c_j) = 0 && \text{for all }j \end{cases}$ 

> PS11.Q6  
> Let $a$ be a strictly positive real number.  
> Consider the following (primal) linear programming problem:  
> $\max 2x_1+4x_2$ &emsp; s.t. $\begin{cases} 3x_1 + 5x_2 \leq a \\ x_1\geq 0 \\ x_2\geq 0\end{cases}$  
> 1.  Write its associated dual problem in terms of $y_1$  
> 2. Obtain the solution of the dual problem. Which constraints bind?  
> 3. Show explicitly that if $(x_1,x_2)$ and $y_1$ are feasible in the primal and dual problems, respectively, then $2x_1+ 4x_2 \leq ay_1$  
> 4. Use the equilibrium theorem to find the solution of the primal problem

1. Rewrite the primal problem:  
$$\max \begin{pmatrix} 2 \\ 4 \end{pmatrix}^T \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix}$$ 
s.t.  
$$\begin{cases} \begin{pmatrix} 3 && 5 \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \leq a \\
\phantom{1234567890}\begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \geq \begin{pmatrix} 0 \\ 0 \end{pmatrix}\end{cases}$$ 
The dual problem:  
$$\min a \cdot y_1$$ 
s.t.  
$$\begin{cases} \begin{pmatrix} 3 \\ 5 \end{pmatrix} \cdot y_1 \geq \begin{pmatrix} 2 \\ 4 \end{pmatrix} \\
\phantom{123456} y_1\geq 0 \end{cases}$$ 
2. The constraints:  
$$\begin{cases} 3y_1 \geq 2 \\ 5y_1\geq 4 \\ \phantom{1} y_1\geq 0 \end{cases}$$ 
Solve:  
$$\begin{aligned}y_1 &= \frac{4}{5} \\ a \cdot y_1 &= a\cdot \frac{4}{5}\end{aligned}$$ 
The binding constraint is $5y_1\geq 4$  
3. Since $x_1,x_2,y_1$ is feasible,  
$$\begin{aligned} Ax &\leq b \\
y^TAx &\leq y^T b\end{aligned}$$ 
and  
$$\begin{aligned} A^Ty &\geq c \\
y^T A &\geq c^T \\
y^T A x &\geq c^T x\end{aligned}$$ 
Therefore,
$$\begin{aligned} c^T x &\leq y^T A x \leq y^T b \\
c^T x &\leq y^T b \\
\begin{pmatrix} 2 \\ 4 \end{pmatrix}^T \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} &\leq y_1\cdot a \\
2x_1+ 4x_2 &\leq ay_1\end{aligned}$$ 
4. Since the first constraint of the dual problem, $3y_1 \geq 2$ is not binding  
$\implies x_1 = 0$  
Since $y_1 = \frac{4}{5} > 0$  
$\implies$ $\begin{pmatrix} 3 && 5 \end{pmatrix} \cdot \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \leq a$ is binding  
$\implies 3x_1 + 5x_2 = a$  
$\implies x_2 = \frac{a}{5}$  
$\implies \max 2x_1+4x_2 = \frac{4a}{5}$ 

## Dynamics

Reference: [Dynamic Optimization user's guide](http://web.stanford.edu/group/econ/uploads/pmwiki/Math.DynamicOptUserGuide.pdf)

- Number of periods: $T \in\mathbb{N}\cup\{+\infty\}$
- Set of actions (States): $X\subset\mathbb{R}^{d}$
    - Markov state variable: $x\in X$
    - Initial state: $x_0 = x_0^*$
    - Set of actions for all periods: $X^T\subset\mathbb{R}^{d\cdot T}$
    - Sequence of decisions (Plan / Program): $(x_t)_{t = 1}^{T} = (x_1, x_2,\dots, x_T) \in X^T$ 
- Set of Feasible Sequences: $A(x_0) \equiv \big\{ (x_t)_{t = 1}^{\infty} \ | \ x_t\in Γ(x_{t-1})$ for all $t\in\mathbb{N}\big\}$
    - $A\subset X^T$
    - $Γ: X\rightrightarrows X$
    $G\big((x_t)_{t = 0}^{T} \big)$  
- (Terminal) Payoff (Object) Function: $G: A\to\mathbb{R}$
- Optimization problem: $\max_A \ G(A)$
    - Generalization: $A\to X^T\to X^{\infty}$
- (Bellman's) Priciple of Optimality  
for an optimal plan $(x^*_t)_{t}^{\infty}$, the remaining decisions must be optimal starting for every state  
$$\begin{aligned}(x^*_t)_{t = 1}^{\infty} &\in \text{argmax}_{(x_t)_{t = 1}^{\infty}} && G\big(x_0^*, (x_t)_{t = 1}^{\infty} \big) \\
\implies (x^*_t)_{t = t'}^{\infty} &\in \text{argmax}_{(x_t)_{t = t'}^{\infty}} && G\big((x_t^*)_{t = 1}^{t'-1}, (x_t)_{t = t'}^{\infty} \big) && \forall t'>0\end{aligned}$$ 

### Recursive Analysis

textbook P553, slide 6 P13

- Recursive structure: $F:X^2\to\mathbb{R}$ s.t. 
$$\begin{aligned}G\big((x_t)_{t = 0}^{\infty} \big) &= \sum_{t=0}^{\infty} β^T\cdot \overbrace{F(x_t,x_{t+1})}^{\text{Time Payoff}} \\
&= F(x_0, x_1) + β\cdot G\big((x_t)_{t = 1}^{\infty} \big)\end{aligned}$$
    - Discount Factor: $β\in [0,1)$
    - Assign to every $x_{t+1}\not\in Γ(x_t)$ a low value of $F(x_t,x_{t+1})$ 
- Assumption
    - (A1): $Γ(x)\neq\emptyset$ for all $x\in X$
    - (A2): $\sum_{t=0}^{\infty} β^T\cdot \big| F(x_t,x_{t+1})\big| <\infty$ for all $(x_t)_{t = 1}^{\infty}\in A(x_0)$
- SP (Sequence Problem) for each $x_0\in X$  
$$\begin{aligned}\max_{(x_t)_{t = 1}^{\infty}} && G\big(x_0, (x_t)_{t = 1}^{\infty} \big)\end{aligned}$$
    - Continuation Payoff $$\begin{aligned}V^*(x_0) &\equiv \sup_{(x_t)_{t = 1}^{\infty}} \ G\big(x_0, (x_t)_{t = 1}^{\infty} \big) \\
    &= G\big(x_0, (x_t^*)_{t = 1}^{\infty} \big) \\
    \implies V^*(x_t^*) &= G\big(x_t^*, (x_{t'})_{t' = t+ 1}^{\infty} \big) && \text{for all }t\end{aligned}$$
- Bellman Equation
$$\begin{aligned}V^*(x_{t-1}^*) = \max_{x\in X} \ \ &F(x_{t-1}^*, x) && \text{for all }t \\
x_t^* \in \text{argmax} \ \ &F(x_{t-1}^*, x)\end{aligned}$$
- Optimality Theorem  
$(x_t^*)_{t = 1}^{\infty}$ is a solution of SP $\iff$ Bellman equation holds
- Markov Solution: $(x_t)_{t = 1}^{\infty}$
    - it is a solution of SP
    - exists optimal policy $g:X\to X$ s.t. $x_{t+1} = g(x_t)$ for all $t\geq 0$
    - SP has unique solution $\iff$ unique Markov solution
- Sufficient Condition: $\implies (x_t^*)_{t = 1}^{\infty}$ is a solution of SP
    - $X$ convex
    - $Γ(x)$ non-empty, compact, continuous
    - $F(x_1,x_2)$ bounded, continuous, strictly increasing in $x_1$, strictly concave, continuously differentiable in $\text{int}(X^2)$
    - Euler Equations $$\begin{aligned}F_{x_2}(x_t^*,x_{t+1}^*) + β\cdot F_{x_1}(x_{t+1}^*,x_{t+2}^*) = 0 && \forall t\geq 0\end{aligned}$$
    - Transversality condition $$\lim_{t\to\infty} \ β^t\cdot F_{x_1}(x_t^*,x_{t+1}^*)\cdot x_t^* = 0$$
    - $(x_t^*)_{t = 1}^{\infty}\in A(x_0)$ is s.t. $x_{t+1}^*\in\text{int} Γ(x_t^*)$

<!-- 
    $\implies \sum_{t=0}^{\infty} β^T\cdot F(x_t,x_{t+1})$ exists for all $(x_t)_{t = 1}^{\infty}\in A(x_0)$
 -->
<!-- 
&= F(x_0, x_1^*) + β\cdot G\big(x_0, (x_t^*)_{t = 2}^{\infty} \big) \\
 -->

> PS12.Q4  
> Consider the following dynamic optimization problem:  
> $$\begin{aligned}\max_{(x_n)_n} && \sum_{n=0}^{\infty}β^n\cdot F(x_n, x_{n+1})\end{aligned}$$
> where $F(x_n,x_{n+1}) = x_{n+1}(x_n−x_{n+1})$.  
> Assume $β = \frac{3}{4}$ and $x_0 = 1$  
> 1. Verify that $V (x) = \frac{1}{3} x^2$ solves the Bellman equation.  
> 2. Compute explicitly the solution $(x_n^*)_n$.  
> 3. Verify that $V(x_0) = \sum_{n=0}^{\infty}β^n\cdot F(x_n^*, x_{n+1}^*)$  
> 4. Verify that the Euler equation holds.  
> 5. Verify that the transversality condition holds.  
> 6. Verify that the Benveniste-Sheinkman envelope theorem holds, that is, $V'(x_n^*) = F_1(x_n^*, x_{n+1}^*)$

1. Bellman equation:  
$$\begin{aligned}V(x_n) = \max_{x_{n+1}} \ F(x_n, x_{n+1}) + βV(x_{n+1})\end{aligned}$$
FOC:  
$$\begin{aligned}\frac{\partial}{\partial x_{n+1}}\big( F(x_n, x_{n+1}) + βV(x_{n+1}) \big)&= 0 \\
\frac{\partial}{\partial x_{n+1}}\bigg(x_{n+1}(x_n−x_{n+1}) + \frac{3}{4}\cdot\frac{1}{3} x_{n+1}^2 \bigg) &= 0 \\
x_n - 2x_{n+1} + \frac{x_{n+1}}{2} &= 0 \\
x_n &= \frac{3}{2}x_{n+1} \\
x_{n+1} &= \frac{2}{3}x_n\end{aligned}$$
Plug in to the right hand side of Bellman equation:  
$$\begin{aligned}V(x_n) &= F(x_n, x_{n+1}) + βV(x_{n+1}) \\
&= x_{n+1}(x_n−x_{n+1}) + \frac{3}{4}\cdot\frac{1}{3} x_{n+1}^2 \\
&= \frac{2}{3}x_n\bigg(x_n−\frac{2}{3}x_n\bigg) + \frac{3}{4}\cdot\frac{1}{3}\bigg(\frac{2}{3}x_n\bigg)^2 \\
&= \frac{2}{9}x_n^2 + \frac{1}{9}x_n^2 \\
&= \frac{1}{3} x_n^2\end{aligned}$$
Coincide with the condition. 
2. Since $x_{n+1}^* = \frac{2}{3}x_n^*$, $x_0 = 1$,  
$x_n^* = \big(\frac{2}{3}\big)^n\cdot x_0 = \big(\frac{2}{3}\big)^n$ for all $n$
3. Left hand side:  
$$\begin{aligned}V(x_0) &= \frac{1}{3}\cdot x_0^2 \\
&= \frac{1}{3}\end{aligned}$$
Right hand side:  
$$\begin{aligned}&\phantom{\ggg}\sum_{n=0}^{\infty}β^n\cdot F(x_n^*, x_{n+1}^*) \\
&= \sum_{n=0}^{\infty} β^n\cdot x_{n+1}^*(x_n^*−x_{n+1}^*) \\
&= \sum_{n=0}^{\infty} \Big(\frac{3}{4}\Big)^n\Big(\frac{2}{3}\Big)^{n+1}\cdot\bigg(\Big(\frac{2}{3}\Big)^{n} - \Big(\frac{2}{3}\Big)^{n+1}\bigg) \\
&= \sum_{n=0}^{\infty} \Big(\frac{3}{4}\Big)^n\Big(\frac{2}{3}\Big)^n\cdot\frac{2}{3}\cdot\Big(\frac{2}{3}\Big)^{n}\cdot\Big(1-\frac{2}{3}\Big) \\
&= \frac{2}{3}\cdot\Big(1-\frac{2}{3}\Big)\cdot\sum_{n=0}^{\infty}\Big(\frac{3}{4}\cdot\frac{2}{3}\cdot\frac{2}{3}\Big)^n \\
&= \frac{2}{9}\cdot\frac{1}{1-\frac{1}{3}} \\
&= \frac{1}{3}\end{aligned}$$
4. Take arbitrary $n\geq 0$  
$$\begin{aligned}&\phantom{\ggg}F_2(x_n^*,x_{n+1}^*) + β\cdot F_1(x_{n+1}^*,x_{n+2}^*) \\
&= \frac{\partial}{\partial x_{n+1}^*}F(x_n^*,x_{n+1}^*) + β\cdot\frac{\partial}{\partial x_{n+1}^*}F(x_{n+1}^*,x_{n+2}^*) \\
&= (x_n^* - 2x_{x+1}^*) + β\cdot x_{x+2}^* \\
&= \Big( \frac{2}{3}\Big)^n - 2\cdot\Big( \frac{2}{3}\Big)^{n+1} + \frac{3}{4}\cdot\Big( \frac{2}{3}\Big)^{n+2} \\
&= \Big( \frac{2}{3}\Big)^n\cdot\Big(1 - 2\cdot \frac{2}{3} + \frac{3}{4}\cdot \big( \frac{2}{3}\big)^2 \Big) \\
&= 0 \end{aligned}$$
5. transversality condition (tbe)
6. Benveniste-Sheinkman envelope theorem (tbe)

### Finite Markov Chain

- State space: $X = \{1,\dots, N\}$ 
- (Stochastic) probability Matrix: $P\in\mathbb{R}^{N\times N}$
    - $P_{ij}\geq 0$ for all $i,j$
    - $\sum_{j = 1}^N P_{ij} = 1$ for all $i$
    - $P_{ij} = Pr$( state i transitions to state j ) $= Pr(x_{t+1} = j \ | \ x_t = i)$ for any $t$
- If $x_0 = i$, probability vector: 
$$\begin{aligned}t = 0 && p_0 &= (\dots, \ 0, \underbrace{1}_{i\text{-th}}, \ 0, \ \dots)' \\
t = 1 && p_1 &= P' \cdot p_0 \\
&& &= (p_{i1}, \ p_{i2}, \ \dots, \ p_{ij})' \\
t = 2 && p_2 &= P'\cdot p_1 \\
&& &= (P')^2\cdot p_0 \\
t && p_t &= P'\cdot p_{t-1} \\
&& &= (P')^t \cdot p_0\end{aligned}$$


> 2019.F1.Q4  
> Fix a binary state space $X\equiv\{1, 2\}$ and a Markov chain.  
> The probability of transitioning from state 1 to state 2 is $α\in [0, 1]$, and the probability of transitioning from state 2 to state 1 is 0.  
> Assume $x_0 = 1$.  
> The per-period payoff in period $t\geq 0$ is $F(x_t,x_{t+1}) = x_t$.  
> 1. Compute the probability that the state is 1 at any time $t\in\mathbb{N}$  
> 2. Give an expression for $E\Big[ \sum_{t=0}^{\infty} β^t\cdot F(x_t,x_{t+1})\Big]$ in terms of $α$ and $β$.  
> Hint: use the corresponding Bellman equation.

1. Compute the probability that the state is 1  
$$\begin{aligned} P &= \begin{bmatrix} 1-α & α \\ 0 & 1 \end{bmatrix}\\
p_0 &= \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
p_t &= (P')^t\cdot p_0 \\
&= \begin{bmatrix} 1-α & 0 \\ α & 1 \end{bmatrix}^t\cdot \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
p_t^1 &= \begin{bmatrix} (1-α)^t & 0\end{bmatrix}\cdot \begin{bmatrix} 1 \\ 0 \end{bmatrix} \\
&= (1-α)^t\end{aligned}$$
2. Let the continuation value at two states $V_1, V_2$, respectively.  
Since $F(x_t,x_{t+1}) = x_t$,  
$$\begin{aligned} V_1 &= 1 + β\cdot\big( (1-α)\cdot V_1 + α\cdot V_2\big) \\
V_2 &= 2 + β\cdot 1\cdot V_2 \\
&= \frac{2}{1-β} \\
\implies V_1 &= 1 + β\cdot\Big( (1-α)\cdot V_1 + α\cdot \frac{2}{1-β}\Big) \\
&= \frac{1+\frac{2αβ}{1-β}}{1-β(1-α)} \\
&= \frac{1-β+2αβ}{(1-β)^2(1-α)}\end{aligned}$$ 

### Backward Induction

textbook P554, slide 6 P9

- When $T$ is finite
- Value Function: the maximum achievable value of $G$  
$$\begin{aligned}\text{Last period:} && V_T\big((x_t)^{T-1}_{t=0}\big) &\equiv \text{max}_{x_T} && G\big((x_t)_{t = 0}^{T} \big) \\
\text{Second last period:} && V_{T-1}\big((x_t)^{T-2}_{t=0}\big) &\equiv \text{max}_{x_T,x_{T-1}} && V\big((x_t)^{T}_{t=1}\big) \\
&& &= \text{max}_{x_{T-1}} && V_T\big((x_t)^{T-1}_{t=0}\big) \\
&& &= \text{max}_{x_T,x_{T-1}} && G\big((x_t)_{t = 0}^{T} \big) \\
\text{First period:} && V_1(x_0) &= \text{max}_{(x_t)_{t=1}^T} && G\big((x_t)_{t = 0}^{T} \big)\end{aligned}$$

<!--
\text{Period }t: && V_{t'}\big((x_{t'})^{t-1}_{t'=0}\big) &= \text{max}_{x_{t}} && V_{t+1}\big((x_{t'})^{t}_{t'=0}\big) \\
-->