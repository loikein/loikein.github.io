<!DOCTYPE html>
<html lang="en-gb">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.72.0" />

<title>Notes on Sensitivity and Copulas - loikein&#39;s notes</title>
<meta property="og:title" content="Notes on Sensitivity and Copulas - loikein&#39;s notes">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-143089736-1', 'auto');
  
  ga('send', 'pageview');
}
</script>


  <link rel='icon' href='/favicon.ico' type='image/x-icon'/>


  






<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />


<link rel="stylesheet" href="/css/normalize.css" media="all">
<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">


<link rel="stylesheet" href="/css/clumsy-toc.css">

<link rel="stylesheet" href="/css/jump-after-toc.css">



</head>
<body>
<section class="a11y-links">
<button class="a11y-links__link" tabindex="0" onclick="jumpAfterTOC()">Skip to main content</button>
</section>
<header class="header">
<nav class="nav">
<a href="/" class="nav-logo">
<img src="/logo.png" alt="Home">
</a>
<ul class="nav-links">


  <li>
  <a class=" active" href="/post/">Notes</a>
  </li>

  <li>
  <a class="" href="/writing/">Writings</a>
  </li>

  <li>
  <a class="" href="/tags/">Tags</a>
  </li>

  <li>
  <a class="" href="/archived/">Archived</a>
  </li>

  <li>
  <a class="" href="https://duckduckgo.com/?q=site%3Anotes.loikein.one">Search</a>
  </li>

  <li>
  <a class="" href="https://github.com/loikein/loikein.github.io">Source</a>
  </li>

</ul>
</nav>

</header>

<main class="content" role="main">
  <article class="article">
    
    <span class="article-duration">18 min read</span>
    
    <h1 class="article-title">Notes on Sensitivity and Copulas</h1>
    
    <span class="article-date">2020-05-14</span>
    
    
    
    <span class="tags">
    
    
    Tags:
    
    <a href='/tags/micro'>micro</a>
    
    
    
    </span>
    
    <div class="article-content">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#reference">Reference</a>
<ul>
<li><a href="#relevant-repositories">Relevant Repositories</a></li>
<li><a href="#links">Links</a></li>
<li><a href="#papers-books">Papers &amp; Books</a></li>
</ul></li>
<li><a href="#working-with-econsa">Working With <span><code>econsa</code></span></a>
<ul>
<li><a href="#eoq-economic-order-quantity-model">EOQ (Economic Order Quantity) Model</a></li>
<li><a href="#main-code">Main Code</a></li>
<li><a href="#figure-2-from-borgonovo-plischke-2016">Figure 2 from Borgonovo &amp; Plischke (2016)</a></li>
<li><a href="#figure-3-from-borgonovo-plischke-2016">Figure 3 from Borgonovo &amp; Plischke (2016)</a></li>
<li><a href="#figure-4-from-borgonovo-plischke-2016">Figure 4 from Borgonovo &amp; Plischke (2016)</a></li>
<li><a href="#figure-1-from-harris-1990">Figure 1 from Harris (1990)</a></li>
</ul></li>
<li><a href="#copulas">Copulas</a>
<ul>
<li><a href="#basic-notations">Basic Notations</a></li>
<li><a href="#definitions-theorems-for-2-dim-space">Definitions &amp; Theorems for 2-Dim Space</a></li>
<li><a href="#copula-sampling">Copula Sampling</a></li>
<li><a href="#families-of-copulas">Families of Copulas</a></li>
</ul></li>
<li><a href="#local-sensitivity-analysis">Local Sensitivity Analysis</a>
<ul>
<li><a href="#one-at-a-time-method">One at a Time Method</a></li>
<li><a href="#scenario-decomposition">Scenario Decomposition</a></li>
<li><a href="#one-way-sensitivity-function">One Way Sensitivity Function</a></li>
<li><a href="#differentiation-based-methods">Differentiation-Based Methods</a></li>
</ul></li>
<li><a href="#global-sensitivity-analysis">Global Sensitivity Analysis</a>
<ul>
<li><a href="#non-parametric-methods">Non-Parametric Methods</a></li>
<li><a href="#variance-based-methods">Variance-Based Methods</a></li>
<li><a href="#density-based-methods">Density-Based Methods</a></li>
<li><a href="#transformation-invariant-methods">Transformation Invariant Methods</a></li>
</ul></li>
<li><a href="#decision-sensitivity-find-the-maximiser">Decision Sensitivity (Find the Maximiser)</a></li>
<li><a href="#estimate-sensitivity">Estimate Sensitivity</a></li>
</ul>
</div>

<p><a href="https://app.clickup.com/2516680/v/dc/2ctp8-134/2ctp8-57">Thesis Roadmap</a></p>
<div id="reference" class="section level2">
<h2>Reference</h2>
<div id="relevant-repositories" class="section level3">
<h3>Relevant Repositories</h3>
<ul>
<li><a href="https://github.com/jonathf/chaospy">jonathf/chaospy</a>
<ul>
<li><a href="https://chaospy.readthedocs.io/en/master/index.html">ChaosPy documentation</a></li>
</ul></li>
<li><a href="https://github.com/OpenSourceEconomics/econsa">OpenSourceEconomics/econsa</a>
<ul>
<li><a href="https://econsa.readthedocs.io/en/latest/">econsa 0.01 documentation</a></li>
</ul></li>
<li><a href="https://github.com/OpenSourceEconomics/tespy">OpenSourceEconomics/tespy</a></li>
<li><a href="https://github.com/covid-19-impact-lab/sid">covid-19-impact-lab/sid</a></li>
<li><a href="https://github.com/simetenn/uncertainpy">simetenn/uncertainpy</a>
<ul>
<li><a href="https://uncertainpy.readthedocs.io/en/latest/index.html">Uncertainpy 1.2.1 documentation</a></li>
</ul></li>
</ul>
</div>
<div id="links" class="section level3">
<h3>Links</h3>
<ul>
<li>(Finished) <a href="https://towardsdatascience.com/introducing-copula-in-monte-carlo-simulation-9ed1fe9f905">Introducing Copula in Monte Carlo Simulation - Towards Data Science</a></li>
<li>(Finished) <a href="https://chaospy.readthedocs.io/en/master/distributions/index.html">Distributions — ChaosPy documentation</a>
<ul>
<li>(Reading) <a href="https://chaospy.readthedocs.io/en/master/distributions/copulas.html">Copulas — ChaosPy documentation</a></li>
</ul></li>
<li><a href="https://uncertainpy.readthedocs.io/en/latest/theory/rosenblatt.html#rosenblatt">Dependency between uncertain parameters — Uncertainpy 1.2.1 documentation</a></li>
<li><a href="https://ose-resources.readthedocs.io/en/latest/miscellaneous.html#continuous-integration">Continuous Integration — ose-resources documentation</a></li>
</ul>
</div>
<div id="papers-books" class="section level3">
<h3>Papers &amp; Books</h3>
<ul>
<li>(Finished) <a href="https://www.sciencedirect.com/science/article/abs/pii/S0377221715005469">Sensitivity analysis: A review of recent advances - ScienceDirect</a></li>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0951832017300625?via%3Dihub">Extending Morris method for qualitative global sensitivity analysis of models with dependent inputs - ScienceDirect</a></li>
<li><a href="https://ethz.ch/content/dam/ethz/special-interest/baug/ibk/risk-safety-and-uncertainty-dam/publications/master-theses/master-thesis-pages/pwiederkehr_MSc.pdf">Global Sensitivity Analysis with Dependent Inputs</a></li>
<li>Chapter 1.5 of <a href="https://ethz.ch/content/dam/ethz/special-interest/baug/ibk/risk-safety-and-uncertainty-dam/publications/doctoral-theses/ThesisCaniou.pdf">Global Sensitivity Analysis for Nested and Multiscale Modelling</a></li>
<li>Section 3 of <a href="https://www.sciencedirect.com/science/article/pii/S1877750315300119">Chaospy: An open source tool for designing methods of uncertainty quantification - ScienceDirect</a></li>
<li>Section 4 of <a href="https://www.sciencedirect.com/science/article/pii/S0010465511004085">Estimation of global sensitivity indices for models with dependent variables - ScienceDirect</a></li>
<li><a href="https://www.amazon.de/dp/B00OD4140E/">Dependence Modeling with Copulas (Chapman &amp; Hall/CRC Monographs on Statistics and Applied Probability Book 133) (English Edition) eBook: Joe, Harry</a></li>
<li><a href="https://www.amazon.de/dp/B00DZ0OVMU/">An Introduction to Copulas (Springer Series in Statistics) (English Edition) eBook: Nelsen, Roger B.</a></li>
</ul>
</div>
</div>
<div id="working-with-econsa" class="section level2">
<h2>Working With <a href="https://github.com/OpenSourceEconomics/econsa"><code>econsa</code></a></h2>
<div id="eoq-economic-order-quantity-model" class="section level3">
<h3>EOQ (Economic Order Quantity) Model</h3>
<p>From: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0377221715005469">Borgonovo &amp; Plischke (2016)</a></p>
<ul>
<li>Notation from <a href="https://pubsonline.informs.org/doi/abs/10.1287/opre.38.6.947">Harris (1990)</a>:
<span class="math display">\[\begin{aligned}T &amp;= \frac{1}{2\cdot 12R\cdot M}(CX + S) +\frac{S}{X} + C \\
X^* &amp;= \sqrt{\frac{24R\cdot MS}{C}} \\
T^* &amp;= \Big( \sqrt{\frac{S}{24R\cdot M}}+\sqrt{C} \Big)^2 \end{aligned}\]</span>
<ul>
<li>Total cost: <span class="math inline">\(T\)</span></li>
<li>Interest &amp; depreciation cost: <span class="math inline">\(R=10\)</span></li>
<li>Unit price of items: <span class="math inline">\(C\)</span></li>
<li>Units per month: <span class="math inline">\(M\)</span></li>
<li>Ordering cost: <span class="math inline">\(S\)</span></li>
</ul></li>
<li>Notation from this paper:
<span class="math display">\[\begin{aligned}y &amp;= g(x) \\
&amp;= \sqrt{\frac{24r\cdot x_1 x_3}{x_2}} \end{aligned}\]</span></li>
<li>Output: <span class="math inline">\(\mathbf{y}\in\mathcal{Y}\subset\mathbb{R}^D\)</span></li>
<li>Inputs: <span class="math inline">\(\mathbf{x}\in\mathcal{X}\subset\mathbb{R}^n\)</span>
<ul>
<li>Units per month: <span class="math inline">\(x_1\)</span></li>
<li>Unit price of items: <span class="math inline">\(x_2\)</span></li>
<li>Ordering cost: <span class="math inline">\(x_3\)</span></li>
</ul></li>
<li>Technology: <span class="math inline">\(g: \mathcal{Y}\to \mathcal{X}\)</span>
<ul>
<li>Interest &amp; depreciation cost: <span class="math inline">\(r=10\)</span></li>
</ul></li>
<li>Subset of inputs: <span class="math inline">\(\mathbf{x}_{\alpha} = \{x_{i_1}, x_{i_2}, \dots, x_{i_k}\}\)</span>
<ul>
<li>Index: <span class="math inline">\(\alpha = \{i_1, i_2, \dots, i_k\}\)</span>, <span class="math inline">\(k\leq n\)</span></li>
<li>Complementary set: <span class="math inline">\(\mathbf{x}_{\sim\alpha}\)</span></li>
</ul></li>
<li>Sensitivity of <span class="math inline">\(x_i\)</span>: sensitivity of model output wrt <span class="math inline">\(x_i\)</span>
<ul>
<li>Also: importance measure</li>
</ul></li>
</ul>
</div>
<div id="main-code" class="section level3">
<h3>Main Code</h3>
<p>Set up:</p>
<pre class="python"><code>import numpy as np
import scipy.stats as stats
import chaospy as cp
import matplotlib.pyplot as plt
import seaborn as sns</code></pre>
<p>Code for this model:</p>
<pre class="python"><code>def eoq_harris(params, x):
    &quot;&quot;&quot;
    Economic order quantity model by Harris (1990),
    https://doi.org/10.1287/opre.38.6.947,
    as seen in Borgonovoa &amp; Plischkeb (2016),
    https://doi.org/10.1016/j.ejor.2015.06.032
    
    Equation: y = sqrt(24*r*x1*x3 / x2),
    where r is interest &amp; depreciation rate,
    and takes a value of 10 in both papers.
    
    Args: 
        params (np.array): 1d numpy array,
                           cuurrently only take the first param,
                           which is interest &amp; depreciation rate, r=10.
        x (np.array or list): 2d numpy array with the independent variables,
                              currently only take the first 3 columns.
    Output:
        y (np.array): 1d numpy array with the dependent variables.
    &quot;&quot;&quot;
    
    x_np = np.array(x)
    r = params.flatten()[0]
    
    y = np.zeros(x_np.T.shape[0])
    y = np.sqrt((24 * r * x_np[0] * x_np[2])/x_np[1])
    
    return(y)</code></pre>
<p>Code for generating data (Monte Carlo):</p>
<pre class="python"><code># Set flags

seed = 1234
n = 10000

x_min_multiplier = 0.9
x_max_multiplier = 1.1
x0_1 = 1230
x0_2 = 0.0135
x0_3 = 2.15

params = np.zeros(shape=(1,1))
params[0,0] = 10

# Monte Carlo with rvs
np.random.seed(seed)
x_1 = stats.uniform(x_min_multiplier*x0_1,
                    x_max_multiplier*x0_1).rvs(10000)
x_2 = stats.uniform(x_min_multiplier*x0_2,
                    x_max_multiplier*x0_2).rvs(10000)
x_3 = stats.uniform(x_min_multiplier*x0_3,
                    x_max_multiplier*x0_3).rvs(10000)
x = np.array([x_1, x_2, x_3])

# Monte Carlo with Chaospy
sample_rule = &quot;random&quot;
np.random.seed(seed)
x_1 = cp.Uniform(x_min_multiplier*x0_1,
                 x_max_multiplier*x0_1).sample(n, rule=sample_rule)
x_2 = cp.Uniform(x_min_multiplier*x0_2,
                 x_max_multiplier*x0_2).sample(n, rule=sample_rule)
x_3 = cp.Uniform(x_min_multiplier*x0_3,
                 x_max_multiplier*x0_3).sample(n, rule=sample_rule)
x = np.array([x_1, x_2, x_3])

# Calculate y
y = eoq_harris(params, x)</code></pre>
</div>
<div id="figure-2-from-borgonovo-plischke-2016" class="section level3">
<h3>Figure 2 from Borgonovo &amp; Plischke (2016)</h3>
<p>Original:</p>
<p><img src="/post-img/notes-rosenblatt-copulas--bp-fig2.png" width="603" /></p>
<pre class="python"><code>plt.clf()
sns.distplot(y, hist_kws=dict(cumulative=True))

plt.clf()
sns.distplot(y)</code></pre>
</div>
<div id="figure-3-from-borgonovo-plischke-2016" class="section level3">
<h3>Figure 3 from Borgonovo &amp; Plischke (2016)</h3>
<p>Original:</p>
<p><img src="/post-img/notes-rosenblatt-copulas--bp-fig3.png" width="702" /></p>
<pre class="python"><code>plt.clf()
sns.regplot(x=x[0], y=y,
            scatter_kws={&quot;alpha&quot;:0.05})

plt.clf()
sns.regplot(x=x[1], y=y, order=2,
            scatter_kws={&quot;alpha&quot;:0.05})

plt.clf()
sns.regplot(x=x[2], y=y,
            scatter_kws={&quot;alpha&quot;:0.05}) </code></pre>
</div>
<div id="figure-4-from-borgonovo-plischke-2016" class="section level3">
<h3>Figure 4 from Borgonovo &amp; Plischke (2016)</h3>
<p>In this graph, we need to calculate <span class="math inline">\(y\)</span> for every single value of one of <span class="math inline">\(x_i\)</span>, resulting in a 10000*10000 array.</p>
<p><img src="/post-img/notes-rosenblatt-copulas--bp-fig4.png" /></p>
<p>Function:</p>
<pre class="python"><code>def eoq_harris_partial(params, x, fix_num=0):
    &quot;&quot;&quot;
    Calculate the value of eoq_harris,
    fixing one x.
    
    Args: 
        params (np.array): 1d numpy array,
                           cuurrently only need the first param,
                           which is interest &amp; depreciation rate, r=10.
        x (np.array or list): 2d numpy array with the independent variables,
                              currently only need the first 3 columns.
        fix_num (int): take value of 0~n-1.
    Output:
        y (np.array): 2d numpy array with the dependent variables,
                      keeping the fix_num-th x fixed.
    &quot;&quot;&quot;
    
    x_np = np.array(x)
    r = params.flatten()[0]
    
    y = np.zeros(shape=(x_np.T.shape[0],x_np.T.shape[0]))
    
    if fix_num==0:
        for i,x_i in enumerate(x_np[fix_num]):
            y[i] = np.sqrt((24 * r * x_i * x_np[2])/x_np[1])
    elif fix_num==1:
        for i,x_i in enumerate(x_np[fix_num]):
            y[i] = np.sqrt((24 * r * x_np[0] * x_np[2])/x_i)
    elif fix_num==2:
        for i,x_i in enumerate(x_np[fix_num]):
            y[i] = np.sqrt((24 * r * x_np[0] * x_i)/x_np[1])
    return(y)</code></pre>
<p>Calculate &amp; plotting:</p>
<pre class="python"><code>y_fix_x_0 = eoq_harris_partial(params, x, fix_num=0)

# Don&#39;t try at home! You are warned!
plt.clf()
for item in y_fix_x_0:
    sns.kdeplot(item)</code></pre>
</div>
<div id="figure-1-from-harris-1990" class="section level3">
<h3>Figure 1 from Harris (1990)</h3>
<p>Since this figure is using deterministic data, we must use different data generation process for it.</p>
<p><img src="/post-img/notes-rosenblatt-copulas--harris-fig1.png" width="479" /></p>
<p>Function:</p>
<pre class="python"><code>def eoq_harris_total_cost(params, x, y):
    &quot;&quot;&quot;
    Economic order quantity model by Harris (1990),
    https://doi.org/10.1287/opre.38.6.947,
    as seen in Borgonovoa &amp; Plischkeb (2016),
    https://doi.org/10.1016/j.ejor.2015.06.032
    
    For plotting convenience, the total cost here excludes ordering cost,
    since it is assumed to be constant, as in Harris (1990).
    
    Args: 
        params (np.array): 1d numpy array,
                           cuurrently only take the first param,
                           which is interest &amp; depreciation rate, r=10.
        x (np.array or list): 1d numpy array with the independent variables,
                              unis per month, unit cost, ordering cost.
        y (np.array): 1d numpy array, the size of order.
    Output:
        t (np.array): 1d numpy array, total cost according to each size.
    &quot;&quot;&quot;
    
    x_np = np.array(x)
    params_np = np.array(params)
    r = params_np.flatten()[0]
    
    t = np.zeros(y.shape)
    
    t_setup = np.zeros(y.shape)
    t_setup = (1/y) * x[2]
    
    t_interest = np.zeros(y.shape)
    t_interest = 1/(24 * r * x[0]) * (y*x[1] + x[2])
    
    t = t_setup + t_interest
    
    return(t_setup, t_interest, t)</code></pre>
<p>Data generation:</p>
<pre class="python"><code>y = np.arange(300,5200,1)

x_1 = 1000
x_2 = 0.1
x_3 = 2

x = np.array([x_1, x_2, x_3])

t_setup, t_interest, t = eoq_harris_total_cost(params, x, y)</code></pre>
<p>Plotting:</p>
<pre class="python"><code>plt.clf()
sns.lineplot(x=y, y=t_setup)
sns.lineplot(x=y, y=t_interest)
sns.lineplot(x=y, y=t)
plt.axvline(2190, linestyle=&quot;--&quot;, color=&quot;silver&quot;)</code></pre>
</div>
</div>
<div id="copulas" class="section level2">
<h2>Copulas</h2>
<div id="basic-notations" class="section level3">
<h3>Basic Notations</h3>
<ul>
<li>Little things
<ul>
<li>Real numbers: <span class="math inline">\(x\)</span>, <span class="math inline">\(y\)</span></li>
<li>Random variables: <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span></li>
<li>Density functions: <span class="math inline">\(F(x)=\Pr (X\leq x)\)</span>, <span class="math inline">\(G(y)=\Pr (Y\leq y)\)</span></li>
<li>Joint density function: <span class="math inline">\(H(x,y)=\Pr (X\leq x, Y\leq y)\)</span></li>
<li>Margins of joint density function:
<span class="math display">\[\begin{aligned}F(x) &amp;= H(x,\infty) \\ G(y) &amp;= H(\infty, y)\end{aligned}\]</span></li>
<li>Survival functions: <span class="math inline">\(\bar{F}(x)=\Pr(X&gt;x)\)</span>, <span class="math inline">\(\bar{G}(y)=\Pr(Y&gt;y)\)</span></li>
<li>Joint survival function: <span class="math inline">\(\bar{H}(x,y)=\Pr(X&gt;x, Y&gt;y)\)</span></li>
</ul></li>
<li>Spacial things
<ul>
<li>Real line: <span class="math inline">\(\mathbb{R}=(-\infty, \infty)\)</span></li>
<li>Extended real line: <span class="math inline">\(\bar{\mathbb{R}}=[-\infty, \infty]\)</span></li>
<li>Extended real plane: <span class="math inline">\(\bar{\mathbb{R}}^2=\bar{\mathbb{R}}\times\bar{\mathbb{R}}\)</span></li>
<li>Rectangle: <span class="math inline">\(B=[x_1,x_2]\times [y_1,y_2]\in\bar{\mathbb{R}}^2\)</span></li>
<li>Unit square: <span class="math inline">\(\mathbf{I}^2=[0,1]\times [0,1]\)</span></li>
<li>2-place real function: <span class="math inline">\(H: \text{Dom}H\to\text{Ran}H\)</span></li>
<li>Domain of <span class="math inline">\(H\)</span>: <span class="math inline">\(\text{Dom}H\subset\bar{\mathbb{R}}^2\)</span></li>
<li>Range of <span class="math inline">\(H\)</span>: <span class="math inline">\(\text{Ran}H\subset\mathbb{R}\)</span></li>
</ul></li>
<li>Copula-ish things
<ul>
<li>Subcopula: <span class="math inline">\(C&#39;\)</span></li>
<li>Copula: <span class="math inline">\(C\)</span></li>
<li>Survival copula: <span class="math display">\[\widehat{C}(x,y)=x+y+C(1-x,1-y)-1\]</span></li>
<li>Joint survival function for <span class="math inline">\(X,Y\sim U(0,1)\)</span>: <span class="math display">\[\bar{C}(x,y)=1-x-y+C(x,y)\]</span></li>
<li>Dual of copula: <span class="math display">\[\widetilde{C}(x,y)=x+y-C(x,y)\]</span></li>
<li>Co-copula: <span class="math display">\[C^*=1-C(1-x, 1-y)\]</span></li>
<li>Frechét-Hoeffding upper bound: <span class="math inline">\(M(x,y)=\min(x,y)\)</span></li>
<li>Frechét-Hoeffding lower bound: <span class="math inline">\(W(x,y) = \max(x+y-1,0)\)</span></li>
<li>Product copula: <span class="math inline">\(\Pi(x,y)=xy\)</span></li>
</ul></li>
</ul>
<p>The great copulas table: (<a href="https://www.springer.com/de/book/9780387286594">Nelsen, 2006</a>, p.35)</p>
<table>
<thead>
<tr class="header">
<th></th>
<th>i</th>
<th>^</th>
<th>~</th>
<th>*</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>i</td>
<td><span class="math inline">\(C\)</span></td>
<td><span class="math inline">\(\widehat{C}\)</span></td>
<td><span class="math inline">\(\widetilde{C}\)</span></td>
<td><span class="math inline">\(C^*\)</span></td>
</tr>
<tr class="even">
<td>^</td>
<td></td>
<td><span class="math inline">\(C\)</span></td>
<td><span class="math inline">\(C^*\)</span></td>
<td><span class="math inline">\(\widetilde{C}\)</span></td>
</tr>
<tr class="odd">
<td>~</td>
<td></td>
<td></td>
<td><span class="math inline">\(C\)</span></td>
<td><span class="math inline">\(\widehat{C}\)</span></td>
</tr>
<tr class="even">
<td>*</td>
<td></td>
<td></td>
<td></td>
<td><span class="math inline">\(C\)</span></td>
</tr>
</tbody>
</table>
<!-- Marginal density functions:
$$\begin{aligned}H(x) &= \int_{-\infty}^{\infty} H(x,y)\, dy\\
H(y) &= \int_{-\infty}^{\infty} H(x,y)\, dx\end{aligned}$$
 -->
</div>
<div id="definitions-theorems-for-2-dim-space" class="section level3">
<h3>Definitions &amp; Theorems for 2-Dim Space</h3>
<blockquote>
<p>Definition 2.1.1. H-Volume of Rectangle <span class="math inline">\(B\)</span></p>
<p>Let the first order differences be</p>
<p><span class="math display">\[\begin{aligned}\Delta_{x_1}^{x_2}H(x,y) &amp;= H(x_2,y)-H(x_2,y)\\
\Delta_{y_1}^{y_2} &amp;= H(x,y_2)-H(x,y_1) \end{aligned}\]</span></p>
<p>Then</p>
<p><span class="math display">\[\begin{aligned}V_H(B) &amp;= \big(H(x_2,y_2)-H(x_2,y_1)\big) - \big( H(x_1,y_2)-H(x_1,y_1)\big)\\
&amp;= \Delta_{y_1}^{y_2} \Delta_{x_1}^{x_2} H(x,y) \end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p>Definition 2.1.2. 2-Increasing Function (Quasi-Monotone Function)</p>
<p>2-place real function: <span class="math inline">\(H: \text{Dom}H\to\text{Ran}H\)</span> is 2-increasing<br />
if <span class="math inline">\(V_H(B)\geq 0\)</span> for all <span class="math inline">\(B\in \text{Dom}H\)</span></p>
<p>H-volume for such functions are called <strong>H-measure</strong>.</p>
</blockquote>
<p>Remark: <span class="math inline">\(H\)</span> is 2-increasing and grounded <span class="math inline">\(\implies H\)</span> is non-decreasing in each argument</p>
<blockquote>
<p>Lemma 2.1.3.</p>
<p>Let <span class="math inline">\(S_1,S_2\subset\bar{\mathbb{R}}\)</span> be non-empty,<br />
<span class="math inline">\(x_1&lt;x_2\in S_1\)</span>, <span class="math inline">\(y_1&lt;y_2\in S_2\)</span>,<br />
<span class="math inline">\(H\)</span> be 2-increasing, <span class="math inline">\(\text{Dom}H=S_1\times S_2\)</span>.</p>
<p>Then <span class="math inline">\(t\mapsto H(t, y_2) - H(t,y_1)\)</span> is non-decreasing on <span class="math inline">\(S_1\)</span>,<br />
<span class="math inline">\(t\mapsto H(x_2,t) - H(x_1,t)\)</span> is non-decreasing on <span class="math inline">\(S_2\)</span>.</p>
</blockquote>
<blockquote>
<p>Grounded Function</p>
<p>Let <span class="math inline">\(S_1\)</span> has a least element <span class="math inline">\(a_1\)</span>, <span class="math inline">\(S_2\)</span> has a least element <span class="math inline">\(a_2\)</span>,<br />
<span class="math inline">\(H: S_1\times S_2\to\mathbb{R}\)</span> is grounded<br />
if <span class="math inline">\(H(x,a_2)= H(a_1, y)=0\)</span> for all <span class="math inline">\((x,y)\in S_1\times S_2\)</span>.</p>
</blockquote>
<blockquote>
<p>Lemma 2.1.5.</p>
<p>Let <span class="math inline">\(S_1,S_2\subset\bar{\mathbb{R}}\)</span> be non-empty,<br />
<span class="math inline">\(x_1,x_2\in S_1\)</span>, <span class="math inline">\(y_1,y_2\in S_2\)</span>,<br />
<span class="math inline">\(H\)</span> be 2-increasing and grounded, has margins (??), <span class="math inline">\(\text{Dom}H=S_1\times S_2\)</span>,</p>
<p>Then</p>
<p><span class="math display">\[\big|H(x_2,y_2) - H(x_1,y_1)\big| \leq \big| F(x_2)-F(x_1)\big| + \big|G(y_2)-G(y_1)\big|\]</span></p>
</blockquote>
<blockquote>
<p>Definition 2.2.1. Subcopula</p>
<p>Let <span class="math inline">\(S_1,S_2\subset I\)</span> both contain 0 and 1,<br />
a 2-subcopula, or subcopula, is a function <span class="math inline">\(C&#39;\)</span> that:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{Dom}C&#39; = S_1\times S_2\)</span></li>
<li><span class="math inline">\(C&#39;\)</span> is grounded and 2-increasing</li>
<li>For every <span class="math inline">\(x\in S_1\)</span> and <span class="math inline">\(y\in S_2\)</span>,
<span class="math display">\[\begin{cases}C&#39;(x,1) &amp;= x \\ C&#39;(1,y) &amp;= y \end{cases}\]</span></li>
</ol>
</blockquote>
<p>Remark: (3) <span class="math inline">\(\implies\text{Ran}C&#39;\subset I\)</span>.</p>
<blockquote>
<p>Definition 2.2.2. Copula</p>
<p>A copula <span class="math inline">\(C\)</span> is a subcopula with <span class="math inline">\(\text{Dom}C=I^2\)</span>.</p>
</blockquote>
<p>Remarks:</p>
<ol style="list-style-type: decimal">
<li>For every <span class="math inline">\(x,y\in I\)</span>,
<span class="math display">\[\begin{cases}C(x,0)=0 \\ C(0,y)=0 \end{cases}\]</span>
<span class="math display">\[\begin{cases}C(x,1)=x \\ C(1,y)=y \end{cases}\]</span></li>
<li>For <span class="math inline">\(x_1\leq x_2\in I\)</span>, <span class="math inline">\(y_1\leq y_2\in I\)</span>,
<span class="math display">\[\big(C(x_2,y_2)-C(x_2,y_1)\big) - \big(C(x_1,y_2)-C(x_1,y_1)\big)\geq 0\]</span></li>
</ol>
<blockquote>
<p>Theorem 2.2.3.</p>
<p>Let <span class="math inline">\(C&#39;\)</span> be a subcopula,<br />
then for every <span class="math inline">\((x,y)\in\text{Dom}C&#39;\)</span>,</p>
<p><span class="math display">\[\max(x+y01,\ 0) \leq C&#39;(x,y) \leq \min(x,y)\]</span></p>
</blockquote>
<p>Remark:</p>
<p>Since every copula is also a subcopula,<br />
let <span class="math inline">\(W(x,y) = \max(x+y01,\ 0)\)</span>, <span class="math inline">\(M(x,y)=\min(x,y)\)</span>,<br />
then for every copula <span class="math inline">\(C\)</span> and every <span class="math inline">\((x,y)\in I^2\)</span>,</p>
<p><span class="math display">\[W(x,y)\leq C(x,y)\leq M(x,y)\]</span></p>
<p>which is the <strong>Frechét-Hoeffding bounds inequality</strong>.</p>
<blockquote>
<p>Theorem 2.2.4. Subcopula is Uniformly Continuous</p>
<p>Let <span class="math inline">\(C&#39;\)</span> be a subcopula,<br />
then for every <span class="math inline">\((x_1, x_2), (y_1, y_2)\)</span> in <span class="math inline">\(\text{Dom}C&#39;\)</span>,</p>
<p><span class="math display">\[\big|C&#39;(x_2,y_2) - C&#39;(x_1,y_1)\big|\leq |x_2-x_1|+|y_2-y_1|\]</span></p>
</blockquote>
<blockquote>
<p>Theorem 2.2.7.</p>
<p>Let <span class="math inline">\(C\)</span> be a copula,<br />
then for almost every <span class="math inline">\(x\in I\)</span> and for almost every <span class="math inline">\(y\in I\)</span>,<br />
partial derivatives exist, and if they exist,</p>
<p><span class="math display">\[\begin{aligned}\frac{\partial C(x,y)}{\partial x} &amp;\in [0, 1] \\
\frac{\partial C(x,y)}{\partial y} &amp;\in [0, 1]\end{aligned}\]</span></p>
<p>Furthermore, <span class="math inline">\(x\mapsto\frac{\partial C(x,y)}{\partial y}\)</span> and <span class="math inline">\(y\mapsto\frac{\partial C(x,y)}{\partial x}\)</span> are defined and non-decreasing almost everywhere in <span class="math inline">\(I\)</span>.</p>
</blockquote>
<blockquote>
<p>Theorem 2.2.8.</p>
<p>Let <span class="math inline">\(C\)</span> be a copula,<br />
if for all <span class="math inline">\(x\in I\)</span> when <span class="math inline">\(y=0\)</span>, <span class="math inline">\(\frac{\partial C(x,y)}{\partial y}\)</span> and <span class="math inline">\(\frac{\partial^2 C(x,y)}{\partial x\partial y}\)</span> are continuous,<br />
then <span class="math inline">\(\frac{\partial C(x,y)}{\partial x}\)</span> and <span class="math inline">\(\frac{\partial^2 C(x,y)}{\partial y\partial x}\)</span> exist,
and <span class="math inline">\(\frac{\partial^2 C(x,y)}{\partial x\partial y}=\frac{\partial^2 C(x,y)}{\partial y\partial x}\)</span>.</p>
</blockquote>
<blockquote>
<p>Definition 2.3.1. (Cumulative) Distribution Function</p>
<p><span class="math inline">\(F\)</span> is a distribution function if:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{Dom}F=\bar{\mathbb{R}}\)</span></li>
<li><span class="math inline">\(F\)</span> is non-decreasing</li>
<li><span class="math inline">\(F(-\infty)=0\)</span> and <span class="math inline">\(F(\infty)=1\)</span></li>
</ol>
</blockquote>
<p>Remark: Uniform distribution on <span class="math inline">\(a&lt;b\in\mathbb{R}\)</span>:</p>
<p><span class="math display">\[U_{ab}(x)=\begin{cases}0, x\in[-\infty, a) \\ \frac{x-a}{b-a} &amp; x\in[a,b]\\ 1 &amp; x\in(b,\infty] \end{cases}\]</span></p>
<blockquote>
<p>Definition 2.3.2. Joint Distribution Function</p>
<p><span class="math inline">\(H\)</span> is a joint distribution function if:</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{Dom}H=\bar{\mathbb{R}}^2\)</span></li>
<li><span class="math inline">\(H\)</span> is 2-increasing</li>
<li><span class="math inline">\(H(x,-\infty)=H(-\infty,y)=0\)</span> and <span class="math inline">\(H(\infty,\infty)=1\)</span></li>
</ol>
<p>Then, <span class="math inline">\(H\)</span> is grounded, and <span class="math inline">\(H\)</span> has margins given by:</p>
<p><span class="math display">\[\begin{cases}F(x)=H(x,\infty) \\ G(y)=H(\infty,y)\end{cases}\]</span></p>
</blockquote>
<blockquote>
<p>Theorem 2.3.3. Skalar’s Theorem</p>
<p>Let <span class="math inline">\(H\)</span> be a joint distribution function with margins <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span>,<br />
then exists a copula <span class="math inline">\(C\)</span> s.t. for all <span class="math inline">\(x,y\in\bar{\mathbb{R}}\)</span>,</p>
<p><span class="math display">\[\begin{aligned}H(x,y) &amp;= C\big( F(x), G(y) \big) \\
&amp;= C\big( H(x,\infty), H(\infty,y) \big) \end{aligned}\]</span></p>
<p>If <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are continuous, then <span class="math inline">\(C\)</span> is unique (regardless),<br />
otherwise, <span class="math inline">\(C\)</span> is <a href="https://math.stackexchange.com/questions/2897625/meaning-of-expression-uniquely-determined">uniquely determined</a> by <span class="math inline">\(\text{Ran}F\times\text{Ran}G\)</span>.</p>
<hr />
<p>If <span class="math inline">\(C\)</span> is a copula and <span class="math inline">\(F\)</span>, <span class="math inline">\(G\)</span> are distribution functions,
then</p>
<p><span class="math display">\[H(x,y) = C\big( F(x), G(y) \big)\]</span></p>
<p>is a joint distribution function with margins:</p>
<p><span class="math display">\[\begin{cases}H(x,\infty) = F(x) \\ H(\infty,y) = G(y)\end{cases}\]</span></p>
</blockquote>
<blockquote>
<p>Lemma 2.3.4.</p>
<p>Let <span class="math inline">\(H\)</span> be a joint distribution with margins <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span>,<br />
then exists a unique subcopula <span class="math inline">\(C&#39;\)</span> s.t.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{Dom}C&#39;=\text{Ran}F\times\text{Ran}G\)</span></li>
<li>For all <span class="math inline">\(x,y\in\bar{\mathbb{R}}\)</span>, <span class="math inline">\(H(x,y)=C\big(F(x),G(y)\big)\)</span></li>
</ol>
</blockquote>
<blockquote>
<p>Lemma 2.3.5.</p>
<p>Let <span class="math inline">\(C&#39;\)</span> be a subcopula,<br />
then exists a copula <span class="math inline">\(C\)</span> s.t. <span class="math inline">\(C(x,y)=C&#39;(x,y)\)</span> for all <span class="math inline">\((x,y)\in\text{Dom}C&#39;\)</span>.</p>
</blockquote>
<p>Remark: This extension is generally non-unique.</p>
<blockquote>
<p>Definition 2.3.6. Quasi-Inverse of Distribution Function</p>
<p>Let <span class="math inline">\(F\)</span> be a distribution function,<br />
then a quasi-inverse of <span class="math inline">\(F\)</span> is any function <span class="math inline">\(F^{(-1)}\)</span> s.t.</p>
<ol style="list-style-type: decimal">
<li><span class="math inline">\(\text{Dom}F^{(-1)}=I\)</span></li>
<li>For every <span class="math inline">\(t\in\text{Ran}F\)</span>,
<span class="math display">\[F\big(F^{(-1)}(t)\big)= t\]</span></li>
<li>For every <span class="math inline">\(t\not\in\text{Ran}F\)</span>,
<span class="math display">\[\begin{aligned}F^{(-1)}(t) &amp;= \inf\{x\ |\ F(x)\geq t\}\\ &amp;= \sup\{x\ |\ F(x)\leq t\} \end{aligned}\]</span></li>
</ol>
<p>If <span class="math inline">\(F\)</span> is strictly increasing,<br />
then <span class="math inline">\(F^{(-1)}\)</span> is unique and denoted as <span class="math inline">\(F^{-1}\)</span>.</p>
</blockquote>
<blockquote>
<p>Lemma 2.3.7.</p>
<p>Let <span class="math inline">\(H,F,G,C&#39;\)</span> be as in Lemma 2.3.4., <span class="math inline">\(F^{(-1)},G^{(-1)}\)</span> be quasi-inverses,<br />
then for any <span class="math inline">\((x,y)\in\text{Dom}C&#39;\)</span>,</p>
<p><span class="math display">\[C&#39;(x,y)=H\big( F^{(-1)}(x), G^{(-1)}(y) \big)\]</span></p>
</blockquote>
<blockquote>
<p>Theorem 2.4.1. Skalar’s Theorem (Random Variables)</p>
<p>Let <span class="math inline">\(X,Y\)</span> be random variables with distribution functions <span class="math inline">\(F,G\)</span> and joint distribution <span class="math inline">\(H\)</span>,<br />
then exists a copula <span class="math inline">\(C_{XY}\)</span> s.t.</p>
<p><span class="math display">\[H(x,y) = C_{XY}\big( F(x), G(y) \big)\]</span></p>
<p>If <span class="math inline">\(F\)</span> and <span class="math inline">\(G\)</span> are continuous, then <span class="math inline">\(C_{XY}\)</span> is unique (regardless),<br />
otherwise, <span class="math inline">\(C_{XY}\)</span> is <a href="https://math.stackexchange.com/questions/2897625/meaning-of-expression-uniquely-determined">uniquely determined</a> by <span class="math inline">\(\text{Ran}F\times\text{Ran}G\)</span>.</p>
</blockquote>
<blockquote>
<p>Theorem 2.4.2.</p>
<p>Let <span class="math inline">\(X,Y\)</span> be continuous random variables,<br />
then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent <span class="math inline">\(\iff\)</span></p>
<p><span class="math display">\[\begin{aligned}C_{XY}&amp;= \Pi \\
C_{XY}\big( F(x), G(y) \big) &amp;= F(x)\cdot G(y) \\
H(x,y) &amp;= F(x)\cdot G(y) \end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p>Theorem 2.4.3. Transformation Invariance (1)</p>
<p>Let <span class="math inline">\(X,Y\)</span> be continuous random variables with copula <span class="math inline">\(C_{XY}\)</span>,<br />
functions <span class="math inline">\(\alpha, \beta\)</span> be strictly increasing on <span class="math inline">\(\text{Ran}X\)</span> and <span class="math inline">\(\text{Ran}Y\)</span>,<br />
then <span class="math inline">\(C_{XY}\)</span> is invariant under strictly increasing transformations:</p>
<p><span class="math display">\[C_{\alpha(X),\beta(Y)} = C_{XY}\]</span></p>
</blockquote>
<blockquote>
<p>Theorem 2.4.4. Transformation Invariance (2)</p>
<p>Let <span class="math inline">\(X,Y\)</span> be continuous random variables with copula <span class="math inline">\(C_{XY}\)</span>,<br />
functions <span class="math inline">\(\alpha, \beta\)</span> be strictly monotone on <span class="math inline">\(\text{Ran}X\)</span> and <span class="math inline">\(\text{Ran}Y\)</span>,</p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(\alpha\)</span> is strictly increasing, <span class="math inline">\(\beta\)</span> is strictly decreasing,<br />
then
<span class="math display">\[C_{\alpha(X),\beta(Y)}(x,y)= x - C_{XY}(x,1-y)\]</span></li>
<li>If <span class="math inline">\(\alpha\)</span> is strictly decreasing, <span class="math inline">\(\beta\)</span> is strictly increasing,<br />
then
<span class="math display">\[C_{\alpha(X),\beta(Y)}(x,y)= y - C_{XY}(1-x,y)\]</span></li>
<li>If <span class="math inline">\(\alpha,\beta\)</span> are both strictly decreasing,<br />
then
<span class="math display">\[C_{\alpha(X),\beta(Y)}(x,y)= x + y - 1 + C_{XY}(1-x,1-y)\]</span></li>
</ol>
</blockquote>
<blockquote>
<p>Order Statistics</p>
<p>Let <span class="math inline">\(X,Y\)</span> be continuous random variables with distribution functions <span class="math inline">\(F,G\)</span> and copula <span class="math inline">\(C_{XY}\)</span>,<br />
then order statistics are the random variables <span class="math inline">\(\min(X,Y)\)</span> and <span class="math inline">\(\max(X,Y)\)</span> with distributions:</p>
<p><span class="math display">\[\begin{aligned}\Pr(\max(X,Y)\leq t) &amp;= C\big( F(t), G(t) \big)\\
\Pr(\min(X,Y)\leq t) &amp;= F(t) + G(t) - C\big( F(t), G(t) \big) \end{aligned}\]</span></p>
<p>and they are bounded by</p>
<p><span class="math display">\[\begin{aligned}\max\big(F(t)+G(t)-1,\ 0\big) &amp;\leq \Pr(\max(X,Y)\leq t) \leq\min\big(F(t), G(t)\big) \\
\max\big(F(t),G(t)\big)&amp;\leq \Pr(\min(X,Y)\leq t) \mkern3.5mu\leq \min\big(F(t)+G(t),\ 1\big)\end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p>Survival Function / Survivor Function / Reliability Function</p>
<p>Let <span class="math inline">\(X\)</span> be random variable with distribution function <span class="math inline">\(F\)</span>,<br />
the survival function of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[\begin{aligned}\bar{F}(x) &amp;= \Pr (X&gt;x)\\ &amp;= 1-F(x) \end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p>Joint Survival Function</p>
<p>Let <span class="math inline">\(X,Y\)</span> be random variables with distribution functions <span class="math inline">\(F,G\)</span> and copula <span class="math inline">\(C_{XY}\)</span>,<br />
the joint survival function</p>
<p><span class="math display">\[\bar{H}(x,y) = \Pr(X&gt;x, Y&gt;y)\]</span></p>
<p>with margins (also the univariate survival functions):</p>
<p><span class="math display">\[\begin{cases}\bar{F}(x)=\bar{H}(x,-\infty) \\ \bar{G}(y)=\bar{H}(-\infty,y)\end{cases}\]</span></p>
<p>where
<span class="math display">\[\begin{aligned}\bar{H}(x,y) &amp;= \big(1 - F(x)\big) + \big(1- G(y)\big) - \big(1 - H(x,y)\big)\\
&amp;= \bar{F}(x) + \bar{G}(y) + C_{XY}\big(F(x), G(y)\big) - 1\\
&amp;= \bar{F}(x) + \bar{G}(y) + C_{XY}\big(1-\bar{F}(x),\ 1-\bar{G}(y)\big) - 1\end{aligned}\]</span></p>
</blockquote>
<blockquote>
<p>Survival Copula</p>
<p>Let <span class="math inline">\(X,Y\)</span> be random variables with joint survival function <span class="math inline">\(\bar{H}\)</span> and copula <span class="math inline">\(C_{XY}\)</span>,<br />
the survival copula is a function <span class="math inline">\(\widehat{C}:I^2\to I\)</span> s.t.</p>
<p><span class="math display">\[\begin{aligned}\widehat{C} &amp;= x + y + C_{XY}(1-x, 1-y) - 1 \\
\bar{H}(x,y) &amp;= \widehat{C}\big(\bar{F}(x), \bar{G}(y)\big) \end{aligned}\]</span></p>
</blockquote>
<!-- ### Definitions & Theorems for N-Dim Space -->
</div>
<div id="copula-sampling" class="section level3">
<h3>Copula Sampling</h3>
<p>Purpose: Given <span class="math inline">\(X\)</span> and density function <span class="math inline">\(F\)</span>, get a series of realisations <span class="math inline">\(x\)</span>.</p>
<div id="inverse-distribution-function" class="section level4">
<h4>Inverse Distribution Function</h4>
<ol style="list-style-type: decimal">
<li>Given <span class="math inline">\(X\)</span> and density function <span class="math inline">\(F\)</span></li>
<li>Generate <span class="math inline">\(u\)</span> that is uniform on <span class="math inline">\((0,1)\)</span></li>
<li>Set <span class="math inline">\(x=F^{(-1)}(u)\)</span></li>
</ol>
</div>
<div id="conditional-distribution-method" class="section level4">
<h4>Conditional Distribution Method</h4>
<ol style="list-style-type: decimal">
<li>Given <span class="math inline">\(X,Y\)</span> and copula <span class="math inline">\(C=C_{XY}\)</span></li>
<li>Generate independent variables <span class="math inline">\(u,t\)</span> that are uniform on <span class="math inline">\((0,1)\)</span></li>
<li>Set <span class="math inline">\(v=c_u^{(-1)}(t)\)</span>, where
<span class="math display">\[\begin{aligned}c_u(x) &amp;= \Pr(X\leq x\ |\ U=u) \\ &amp;= \frac{\partial C(u,x)}{\partial u}\end{aligned}\]</span></li>
<li>Then <span class="math inline">\((x,y)=(u,v)\)</span></li>
</ol>
</div>
<div id="conditional-distribution-with-survival-copulas" class="section level4">
<h4>Conditional Distribution With Survival Copulas</h4>
</div>
</div>
<div id="families-of-copulas" class="section level3">
<h3>Families of Copulas</h3>
<div id="frechét-mardia-families-of-copula" class="section level4">
<h4>Frechét-Mardia Families of Copula</h4>
<p>Let <span class="math inline">\(\alpha,\beta\in I\)</span> s.t. <span class="math inline">\(\alpha+\beta\leq 1\)</span>, a comprehensive copula:</p>
<p><span class="math display">\[C_{\alpha,\beta}(x,y) = \alpha M(x,y) + (1-\alpha-\beta)\Pi(x,y) +\beta W(x,y)\]</span></p>
<p>Let <span class="math inline">\(\theta\in[0,1]\)</span>, a comprehensive copula:</p>
<p><span class="math display">\[C_{\theta} = \frac{\theta^2(1+\theta)}{2} M(x,y) + (1-\theta^2)\Pi(x,y) + \frac{\theta^2(1-\theta)}{2} W(x,y)\]</span></p>
</div>
<div id="cuadras-augé-family-of-copulas" class="section level4">
<h4>Cuadras-Augé Family of Copulas</h4>
<p>Let <span class="math inline">\(\theta\in[0,1]\)</span>, weighted geometric mean copula:</p>
<p><span class="math display">\[\begin{aligned}C_{\theta}(x,y) &amp;= \big(\min(x,y)\big)^{\theta}\cdot(xy)^{1-\theta}\\
&amp;= \begin{cases}xy^{1-\theta} &amp; x\leq y\\ x^{1-\theta}y &amp; x\geq y\end{cases}\end{aligned}\]</span></p>
<p>note that <span class="math inline">\(C_0=\Pi(x,y)\)</span> and <span class="math inline">\(C_1=M(x,y)\)</span>.</p>
</div>
<div id="gumbels-bivariate-exponential-distribution" class="section level4">
<h4>Gumbel’s Bivariate Exponential Distribution</h4>
<p>Let <span class="math inline">\(H_{\theta}\)</span> be a joint distribution, <span class="math inline">\(\theta\in[0,1]\)</span>,</p>
<p><span class="math display">\[H_{\theta} = \begin{cases}1-e^{-x}-e^{-y}+e^{-(x+y+\theta xy)} &amp; x,y&gt;0 \\ 0 &amp; \text{otherwise} \end{cases}\]</span></p>
<p>then quasi-inverses are</p>
<p><span class="math display">\[\begin{cases}F^{(-1)}(x)= -\ln(1-x)\\G^{(-1)}(y)= -\ln(1-y) \end{cases}\]</span></p>
<p>and the copula is</p>
<p><span class="math display">\[C_{\theta}(x,y) = x+y-1 + (1-x)(1-y) e^{-\theta \ln(1-x) \ln(1-y)}\]</span></p>
</div>
<div id="gumbel-hougaard-family-of-copulas" class="section level4">
<h4>Gumbel-Hougaard Family of Copulas</h4>
<p>Let <span class="math inline">\(X,Y\)</span> be random variables, <span class="math inline">\(\theta\geq 1\)</span>, then for all <span class="math inline">\(x,y\in\bar{\mathbb{R}}\)</span>,</p>
<p><span class="math display">\[H_{\theta}(x,y)=\exp\big( -(e^{-\theta x} + e^{-\theta y})^{\frac{1}{\theta}} \big)\]</span></p>
<p>and the copula is</p>
<p><span class="math display">\[C_{\theta}(x,y)=\exp\Big( -\big((-\ln x)^{\theta} + (-\ln y)^{\theta}\big)^{\frac{1}{\theta}} \Big)\]</span></p>
</div>
<div id="ali-mikhail-haq-family-of-copulas" class="section level4">
<h4>Ali-Mikhail-Haq Family of Copulas</h4>
<p>Let <span class="math inline">\(X,Y\)</span> be random variables, <span class="math inline">\(\theta\in[-1,1]\)</span>, then for all <span class="math inline">\(x,y\in\bar{\mathbb{R}}\)</span>,</p>
<p><span class="math display">\[H_{\theta} = \big( 1+e^{-x}-e^{-y}+(1-\theta)e^{-x-y} \big)^{-1}\]</span></p>
<ol style="list-style-type: decimal">
<li>If <span class="math inline">\(\theta=1\)</span>, the copula is <a href="#gumbels-bivariate-exponential-distribution">Gumbel’s Bivariate Exponential Distribution</a></li>
<li>If <span class="math inline">\(\theta=0\)</span>, <span class="math inline">\(X,Y\)</span> are independent, and <span class="math inline">\(C_{XY}=\Pi\)</span></li>
<li>Otherwise, the copula is</li>
</ol>
<p><span class="math display">\[C_{\theta}(x,y)=\frac{xy}{1-\theta(1-x)(1-y)}\]</span></p>
</div>
</div>
</div>
<div id="local-sensitivity-analysis" class="section level2">
<h2>Local Sensitivity Analysis</h2>
<ul>
<li>Local sensitivity: sensitivity around <span class="math inline">\(\mathbf{x}^0\in\mathcal{X}\)</span>
<ul>
<li>Base case / reference input: <span class="math inline">\(\mathbf{x}^0\)</span></li>
<li>Inputs are deterministic</li>
</ul></li>
<li>Output: <span class="math inline">\(y^0 = g(\mathbf{x}^0)\)</span></li>
</ul>
<div id="one-at-a-time-method" class="section level3">
<h3>One at a Time Method</h3>
<ul>
<li>Graph: tornado diagram</li>
<li>Base case: <span class="math inline">\(\mathbf{x}^0\)</span></li>
<li>Sensitivity case: <span class="math inline">\(\mathbf{x}^+\)</span></li>
<li>Input changes: <span class="math inline">\(\Delta ^+\mathbf{x} = \mathbf{x}^+ - \mathbf{x}^0\)</span></li>
<li>Sensitivity: finite changes of output by individual changes of input (magnitude + direction)
<span class="math display">\[\Delta ^+_i y = g\big((x_i +  \Delta ^+ x_i), \mathbf{x}^0_{\sim i} \big) - g(\mathbf{x}^0)\]</span></li>
<li>Weakness: neglects interaction effects between <span class="math inline">\(x\)</span>’s</li>
</ul>
</div>
<div id="scenario-decomposition" class="section level3">
<h3>Scenario Decomposition</h3>
<ul>
<li>Graph: generalised tornado diagram</li>
<li>Fix 2 or more base points of input
<ul>
<li><span class="math inline">\(\mathbf{x}^0\)</span>, <span class="math inline">\(\mathbf{x}^+\)</span> and <span class="math inline">\(\mathbf{x}^-\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[\begin{aligned}\Delta y &amp;= g(\mathbf{x}^+) - g(\mathbf{x}^0)\\
&amp;= \sum_{i=1}^n \phi_i + \sum_{i&lt;j} \phi_{i,j} + \sum_{i&lt;j&lt;k} \phi_{i,j,k} +\dots + \phi_{1,2,\dots,n}\end{aligned}\]</span>
where the <span class="math inline">\(2^n\)</span> terms are given by
<span class="math display">\[\begin{aligned} \phi_i &amp;= g(x_i^+,\mathbf{x}^0_{\sim i}) - g(\mathbf{x}^0) \\
\phi_{i,j} &amp;= g(x_i^+,x_j^+,\mathbf{x}^0_{\sim i,j}) - \phi_i - \phi_j - g(\mathbf{x}^0) \\
\phi_{i,j,k} &amp;= g(x_i^+,x_j^+,x_k^+,\mathbf{x}^0_{\sim i,j,k}) - \phi_{i,j} - \phi_{i,k} - \phi_{j,k} - \phi_i - \phi_j - \phi_k - g(\mathbf{x}^0) \\
\vdots &amp; \end{aligned}\]</span>
- Individual effect: <span class="math inline">\(\phi_i = \Delta ^+_i y\)</span>
- Residual interaction effect: <span class="math inline">\(\phi_{i,j}\)</span></p>
</div>
<div id="one-way-sensitivity-function" class="section level3">
<h3>One Way Sensitivity Function</h3>
<ul>
<li>Graph: spider plot</li>
<li>Register output values with varying input in normalised range</li>
<li>Sensitivity:
<span class="math display">\[\begin{aligned}h_i(x_i) &amp;= g(x_i,\mathbf{x}^0_{\sim i})\\
h_i^* &amp;= h_i(x_i) - g(\mathbf{x}^0) \\
&amp;= \phi_i = \Delta ^+_i y \end{aligned}\]</span></li>
</ul>
</div>
<div id="differentiation-based-methods" class="section level3">
<h3>Differentiation-Based Methods</h3>
<ul>
<li>Decompose <span class="math inline">\(\Delta y\)</span> using Taylor expansion:
<span class="math display">\[\begin{aligned}\Delta y &amp;= g(\mathbf{x}^+) - g(\mathbf{x}^0)\\
&amp;= \sum_{i=1}^n \frac{\partial g(\mathbf{x}^0)}{\partial x_i}\cdot(x_i^+ - x_i^0) \\
&amp;\phantom{\ggg}+\frac{1}{2}\cdot \sum_{i=1}^n\sum_{j=1}^n \frac{\partial ^2 g(\mathbf{x}^0)}{\partial x_i\partial x_j}\cdot(x_i^+ - x_i^0)(x_j^+ - x_j^0)\\
&amp;\phantom{\ggg}+ o\big(\|\mathbf{x}^+ - \mathbf{x}^0 \|^2\big)\\
&amp;\approx \sum_{i=1}^n \underbrace{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}}_{\begin{subarray}{l}\text{Natural}\\\text{sensitivity}\end{subarray}}\cdot(x_i^+ - x_i^0) \end{aligned}\]</span>
However, natural sensitivities are not comparable since different <span class="math inline">\(x\)</span>’s have different units.<br />
</li>
<li>Differential importance measure:<br />
If <span class="math inline">\(dx_i = dx_j\)</span> for all <span class="math inline">\(i,j\)</span> (uniform perturbation), this is equivalent to partial derivatives.<br />
Comparable &amp; additive (does not account for interaction).
<span class="math display">\[D_i = \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ dx_i}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ dx_j\big)}\]</span></li>
<li>Elasticity of <span class="math inline">\(y\)</span> wrt <span class="math inline">\(x_i\)</span> (also as: criticality importance measure)<br />
If <span class="math inline">\(\frac{d x_i}{x_i^0} = \frac{d x_j}{x_j^0}\)</span> for all <span class="math inline">\(i,j\)</span> (proportional perturbation), elasticity is equivalent to differential importance:
<span class="math display">\[\begin{aligned}E_i &amp;= \frac{\partial g(\mathbf{x}^0)}{\partial x_i}\cdot \frac{x_i^0}{g(\mathbf{x}^0)} \\
D_i &amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ dx_i}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ dx_j\big)} \\
&amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ dx_i\cdot\frac{x_i^0}{x_i^0}}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ dx_j\cdot\frac{x_j^0}{x_j^0}\big)} \\
&amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ x_i^0 \cdot\frac{dx_i}{x_i^0}}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ x_j^0 \cdot\frac{dx_j}{x_j^0}\big)} \\
&amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ x_i^0}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ x_j^0} \\
&amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\cdot\frac{x_i^0}{g(\mathbf{x}^0)}}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\cdot\frac{x_j^0}{g(\mathbf{x}^0)}} \\
&amp;= \frac{E_i}{\sum_{j=1}^n E_j}\end{aligned}\]</span></li>
</ul>
</div>
</div>
<div id="global-sensitivity-analysis" class="section level2">
<h2>Global Sensitivity Analysis</h2>
<ul>
<li>Assumption: have info about model inputs’ probability distribution</li>
<li>Input
<ul>
<li>Probability space: <span class="math inline">\(\big(\mathcal{X}, \mathcal{B}(\mathcal{X}), \mathbb{P}_{\mathbf{X}}\big)\)</span></li>
<li>Probability distribution: <span class="math inline">\(\mathbb{P}_{\mathbf{X}}\)</span></li>
<li>Random vector: <span class="math inline">\(\mathbf{X} = (X_1, \dots, X_n)\)</span></li>
<li>Realisation: <span class="math inline">\(\mathbf{x} = (x_1, \dots, x_n)\)</span></li>
<li>Joint cdf &amp; pdf: <span class="math inline">\(F_{\mathbf{X}}(\mathbf{x})\)</span>, <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x})\)</span></li>
<li>Marginal cdf &amp; pdf: <span class="math inline">\(F_{X_i}(x_i)\)</span>, <span class="math inline">\(f_{X_i}(x_i)\)</span></li>
<li>Standard deviation of <span class="math inline">\(X_i\)</span>: <span class="math inline">\(\sigma_i\)</span></li>
<li>If inputs are independent, then
<span class="math display">\[F_{\mathbf{X}}(\mathbf{x})=\prod_{i=1}^n F_i(x_i)\]</span></li>
</ul></li>
<li>Output
<ul>
<li>Random variable: <span class="math inline">\(Y = g(\mathbf{X})\)</span></li>
<li>Probability distribution: <span class="math inline">\(\mathbb{P}_Y(\cdot) = \mathbb{P}_{\mathbf{X}}\big(g^{-1}(\cdot)\big)\)</span></li>
<li>cdf &amp; pdf: <span class="math inline">\(F_Y(y)\)</span>, <span class="math inline">\(f_Y(y)\)</span></li>
<li>Standard deviation of <span class="math inline">\(Y\)</span>: <span class="math inline">\(\sigma_Y\)</span></li>
</ul></li>
<li>Monte Carlo
<ul>
<li>Generate input sample: <span class="math inline">\(N\times n\)</span> (N <span class="math inline">\(\mathbf{x}\)</span>’s)</li>
<li>Evaluate output: <span class="math inline">\(N\times D\)</span> (N <span class="math inline">\(Y\)</span>’s)</li>
</ul></li>
</ul>
<div id="non-parametric-methods" class="section level3">
<h3>Non-Parametric Methods</h3>
<ul>
<li>Assumption: input-output sample is well fitted by linear regression</li>
<li>If <span class="math inline">\(R_Y^2\)</span> (goodness-of-fit) is too low, should consider other methods.</li>
<li>Response surface:
<span class="math display">\[g(\mathbf{x}) \approx b_0 + \sum_{i=1}^n b_i X_i\]</span></li>
<li>Standardised regression coefficient:
<span class="math display">\[\begin{aligned}SRC_i &amp;= b_i\cdot\frac{\sigma_i}{\sigma_Y} \\
&amp;= b_i\cdot\frac{\sqrt{\sum_{j=1}^N (x_{ji} - \bar{x}_i)^2}}{\sqrt{\sum_{j=1}^N (y_j - \bar{y})^2}}  \end{aligned}\]</span></li>
<li>Pearson’s product moment correlation coefficient:
<span class="math display">\[\begin{aligned}PEAR_i &amp;= \varrho(Y,X_i)\\
&amp;= \frac{Cov(Y,X_i)}{\sigma_i \sigma_Y}\\
&amp;= \frac{\sum_{j=1}^N (x_{ji} - \bar{x}_i)(y_j - \bar{y})}{\sqrt{\sum_{j=1}^N (x_{ji} - \bar{x}_i)^2}\cdot \sqrt{\sum_{j=1}^N (y_j - \bar{y})^2}} \end{aligned}\]</span></li>
</ul>
</div>
<div id="variance-based-methods" class="section level3">
<h3>Variance-Based Methods</h3>
<ul>
<li>Importance of input = reduction of output variance if know this input for sure</li>
<li>Variance decomposition:
<span class="math display">\[Var(Y) = Var\big(E[Y\ |\ X_{\alpha}]\big) + E\big[Var(Y\ |\ X_{\alpha})\big]\]</span></li>
<li>Non-linear response surface:
<span class="math display">\[\phi_{\alpha}(x_{\alpha}) = E[Y\ |\ X_{\alpha}=x_{\alpha}]\]</span></li>
<li>Group effect: (fraction of output variance that is explained by functional dependence on <span class="math inline">\(x_{\alpha}\)</span>)
<span class="math display">\[S_{\alpha}^{\text{group}} = \frac{Var(\phi_{\alpha}(x_{\alpha}))}{Var(Y)}\]</span>
If <span class="math inline">\(\alpha=\{i\}\)</span>, then the main effect / first order index:
<span class="math display">\[\begin{aligned} S_i &amp;= S_{i}^{\text{group}}\\
&amp;= \varrho^2\big(Y,E[Y\ |\ X_i]\big) \\
&amp;\geq \beta_i^2\end{aligned}\]</span>
Higher order effect:
<span class="math display">\[S_{\alpha} = S_{\alpha}^{\text{group}} - \sum_{\beta\not\subset\alpha} S_{\beta}\]</span>
Total effect: (this is the measure to decide whether input <span class="math inline">\(i\)</span> is important or not)
<span class="math display">\[S^T_i = 1 - S_{\sim i}^{\text{group}}\]</span>
When inputs are uncorrelated, <span class="math inline">\(S^T_i\geq S_i\)</span>, and <span class="math inline">\(\sum_{i=1}^n S_i\leq 1\)</span>.</li>
<li>ANOVA decomposition of <span class="math inline">\(g(\mathbf{x})\)</span>:
<span class="math display">\[\begin{aligned}g(\mathbf{x}) &amp;= g_0 + \sum_{i=1}^n g_i(x_i) + \sum_{i&lt;j} g_{i,j}(x_i, x_j) \\
&amp;\phantom{\ggg}+\sum_{i&lt;j&lt;k} g_{i,j,k}(x_i,x_j,x_k)+\dots + g_{1,2,\dots,n}(x_1, x_2,\dots,x_n) \end{aligned}\]</span>
where
<span class="math display">\[\begin{aligned}g_0 &amp;= \int_{\mu_1}\dots\int_{\mu_n} g(\mathbf{x})\,d\mu \\
g_i(x_i) &amp;= \int\dots\int g(\mathbf{x})\,d\mu_{k\neq i} - g_0 \\
g_{i,j}(x_i, x_j) &amp;= \int\dots\int g(\mathbf{x})\,d\mu_{k\neq i,j} - g_i(x_i) - g_j(x_j) - g_0 \\
\vdots &amp;\end{aligned}\]</span>
If inputs are independent, then <span class="math inline">\(g_{\alpha}(\mathbf{x}_{\alpha})\)</span> are orthogonal, and
<span class="math display">\[\begin{aligned}Var(Y) &amp;= E\big[\big(g(\mathbf{X}) - g_0\big)^2 \big] \\
&amp;= \sum_{\alpha} V_{\alpha}\\
V_{\alpha} &amp;= \int_{x_{\alpha}} \big[g_{\alpha}(\mathbf{x}_{\alpha}) \big]^2\cdot f_{\alpha}(\mathbf{x}_{\alpha})\, d\mathbf{x}_{\alpha} \end{aligned}\]</span></li>
<li>Zero value of <span class="math inline">\(S_i\)</span> or <span class="math inline">\(V_i\)</span> does not assure that output is independent on <span class="math inline">\(X_i\)</span>
<ul>
<li><span class="math inline">\(\implies\)</span> Ishigami test function</li>
<li>In general, using moments as proxy to uncertainty is a bad idea.</li>
</ul></li>
</ul>
</div>
<div id="density-based-methods" class="section level3">
<h3>Density-Based Methods</h3>
<ul>
<li>Consider the entire distribution w/o referencing a particular moment</li>
<li>Unconditional output pdf: <span class="math inline">\(f_Y(y)\)</span></li>
<li>Local sensitivity: plot output pdf given <span class="math inline">\(X_i\)</span>, <span class="math inline">\(f_{Y\ |\ X_i}(y)\)</span>, then compare the distance (separation)</li>
<li>Global sensitivity: plot as many pdf curves as the possible <span class="math inline">\(X_i\)</span>’s
<ul>
<li>Inner separation: <span class="math inline">\(\gamma_i(x_i)\)</span></li>
</ul></li>
<li><span class="math inline">\(\delta\)</span>-sensitivity measure: (does not require independence)<br />
<span class="math inline">\(\delta_i=0\iff Y\)</span> is independent of <span class="math inline">\(X_i\)</span>.<br />
<span class="math inline">\(\delta_i\)</span> is monotonic transformation invariant.
<span class="math display">\[\begin{aligned}\gamma_i(x_i) &amp;= \int_y \big| f_Y(y) - f_{Y\ |\ X_i}(y)\big| \,dy \\
\delta_i &amp;=\frac{1}{2}\cdot E\big[\gamma_i(x_i)\big] \\
&amp;= \frac{1}{2}\cdot\int_{X_i}\int_Y \big| f_{Y,X_i}(y,x_i) - f_{Y}(y)\cdot f_{X_i}(x_i)\big|\, dydx_i \\
&amp;\in [0,1]  \end{aligned}\]</span></li>
<li>Kullback-Leibler divergence:<br />
<span class="math inline">\(\theta_i^{KL}=0\iff Y\)</span> is statistically independent of <span class="math inline">\(X_i\)</span>.<br />
<span class="math inline">\(\theta_i^{KL}\)</span> is monotonic transformation invariant.
<span class="math display">\[\begin{aligned}\gamma_i^{KL}(x_i) &amp;= \int_y f_{Y\ |\ X_i}(y)\cdot\big(\log f_{Y\ |\ X_i}(y) - \log f_Y(y)\big)\, dy \\
\theta_i^{KL} &amp;= E\big[\gamma_i^{KL}(x_i) \big]\end{aligned}\]</span></li>
<li>Both <span class="math inline">\(\delta_i\)</span> and <span class="math inline">\(\theta_i^{KL}\)</span> are in the family of <a href="https://en.wikipedia.org/wiki/F-divergence">Csiszar divergence</a>:
<span class="math display">\[\begin{aligned}\gamma_i^F(X_i) &amp;= \int_y f_{Y\ |\ X_i}(y)\cdot t\Big(\frac{f_Y(y)}{f_{Y\ |\ X_i}(y)}\Big)\, dy \\
\theta_i^F &amp;=E\big[\gamma_i^F(X_i) \big] \end{aligned}\]</span>
where <span class="math inline">\(t(\cdot)\)</span> is convex, <span class="math inline">\(t(0)=1\)</span>.</li>
</ul>
</div>
<div id="transformation-invariant-methods" class="section level3">
<h3>Transformation Invariant Methods</h3>
<ul>
<li>Necessary when model output is sparse</li>
<li>Any monotone function: <span class="math inline">\(u(\cdot)\)</span>
<ul>
<li>This is also convenient when <span class="math inline">\(u(\cdot)\)</span> is utility function</li>
</ul></li>
<li>Monotonic transformation of output: <span class="math inline">\(U=u(Y)\)</span>
<ul>
<li>Must reinterpret results for the original scale</li>
<li>Ranking of input is not guaranteed to be the same<br />
(Density-based measures are fine!)</li>
</ul></li>
<li>Some sensitivity measure of <span class="math inline">\(X_i\)</span>: <span class="math inline">\(\xi_i(Y)\big[\xi_i(U)]\)</span>
<ul>
<li>Monotonic transformation invariant: <span class="math inline">\(\xi_i(U)=\xi_i(Y)\)</span> for all <span class="math inline">\(U=u(Y)\)</span></li>
</ul></li>
<li>Global sensitivity measure is transformation invariant if the inner statistic is transformation invariant.</li>
<li>Kolmogorov-Smirnov metric:
<span class="math display">\[\beta_i^{KS} = E\big[ \sup_y \big| F_Y(y)-F_{Y\ |\ X_i}(y) \big| \big]\]</span></li>
<li>Kuiper distance:
<span class="math display">\[\beta_i^{Ku} = E\Big[ \sup_y \big( F_Y(y)-F_{Y\ |\ X_i}(y) \big) + \sup_y \big(F_{Y\ |\ X_i}(y) - F_Y(y)\big) \big]\]</span></li>
<li>The inner statistic of both <span class="math inline">\(\beta_i^{KS}\)</span> and <span class="math inline">\(\beta_i^{Ku}\)</span> are in the family of:
<span class="math display">\[z(\mathbb{P}, \mathbb{Q}) = \sup_{A\in\mathcal{A}} h\big(\big|\mathbb{P}(A) - \mathbb{Q}(A)\big|\big)\]</span>
where <span class="math inline">\(\mathbb{P}, \mathbb{Q}\)</span> are probability measures on <span class="math inline">\((\Omega, \mathcal{A})\)</span>,<br />
<span class="math inline">\(h: \mathbb{R}_+\mapsto\mathbb{R}_+\)</span> is continuous and non-decreasing,<br />
<span class="math inline">\(h(0)=0\)</span>, and <span class="math inline">\(\sup_t \frac{h(2t)}{h(t)}&lt;\infty\)</span>.</li>
</ul>
</div>
</div>
<div id="decision-sensitivity-find-the-maximiser" class="section level2">
<h2>Decision Sensitivity (Find the Maximiser)</h2>
<p>(TBA when needed.)</p>
</div>
<div id="estimate-sensitivity" class="section level2">
<h2>Estimate Sensitivity</h2>
<ul>
<li>Types of estimation strategies
<ul>
<li>Require some specific design: data must be generated under the scheme</li>
<li>Does not require design: can use any Monte Carlo generated data, or data generated under any scheme</li>
</ul></li>
<li>Design schemes
<ul>
<li>FAST (Fourier Amplitude Sensitivity Test)</li>
<li>RBD (Random Balance Design)</li>
</ul></li>
<li>Given data approaches
<ul>
<li>EASI (Effective Algorithm for global Sensitivity Indices)</li>
<li>COSI (COsine transformation for global Sensitivity Indices)</li>
</ul></li>
<li>Meta-model (emulator) approach
<ul>
<li>Gaussian process</li>
<li>Smoothing spline ANOVA</li>
<li>Polynomial chaos expansion (this is probably relevant)</li>
<li>Sparse grid interpolation</li>
</ul></li>
</ul>
<p>(TBA when needed.)</p>
</div>

    </div>
  </article>
  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  (function() { 
  var d = document, s = d.createElement('script');
  s.src = 'https://loikein-github.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>

</main>

<footer class="footer">
<ul class="footer-links">
<li>
  <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
</li>
<li>Theme by loikein with love</li>
</ul>

</footer>

<script async src="/js/jump-after-toc.js"></script>






<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/applescript.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/vim.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



<script src="/js/math-code.js"></script>
<script async defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=TeX-MML-AM_CHTML"></script>



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-143089736-1', 'auto');
  
  ga('send', 'pageview');
}
</script>

</body>
</html>
