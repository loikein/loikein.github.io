<!DOCTYPE html>
<html lang="en-gb">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.71.1" />

<title>Notes on Sensitivity and Copulas - loikein&#39;s notes</title>
<meta property="og:title" content="Notes on Sensitivity and Copulas - loikein&#39;s notes">


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-143089736-1', 'auto');
  
  ga('send', 'pageview');
}
</script>


  <link rel='icon' href='/favicon.ico' type='image/x-icon'/>


  






<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />


<link rel="stylesheet" href="/css/normalize.css" media="all">
<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">
<link rel="stylesheet" href="/css/clumsy-toc.css" media="all">





  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/logo.png"
         alt="Home">
  </a>
  <ul class="nav-links">
    
    <li><a href="/post/">Notes</a></li>
    
    <li><a href="/writings/">Writings</a></li>
    
    <li><a href="/tags/">Tags</a></li>
    
    <li><a href="https://duckduckgo.com/?q=site%3Anotes.loikein.one">Search</a></li>
    
    <li><a href="https://github.com/loikein/loikein.github.io">Source</a></li>
    
  </ul>
</nav>

      </header>
<main class="content" role="main">
  <article class="article">
    
    <span class="article-duration">11 min read</span>
    
    <h1 class="article-title">Notes on Sensitivity and Copulas</h1>
    
    <span class="article-date">2020-05-14</span>
    
    
    
    <span class="tags">
    
    
    Tags:
    
    <a href='/tags/micro'>micro</a>
    
    
    
    </span>
    
    <div class="article-content">
      
<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>

<div id="TOC">
<ul>
<li><a href="#reference">Reference</a>
<ul>
<li><a href="#relevant-repositories">Relevant Repositories</a></li>
<li><a href="#links">Links</a></li>
<li><a href="#papers-books">Papers &amp; Books</a></li>
</ul></li>
<li><a href="#working-with-econsa">Working With <span><code>econsa</code></span></a>
<ul>
<li><a href="#eoq-economic-order-quantity-model">EOQ (Economic Order Quantity) Model</a></li>
<li><a href="#main-code">Main Code</a></li>
<li><a href="#figure-2-from-borgonovo-plischke-2016">Figure 2 from Borgonovo &amp; Plischke (2016)</a></li>
<li><a href="#figure-3-from-borgonovo-plischke-2016">Figure 3 from Borgonovo &amp; Plischke (2016)</a></li>
<li><a href="#figure-4-from-borgonovo-plischke-2016">Figure 4 from Borgonovo &amp; Plischke (2016)</a></li>
<li><a href="#figure-1-from-harris-1990">Figure 1 from Harris (1990)</a></li>
</ul></li>
<li><a href="#local-sensitivity-analysis">Local Sensitivity Analysis</a>
<ul>
<li><a href="#one-at-a-time-method">One at a Time Method</a></li>
<li><a href="#scenario-decomposition">Scenario Decomposition</a></li>
<li><a href="#one-way-sensitivity-function">One Way Sensitivity Function</a></li>
<li><a href="#differentiation-based-methods">Differentiation-Based Methods</a></li>
<li><a href="#screening-morris-method">Screening: Morris Method</a></li>
</ul></li>
<li><a href="#global-sensitivity-analysis">Global Sensitivity Analysis</a>
<ul>
<li><a href="#non-parametric-methods">Non-Parametric Methods</a></li>
<li><a href="#variance-based-methods">Variance-Based Methods</a></li>
<li><a href="#density-based-methods">Density-Based Methods</a></li>
<li><a href="#transformation-invariant-methods">Transformation Invariant Methods</a></li>
</ul></li>
<li><a href="#decision-sensitivity-find-the-maximiser">Decision Sensitivity (Find the Maximiser)</a></li>
<li><a href="#estimate-sensitivity">Estimate Sensitivity</a></li>
</ul>
</div>

<p><a href="https://app.clickup.com/2516680/v/dc/2ctp8-134/2ctp8-57">Thesis Roadmap</a></p>
<div id="reference" class="section level2">
<h2>Reference</h2>
<div id="relevant-repositories" class="section level3">
<h3>Relevant Repositories</h3>
<ul>
<li><a href="https://github.com/jonathf/chaospy">jonathf/chaospy</a>
<ul>
<li><a href="https://chaospy.readthedocs.io/en/master/index.html">ChaosPy documentation</a></li>
</ul></li>
<li><a href="https://github.com/OpenSourceEconomics/econsa">OpenSourceEconomics/econsa</a>
<ul>
<li><a href="https://econsa.readthedocs.io/en/latest/">econsa 0.01 documentation</a></li>
</ul></li>
<li><a href="https://github.com/OpenSourceEconomics/tespy">OpenSourceEconomics/tespy</a></li>
<li><a href="https://github.com/covid-19-impact-lab/sid">covid-19-impact-lab/sid</a></li>
<li><a href="https://github.com/simetenn/uncertainpy">simetenn/uncertainpy</a>
<ul>
<li><a href="https://uncertainpy.readthedocs.io/en/latest/index.html">Uncertainpy 1.2.1 documentation</a></li>
</ul></li>
</ul>
</div>
<div id="links" class="section level3">
<h3>Links</h3>
<ul>
<li>(Finished) <a href="https://towardsdatascience.com/introducing-copula-in-monte-carlo-simulation-9ed1fe9f905">Introducing Copula in Monte Carlo Simulation - Towards Data Science</a></li>
<li>(Finished) <a href="https://chaospy.readthedocs.io/en/master/distributions/index.html">Distributions — ChaosPy documentation</a>
<ul>
<li>(Reading) <a href="https://chaospy.readthedocs.io/en/master/distributions/copulas.html">Copulas — ChaosPy documentation</a></li>
</ul></li>
<li><a href="https://uncertainpy.readthedocs.io/en/latest/theory/rosenblatt.html#rosenblatt">Dependency between uncertain parameters — Uncertainpy 1.2.1 documentation</a></li>
<li><a href="https://ose-resources.readthedocs.io/en/latest/miscellaneous.html#continuous-integration">Continuous Integration — ose-resources documentation</a></li>
</ul>
</div>
<div id="papers-books" class="section level3">
<h3>Papers &amp; Books</h3>
<ul>
<li>(Finished) <a href="https://www.sciencedirect.com/science/article/abs/pii/S0377221715005469">Sensitivity analysis: A review of recent advances - ScienceDirect</a></li>
<li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0951832017300625?via%3Dihub">Extending Morris method for qualitative global sensitivity analysis of models with dependent inputs - ScienceDirect</a></li>
<li><a href="https://ethz.ch/content/dam/ethz/special-interest/baug/ibk/risk-safety-and-uncertainty-dam/publications/master-theses/master-thesis-pages/pwiederkehr_MSc.pdf">Global Sensitivity Analysis with Dependent Inputs</a></li>
<li>Chapter 1.5 of <a href="https://ethz.ch/content/dam/ethz/special-interest/baug/ibk/risk-safety-and-uncertainty-dam/publications/doctoral-theses/ThesisCaniou.pdf">Global Sensitivity Analysis for Nested and Multiscale Modelling</a></li>
<li>Section 3 of <a href="https://www.sciencedirect.com/science/article/pii/S1877750315300119">Chaospy: An open source tool for designing methods of uncertainty quantification - ScienceDirect</a></li>
<li>Section 4 of <a href="https://www.sciencedirect.com/science/article/pii/S0010465511004085">Estimation of global sensitivity indices for models with dependent variables - ScienceDirect</a></li>
<li><a href="https://www.amazon.de/dp/B00OD4140E/">Dependence Modeling with Copulas (Chapman &amp; Hall/CRC Monographs on Statistics and Applied Probability Book 133) (English Edition) eBook: Joe, Harry</a></li>
<li><a href="https://www.amazon.de/dp/B00DZ0OVMU/">An Introduction to Copulas (Springer Series in Statistics) (English Edition) eBook: Nelsen, Roger B.</a></li>
</ul>
</div>
</div>
<div id="working-with-econsa" class="section level2">
<h2>Working With <a href="https://github.com/OpenSourceEconomics/econsa"><code>econsa</code></a></h2>
<div id="eoq-economic-order-quantity-model" class="section level3">
<h3>EOQ (Economic Order Quantity) Model</h3>
<p>From: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0377221715005469">Borgonovo &amp; Plischke (2016)</a></p>
<ul>
<li>Notation from <a href="https://pubsonline.informs.org/doi/abs/10.1287/opre.38.6.947">Harris (1990)</a>:
<span class="math display">\[\begin{aligned}T &amp;= \frac{1}{2\cdot 12R\cdot M}(CX + S) +\frac{S}{X} + C \\
X^* &amp;= \sqrt{\frac{24R\cdot MS}{C}} \\
T^* &amp;= \Big( \sqrt{\frac{S}{24R\cdot M}}+\sqrt{C} \Big)^2 \end{aligned}\]</span>
<ul>
<li>Total cost: <span class="math inline">\(T\)</span></li>
<li>Interest &amp; depreciation cost: <span class="math inline">\(R=10\)</span></li>
<li>Unit price of items: <span class="math inline">\(C\)</span></li>
<li>Units per month: <span class="math inline">\(M\)</span></li>
<li>Ordering cost: <span class="math inline">\(S\)</span></li>
</ul></li>
<li>Notation from this paper:
<span class="math display">\[\begin{aligned}y &amp;= g(x) \\
&amp;= \sqrt{\frac{24r\cdot x_1 x_3}{x_2}} \end{aligned}\]</span></li>
<li>Output: <span class="math inline">\(\mathbf{y}\in\mathcal{Y}\subset\mathbb{R}^D\)</span></li>
<li>Inputs: <span class="math inline">\(\mathbf{x}\in\mathcal{X}\subset\mathbb{R}^n\)</span>
<ul>
<li>Units per month: <span class="math inline">\(x_1\)</span></li>
<li>Unit price of items: <span class="math inline">\(x_2\)</span></li>
<li>Ordering cost: <span class="math inline">\(x_3\)</span></li>
</ul></li>
<li>Technology: <span class="math inline">\(g: \mathcal{Y}\to \mathcal{X}\)</span>
<ul>
<li>Interest &amp; depreciation cost: <span class="math inline">\(r=10\)</span></li>
</ul></li>
<li>Subset of inputs: <span class="math inline">\(\mathbf{x}_{\alpha} = \{x_{i_1}, x_{i_2}, \dots, x_{i_k}\}\)</span>
<ul>
<li>Index: <span class="math inline">\(\alpha = \{i_1, i_2, \dots, i_k\}\)</span>, <span class="math inline">\(k\leq n\)</span></li>
<li>Complementary set: <span class="math inline">\(\mathbf{x}_{\sim\alpha}\)</span></li>
</ul></li>
<li>Sensitivity of <span class="math inline">\(x_i\)</span>: sensitivity of model output wrt <span class="math inline">\(x_i\)</span>
<ul>
<li>Also: importance measure</li>
</ul></li>
</ul>
</div>
<div id="main-code" class="section level3">
<h3>Main Code</h3>
<p>Set up:</p>
<pre class="python"><code>import numpy as np
import scipy.stats as stats
import chaospy as cp
import matplotlib.pyplot as plt
import seaborn as sns</code></pre>
<p>Code for this model:</p>
<pre class="python"><code>def eoq_harris(params, x):
    &quot;&quot;&quot;
    Economic order quantity model by Harris (1990),
    https://doi.org/10.1287/opre.38.6.947,
    as seen in Borgonovoa &amp; Plischkeb (2016),
    https://doi.org/10.1016/j.ejor.2015.06.032
    
    Equation: y = sqrt(24*r*x1*x3 / x2),
    where r is interest &amp; depreciation rate,
    and takes a value of 10 in both papers.
    
    Args: 
        params (np.array): 1d numpy array,
                           cuurrently only take the first param,
                           which is interest &amp; depreciation rate, r=10.
        x (np.array or list): 2d numpy array with the independent variables,
                              currently only take the first 3 columns.
    Output:
        y (np.array): 1d numpy array with the dependent variables.
    &quot;&quot;&quot;
    
    x_np = np.array(x)
    r = params.flatten()[0]
    
    y = np.zeros(x_np.T.shape[0])
    y = np.sqrt((24 * r * x_np[0] * x_np[2])/x_np[1])
    
    return(y)</code></pre>
<p>Code for generating data (Monte Carlo):</p>
<pre class="python"><code># Set flags

seed = 1234
n = 10000

x_min_multiplier = 0.9
x_max_multiplier = 1.1
x0_1 = 1230
x0_2 = 0.0135
x0_3 = 2.15

params = np.zeros(shape=(1,1))
params[0,0] = 10

# Monte Carlo with rvs
np.random.seed(seed)
x_1 = stats.uniform(x_min_multiplier*x0_1,
                    x_max_multiplier*x0_1).rvs(10000)
x_2 = stats.uniform(x_min_multiplier*x0_2,
                    x_max_multiplier*x0_2).rvs(10000)
x_3 = stats.uniform(x_min_multiplier*x0_3,
                    x_max_multiplier*x0_3).rvs(10000)
x = np.array([x_1, x_2, x_3])

# Monte Carlo with Chaospy
sample_rule = &quot;random&quot;
np.random.seed(seed)
x_1 = cp.Uniform(x_min_multiplier*x0_1,
                 x_max_multiplier*x0_1).sample(n, rule=sample_rule)
x_2 = cp.Uniform(x_min_multiplier*x0_2,
                 x_max_multiplier*x0_2).sample(n, rule=sample_rule)
x_3 = cp.Uniform(x_min_multiplier*x0_3,
                 x_max_multiplier*x0_3).sample(n, rule=sample_rule)
x = np.array([x_1, x_2, x_3])

# Calculate y
y = eoq_harris(params, x)</code></pre>
</div>
<div id="figure-2-from-borgonovo-plischke-2016" class="section level3">
<h3>Figure 2 from Borgonovo &amp; Plischke (2016)</h3>
<p>Original:</p>
<p><img src="/post-img/notes-rosenblatt-copulas--bp-fig2.png" width="603" /></p>
<pre class="python"><code>plt.clf()
sns.distplot(y, hist_kws=dict(cumulative=True))

plt.clf()
sns.distplot(y)</code></pre>
</div>
<div id="figure-3-from-borgonovo-plischke-2016" class="section level3">
<h3>Figure 3 from Borgonovo &amp; Plischke (2016)</h3>
<p>Original:</p>
<p><img src="/post-img/notes-rosenblatt-copulas--bp-fig3.png" width="702" /></p>
<pre class="python"><code>plt.clf()
sns.regplot(x=x[0], y=y,
            scatter_kws={&quot;alpha&quot;:0.05})

plt.clf()
sns.regplot(x=x[1], y=y, order=2,
            scatter_kws={&quot;alpha&quot;:0.05})

plt.clf()
sns.regplot(x=x[2], y=y,
            scatter_kws={&quot;alpha&quot;:0.05}) </code></pre>
</div>
<div id="figure-4-from-borgonovo-plischke-2016" class="section level3">
<h3>Figure 4 from Borgonovo &amp; Plischke (2016)</h3>
<p>In this graph, we need to calculate <span class="math inline">\(y\)</span> for every single value of one of <span class="math inline">\(x_i\)</span>, resulting in a 10000*10000 array.</p>
<p><img src="/post-img/notes-rosenblatt-copulas--bp-fig4.png" /></p>
<p>Function:</p>
<pre class="python"><code>def eoq_harris_partial(params, x, fix_num=0):
    &quot;&quot;&quot;
    Calculate the value of eoq_harris,
    fixing one x.
    
    Args: 
        params (np.array): 1d numpy array,
                           cuurrently only need the first param,
                           which is interest &amp; depreciation rate, r=10.
        x (np.array or list): 2d numpy array with the independent variables,
                              currently only need the first 3 columns.
        fix_num (int): take value of 0~n-1.
    Output:
        y (np.array): 2d numpy array with the dependent variables,
                      keeping the fix_num-th x fixed.
    &quot;&quot;&quot;
    
    x_np = np.array(x)
    r = params.flatten()[0]
    
    y = np.zeros(shape=(x_np.T.shape[0],x_np.T.shape[0]))
    
    if fix_num==0:
        for i,x_i in enumerate(x_np[fix_num]):
            y[i] = np.sqrt((24 * r * x_i * x_np[2])/x_np[1])
    elif fix_num==1:
        for i,x_i in enumerate(x_np[fix_num]):
            y[i] = np.sqrt((24 * r * x_np[0] * x_np[2])/x_i)
    elif fix_num==2:
        for i,x_i in enumerate(x_np[fix_num]):
            y[i] = np.sqrt((24 * r * x_np[0] * x_i)/x_np[1])
    return(y)</code></pre>
<p>Calculate &amp; plotting:</p>
<pre class="python"><code>y_fix_x_0 = eoq_harris_partial(params, x, fix_num=0)

# Don&#39;t try at home! You are warned!
plt.clf()
for item in y_fix_x_0:
    sns.kdeplot(item)</code></pre>
</div>
<div id="figure-1-from-harris-1990" class="section level3">
<h3>Figure 1 from Harris (1990)</h3>
<p>Since this figure is using deterministic data, we must use different data generation process for it.</p>
<p><img src="/post-img/notes-rosenblatt-copulas--harris-fig1.png" width="479" /></p>
<p>Function:</p>
<pre class="python"><code>def eoq_harris_total_cost(params, x, y):
    &quot;&quot;&quot;
    Economic order quantity model by Harris (1990),
    https://doi.org/10.1287/opre.38.6.947,
    as seen in Borgonovoa &amp; Plischkeb (2016),
    https://doi.org/10.1016/j.ejor.2015.06.032
    
    For plotting convenience, the total cost here excludes ordering cost,
    since it is assumed to be constant, as in Harris (1990).
    
    Args: 
        params (np.array): 1d numpy array,
                           cuurrently only take the first param,
                           which is interest &amp; depreciation rate, r=10.
        x (np.array or list): 1d numpy array with the independent variables,
                              unis per month, unit cost, ordering cost.
        y (np.array): 1d numpy array, the size of order.
    Output:
        t (np.array): 1d numpy array, total cost according to each size.
    &quot;&quot;&quot;
    
    x_np = np.array(x)
    params_np = np.array(params)
    r = params_np.flatten()[0]
    
    t = np.zeros(y.shape)
    
    t_setup = np.zeros(y.shape)
    t_setup = (1/y) * x[2]
    
    t_interest = np.zeros(y.shape)
    t_interest = 1/(24 * r * x[0]) * (y*x[1] + x[2])
    
    t = t_setup + t_interest
    
    return(t_setup, t_interest, t)</code></pre>
<p>Data generation:</p>
<pre class="python"><code>y = np.arange(300,5200,1)

x_1 = 1000
x_2 = 0.1
x_3 = 2

x = np.array([x_1, x_2, x_3])

t_setup, t_interest, t = eoq_harris_total_cost(params, x, y)</code></pre>
<p>Plotting:</p>
<pre class="python"><code>plt.clf()
sns.lineplot(x=y, y=t_setup)
sns.lineplot(x=y, y=t_interest)
sns.lineplot(x=y, y=t)
plt.axvline(2190, linestyle=&quot;--&quot;, color=&quot;silver&quot;)</code></pre>
</div>
</div>
<div id="local-sensitivity-analysis" class="section level2">
<h2>Local Sensitivity Analysis</h2>
<ul>
<li>Local sensitivity: sensitivity around <span class="math inline">\(\mathbf{x}^0\in\mathcal{X}\)</span>
<ul>
<li>Base case / reference input: <span class="math inline">\(\mathbf{x}^0\)</span></li>
<li>Inputs are deterministic</li>
</ul></li>
<li>Output: <span class="math inline">\(y^0 = g(\mathbf{x}^0)\)</span></li>
</ul>
<div id="one-at-a-time-method" class="section level3">
<h3>One at a Time Method</h3>
<ul>
<li>Graph: tornado diagram</li>
<li>Base case: <span class="math inline">\(\mathbf{x}^0\)</span></li>
<li>Sensitivity case: <span class="math inline">\(\mathbf{x}^+\)</span></li>
<li>Input changes: <span class="math inline">\(\Delta ^+\mathbf{x} = \mathbf{x}^+ - \mathbf{x}^0\)</span></li>
<li>Sensitivity: finite changes of output by individual changes of input (magnitude + direction)
<span class="math display">\[\Delta ^+_i y = g\big((x_i +  \Delta ^+ x_i), \mathbf{x}^0_{\sim i} \big) - g(\mathbf{x}^0)\]</span></li>
<li>Weakness: neglects interaction effects between <span class="math inline">\(x\)</span>’s</li>
</ul>
</div>
<div id="scenario-decomposition" class="section level3">
<h3>Scenario Decomposition</h3>
<ul>
<li>Graph: generalised tornado diagram</li>
<li>Fix 2 or more base points of input
<ul>
<li><span class="math inline">\(\mathbf{x}^0\)</span>, <span class="math inline">\(\mathbf{x}^+\)</span> and <span class="math inline">\(\mathbf{x}^-\)</span></li>
</ul></li>
</ul>
<p><span class="math display">\[\begin{aligned}\Delta y &amp;= g(\mathbf{x}^+) - g(\mathbf{x}^0)\\
&amp;= \sum_{i=1}^n \phi_i + \sum_{i&lt;j} \phi_{i,j} + \sum_{i&lt;j&lt;k} \phi_{i,j,k} +\dots + \phi_{1,2,\dots,n}\end{aligned}\]</span>
where the <span class="math inline">\(2^n\)</span> terms are given by
<span class="math display">\[\begin{aligned} \phi_i &amp;= g(x_i^+,\mathbf{x}^0_{\sim i}) - g(\mathbf{x}^0) \\
\phi_{i,j} &amp;= g(x_i^+,x_j^+,\mathbf{x}^0_{\sim i,j}) - \phi_i - \phi_j - g(\mathbf{x}^0) \\
\phi_{i,j,k} &amp;= g(x_i^+,x_j^+,x_k^+,\mathbf{x}^0_{\sim i,j,k}) - \phi_{i,j} - \phi_{i,k} - \phi_{j,k} - \phi_i - \phi_j - \phi_k - g(\mathbf{x}^0) \\
\vdots &amp; \end{aligned}\]</span>
- Individual effect: <span class="math inline">\(\phi_i = \Delta ^+_i y\)</span>
- Residual interaction effect: <span class="math inline">\(\phi_{i,j}\)</span></p>
</div>
<div id="one-way-sensitivity-function" class="section level3">
<h3>One Way Sensitivity Function</h3>
<ul>
<li>Graph: spider plot</li>
<li>Register output values with varying input in normalised range</li>
<li>Sensitivity:
<span class="math display">\[\begin{aligned}h_i(x_i) &amp;= g(x_i,\mathbf{x}^0_{\sim i})\\
h_i^* &amp;= h_i(x_i) - g(\mathbf{x}^0) \\
&amp;= \phi_i = \Delta ^+_i y \end{aligned}\]</span></li>
</ul>
</div>
<div id="differentiation-based-methods" class="section level3">
<h3>Differentiation-Based Methods</h3>
<ul>
<li>Decompose <span class="math inline">\(\Delta y\)</span> using Taylor expansion:
<span class="math display">\[\begin{aligned}\Delta y &amp;= g(\mathbf{x}^+) - g(\mathbf{x}^0)\\
&amp;= \sum_{i=1}^n \frac{\partial g(\mathbf{x}^0)}{\partial x_i}\cdot(x_i^+ - x_i^0) \\
&amp;\phantom{\ggg}+\frac{1}{2}\cdot \sum_{i=1}^n\sum_{j=1}^n \frac{\partial ^2 g(\mathbf{x}^0)}{\partial x_i\partial x_j}\cdot(x_i^+ - x_i^0)(x_j^+ - x_j^0)\\
&amp;\phantom{\ggg}+ o\big(\|\mathbf{x}^+ - \mathbf{x}^0 \|^2\big)\\
&amp;\approx \sum_{i=1}^n \underbrace{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}}_{\begin{subarray}{l}\text{Natural}\\\text{sensitivity}\end{subarray}}\cdot(x_i^+ - x_i^0) \end{aligned}\]</span>
However, natural sensitivities are not comparable since different <span class="math inline">\(x\)</span>’s have different units.<br />
</li>
<li>Differential importance measure:<br />
If <span class="math inline">\(dx_i = dx_j\)</span> for all <span class="math inline">\(i,j\)</span> (uniform perturbation), this is equivalent to partial derivatives.<br />
Comparable &amp; additive (does not account for interaction).
<span class="math display">\[D_i = \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ dx_i}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ dx_j\big)}\]</span></li>
<li>Elasticity of <span class="math inline">\(y\)</span> wrt <span class="math inline">\(x_i\)</span> (also as: criticality importance measure)<br />
If <span class="math inline">\(\frac{d x_i}{x_i^0} = \frac{d x_j}{x_j^0}\)</span> for all <span class="math inline">\(i,j\)</span> (proportional perturbation), elasticity is equivalent to differential importance:
<span class="math display">\[\begin{aligned}E_i &amp;= \frac{\partial g(\mathbf{x}^0)}{\partial x_i}\cdot \frac{x_i^0}{g(\mathbf{x}^0)} \\
D_i &amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ dx_i}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ dx_j\big)} \\
&amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ dx_i\cdot\frac{x_i^0}{x_i^0}}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ dx_j\cdot\frac{x_j^0}{x_j^0}\big)} \\
&amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ x_i^0 \cdot\frac{dx_i}{x_i^0}}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ x_j^0 \cdot\frac{dx_j}{x_j^0}\big)} \\
&amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\ x_i^0}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\ x_j^0} \\
&amp;= \frac{\frac{\partial g(\mathbf{x}^0)}{\partial x_i}\cdot\frac{x_i^0}{g(\mathbf{x}^0)}}
{\sum_{j=1}^n \big( \frac{\partial g(\mathbf{x}^0)}{\partial x_j}\cdot\frac{x_j^0}{g(\mathbf{x}^0)}} \\
&amp;= \frac{E_i}{\sum_{j=1}^n E_j}\end{aligned}\]</span></li>
</ul>
</div>
<div id="screening-morris-method" class="section level3">
<h3>Screening: Morris Method</h3>
<p>TBA</p>
</div>
</div>
<div id="global-sensitivity-analysis" class="section level2">
<h2>Global Sensitivity Analysis</h2>
<ul>
<li>Assumption: have info about model inputs’ probability distribution</li>
<li>Input
<ul>
<li>Probability space: <span class="math inline">\(\big(\mathcal{X}, \mathcal{B}(\mathcal{X}), \mathbb{P}_{\mathbf{X}}\big)\)</span></li>
<li>Probability distribution: <span class="math inline">\(\mathbb{P}_{\mathbf{X}}\)</span></li>
<li>Random vector: <span class="math inline">\(\mathbf{X} = (X_1, \dots, X_n)\)</span></li>
<li>Realisation: <span class="math inline">\(\mathbf{x} = (x_1, \dots, x_n)\)</span></li>
<li>Joint cdf &amp; pdf: <span class="math inline">\(F_{\mathbf{X}}(\mathbf{x})\)</span>, <span class="math inline">\(f_{\mathbf{X}}(\mathbf{x})\)</span></li>
<li>Marginal cdf &amp; pdf: <span class="math inline">\(F_{X_i}(x_i)\)</span>, <span class="math inline">\(f_{X_i}(x_i)\)</span></li>
<li>Standard deviation of <span class="math inline">\(X_i\)</span>: <span class="math inline">\(\sigma_i\)</span></li>
<li>If inputs are independent, then
<span class="math display">\[F_{\mathbf{X}}(\mathbf{x})=\prod_{i=1}^n F_i(x_i)\]</span></li>
</ul></li>
<li>Output
<ul>
<li>Random variable: <span class="math inline">\(Y = g(\mathbf{X})\)</span></li>
<li>Probability distribution: <span class="math inline">\(\mathbb{P}_Y(\cdot) = \mathbb{P}_{\mathbf{X}}\big(g^{-1}(\cdot)\big)\)</span></li>
<li>cdf &amp; pdf: <span class="math inline">\(F_Y(y)\)</span>, <span class="math inline">\(f_Y(y)\)</span></li>
<li>Standard deviation of <span class="math inline">\(Y\)</span>: <span class="math inline">\(\sigma_Y\)</span></li>
</ul></li>
<li>Monte Carlo
<ul>
<li>Generate input sample: <span class="math inline">\(N\times n\)</span> (N <span class="math inline">\(\mathbf{x}\)</span>’s)</li>
<li>Evaluate output: <span class="math inline">\(N\times D\)</span> (N <span class="math inline">\(Y\)</span>’s)</li>
</ul></li>
</ul>
<div id="non-parametric-methods" class="section level3">
<h3>Non-Parametric Methods</h3>
<ul>
<li>Assumption: input-output sample is well fitted by linear regression</li>
<li>If <span class="math inline">\(R_Y^2\)</span> (goodness-of-fit) is too low, should consider other methods.</li>
<li>Response surface:
<span class="math display">\[g(\mathbf{x}) \approx b_0 + \sum_{i=1}^n b_i X_i\]</span></li>
<li>Standardised regression coefficient:
<span class="math display">\[\begin{aligned}SRC_i &amp;= b_i\cdot\frac{\sigma_i}{\sigma_Y} \\
&amp;= b_i\cdot\frac{\sqrt{\sum_{j=1}^N (x_{ji} - \bar{x}_i)^2}}{\sqrt{\sum_{j=1}^N (y_j - \bar{y})^2}}  \end{aligned}\]</span></li>
<li>Pearson’s product moment correlation coefficient:
<span class="math display">\[\begin{aligned}PEAR_i &amp;= \varrho(Y,X_i)\\
&amp;= \frac{Cov(Y,X_i)}{\sigma_i \sigma_Y}\\
&amp;= \frac{\sum_{j=1}^N (x_{ji} - \bar{x}_i)(y_j - \bar{y})}{\sqrt{\sum_{j=1}^N (x_{ji} - \bar{x}_i)^2}\cdot \sqrt{\sum_{j=1}^N (y_j - \bar{y})^2}} \end{aligned}\]</span></li>
</ul>
</div>
<div id="variance-based-methods" class="section level3">
<h3>Variance-Based Methods</h3>
<ul>
<li>Importance of input = reduction of output variance if know this input for sure</li>
<li>Variance decomposition:
<span class="math display">\[Var(Y) = Var\big(E[Y\ |\ X_{\alpha}]\big) + E\big[Var(Y\ |\ X_{\alpha})\big]\]</span></li>
<li>Non-linear response surface:
<span class="math display">\[\phi_{\alpha}(x_{\alpha}) = E[Y\ |\ X_{\alpha}=x_{\alpha}]\]</span></li>
<li>Group effect: (fraction of output variance that is explained by functional dependence on <span class="math inline">\(x_{\alpha}\)</span>)
<span class="math display">\[S_{\alpha}^{\text{group}} = \frac{Var(\phi_{\alpha}(x_{\alpha}))}{Var(Y)}\]</span>
If <span class="math inline">\(\alpha=\{i\}\)</span>, then the main effect / first order index:
<span class="math display">\[\begin{aligned} S_i &amp;= S_{i}^{\text{group}}\\
&amp;= \varrho^2\big(Y,E[Y\ |\ X_i]\big) \\
&amp;\geq \beta_i^2\end{aligned}\]</span>
Higher order effect:
<span class="math display">\[S_{\alpha} = S_{\alpha}^{\text{group}} - \sum_{\beta\not\subset\alpha} S_{\beta}\]</span>
Total effect: (this is the measure to decide whether input <span class="math inline">\(i\)</span> is important or not)
<span class="math display">\[S^T_i = 1 - S_{\sim i}^{\text{group}}\]</span>
When inputs are uncorrelated, <span class="math inline">\(S^T_i\geq S_i\)</span>, and <span class="math inline">\(\sum_{i=1}^n S_i\leq 1\)</span>.</li>
<li>ANOVA decomposition of <span class="math inline">\(g(\mathbf{x})\)</span>:
<span class="math display">\[\begin{aligned}g(\mathbf{x}) &amp;= g_0 + \sum_{i=1}^n g_i(x_i) + \sum_{i&lt;j} g_{i,j}(x_i, x_j) \\
&amp;\phantom{\ggg}+\sum_{i&lt;j&lt;k} g_{i,j,k}(x_i,x_j,x_k)+\dots + g_{1,2,\dots,n}(x_1, x_2,\dots,x_n) \end{aligned}\]</span>
where
<span class="math display">\[\begin{aligned}g_0 &amp;= \int_{\mu_1}\dots\int_{\mu_n} g(\mathbf{x})\,d\mu \\
g_i(x_i) &amp;= \int\dots\int g(\mathbf{x})\,d\mu_{k\neq i} - g_0 \\
g_{i,j}(x_i, x_j) &amp;= \int\dots\int g(\mathbf{x})\,d\mu_{k\neq i,j} - g_i(x_i) - g_j(x_j) - g_0 \\
\vdots &amp;\end{aligned}\]</span>
If inputs are independent, then <span class="math inline">\(g_{\alpha}(\mathbf{x}_{\alpha})\)</span> are orthogonal, and
<span class="math display">\[\begin{aligned}Var(Y) &amp;= E\big[\big(g(\mathbf{X}) - g_0\big)^2 \big] \\
&amp;= \sum_{\alpha} V_{\alpha}\\
V_{\alpha} &amp;= \int_{x_{\alpha}} \big[g_{\alpha}(\mathbf{x}_{\alpha}) \big]^2\cdot f_{\alpha}(\mathbf{x}_{\alpha})\, d\mathbf{x}_{\alpha} \end{aligned}\]</span></li>
<li>Zero value of <span class="math inline">\(S_i\)</span> or <span class="math inline">\(V_i\)</span> does not assure that output is independent on <span class="math inline">\(X_i\)</span>
<ul>
<li><span class="math inline">\(\implies\)</span> Ishigami test function</li>
<li>In general, using moments as proxy to uncertainty is a bad idea.</li>
</ul></li>
</ul>
</div>
<div id="density-based-methods" class="section level3">
<h3>Density-Based Methods</h3>
<ul>
<li>Consider the entire distribution w/o referencing a particular moment</li>
<li>Unconditional output pdf: <span class="math inline">\(f_Y(y)\)</span></li>
<li>Local sensitivity: plot output pdf given <span class="math inline">\(X_i\)</span>, <span class="math inline">\(f_{Y\ |\ X_i}(y)\)</span>, then compare the distance (separation)</li>
<li>Global sensitivity: plot as many pdf curves as the possible <span class="math inline">\(X_i\)</span>’s
<ul>
<li>Inner separation: <span class="math inline">\(\gamma_i(x_i)\)</span></li>
</ul></li>
<li><span class="math inline">\(\delta\)</span>-sensitivity measure: (does not require independence)<br />
<span class="math inline">\(\delta_i=0\iff Y\)</span> is independent of <span class="math inline">\(X_i\)</span>.<br />
<span class="math inline">\(\delta_i\)</span> is monotonic transformation invariant.
<span class="math display">\[\begin{aligned}\gamma_i(x_i) &amp;= \int_y \big| f_Y(y) - f_{Y\ |\ X_i}(y)\big| \,dy \\
\delta_i &amp;=\frac{1}{2}\cdot E\big[\gamma_i(x_i)\big] \\
&amp;= \frac{1}{2}\cdot\int_{X_i}\int_Y \big| f_{Y,X_i}(y,x_i) - f_{Y}(y)\cdot f_{X_i}(x_i)\big|\, dydx_i \\
&amp;\in [0,1]  \end{aligned}\]</span></li>
<li>Kullback-Leibler divergence:<br />
<span class="math inline">\(\theta_i^{KL}=0\iff Y\)</span> is statistically independent of <span class="math inline">\(X_i\)</span>.<br />
<span class="math inline">\(\theta_i^{KL}\)</span> is monotonic transformation invariant.
<span class="math display">\[\begin{aligned}\gamma_i^{KL}(x_i) &amp;= \int_y f_{Y\ |\ X_i}(y)\cdot\big(\log f_{Y\ |\ X_i}(y) - \log f_Y(y)\big)\, dy \\
\theta_i^{KL} &amp;= E\big[\gamma_i^{KL}(x_i) \big]\end{aligned}\]</span></li>
<li>Both <span class="math inline">\(\delta_i\)</span> and <span class="math inline">\(\theta_i^{KL}\)</span> are in the family of <a href="https://en.wikipedia.org/wiki/F-divergence">Csiszar divergence</a>:
<span class="math display">\[\begin{aligned}\gamma_i^F(X_i) &amp;= \int_y f_{Y\ |\ X_i}(y)\cdot t\Big(\frac{f_Y(y)}{f_{Y\ |\ X_i}(y)}\Big)\, dy \\
\theta_i^F &amp;=E\big[\gamma_i^F(X_i) \big] \end{aligned}\]</span>
where <span class="math inline">\(t(\cdot)\)</span> is convex, <span class="math inline">\(t(0)=1\)</span>.</li>
</ul>
</div>
<div id="transformation-invariant-methods" class="section level3">
<h3>Transformation Invariant Methods</h3>
<ul>
<li>Necessary when model output is sparse</li>
<li>Any monotone function: <span class="math inline">\(u(\cdot)\)</span>
<ul>
<li>This is also convenient when <span class="math inline">\(u(\cdot)\)</span> is utility function</li>
</ul></li>
<li>Monotonic transformation of output: <span class="math inline">\(U=u(Y)\)</span>
<ul>
<li>Must reinterpret results for the original scale</li>
<li>Ranking of input is not guaranteed to be the same<br />
(Density-based measures are fine!)</li>
</ul></li>
<li>Some sensitivity measure of <span class="math inline">\(X_i\)</span>: <span class="math inline">\(\xi_i(Y)\big[\xi_i(U)]\)</span>
<ul>
<li>Monotonic transformation invariant: <span class="math inline">\(\xi_i(U)=\xi_i(Y)\)</span> for all <span class="math inline">\(U=u(Y)\)</span></li>
</ul></li>
<li>Global sensitivity measure is transformation invariant if the inner statistic is transformation invariant.</li>
<li>Kolmogorov-Smirnov metric:
<span class="math display">\[\beta_i^{KS} = E\big[ \sup_y \big| F_Y(y)-F_{Y\ |\ X_i}(y) \big| \big]\]</span></li>
<li>Kuiper distance:
<span class="math display">\[\beta_i^{Ku} = E\Big[ \sup_y \big( F_Y(y)-F_{Y\ |\ X_i}(y) \big) + \sup_y \big(F_{Y\ |\ X_i}(y) - F_Y(y)\big) \big]\]</span></li>
<li>The inner statistic of both <span class="math inline">\(\beta_i^{KS}\)</span> and <span class="math inline">\(\beta_i^{Ku}\)</span> are in the family of:
<span class="math display">\[z(\mathbb{P}, \mathbb{Q}) = \sup_{A\in\mathcal{A}} h\big(\big|\mathbb{P}(A) - \mathbb{Q}(A)\big|\big)\]</span>
where <span class="math inline">\(\mathbb{P}, \mathbb{Q}\)</span> are probability measures on <span class="math inline">\((\Omega, \mathcal{A})\)</span>,<br />
<span class="math inline">\(h: \mathbb{R}_+\mapsto\mathbb{R}_+\)</span> is continuous and non-decreasing,<br />
<span class="math inline">\(h(0)=0\)</span>, and <span class="math inline">\(\sup_t \frac{h(2t)}{h(t)}&lt;\infty\)</span>.</li>
</ul>
</div>
</div>
<div id="decision-sensitivity-find-the-maximiser" class="section level2">
<h2>Decision Sensitivity (Find the Maximiser)</h2>
<p>(TBA when needed.)</p>
</div>
<div id="estimate-sensitivity" class="section level2">
<h2>Estimate Sensitivity</h2>
<ul>
<li>Types of estimation strategies
<ul>
<li>Require some specific design: data must be generated under the scheme</li>
<li>Does not require design: can use any Monte Carlo generated data, or data generated under any scheme</li>
</ul></li>
<li>Design schemes
<ul>
<li>FAST (Fourier Amplitude Sensitivity Test)</li>
<li>RBD (Random Balance Design)</li>
</ul></li>
<li>Given data approaches
<ul>
<li>EASI (Effective Algorithm for global Sensitivity Indices)</li>
<li>COSI (COsine transformation for global Sensitivity Indices)</li>
</ul></li>
<li>Meta-model (emulator) approach
<ul>
<li>Gaussian process</li>
<li>Smoothing spline ANOVA</li>
<li>Polynomial chaos expansion (this is probably relevant)</li>
<li>Sparse grid interpolation</li>
</ul></li>
</ul>
<p>(TBA when needed.)</p>
</div>

    </div>
  </article>
  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  (function() { 
  var d = document, s = d.createElement('script');
  s.src = 'https://loikein-github.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>

</main>
      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>

          <li>Theme by loikein with love</li>
        </ul>
      </footer>
    </div>
    
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/applescript.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/10.0.0/languages/vim.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>


    
<script src="/js/math-code.js"></script>
<script async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-143089736-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>
