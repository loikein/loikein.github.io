<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Implementing the “Wisdom of the Crowd”</title>
    <meta charset="utf-8" />
    <meta name="author" content="loikein" />
    <script src="slides-libs/jquery-1.11.3/jquery.min.js"></script>
    <script src="slides-libs/elevate-section-attrs-2.0/elevate-section-attrs.js"></script>
    <link href="slides-libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="slides-libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="slides-libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Implementing the “Wisdom of the Crowd”
## Kremer, Mansour, Perry (2014)
### loikein
### 2020/01/27

---


## Motivation &amp; Background

- Online reputation systems
    - Principal (platform) gives recommendation
    - Agents (users) give feedback

--

- Examples
    - TripAdvisor, Booking, etc
    - Waze, Moovit, etc
    - Doctors recommendation platforms

???

Here go the presenter notes.

---

## The Model

- Two possible actions: `\(A=\{a_1,a_2\}\)`
- Unknown reward: `\(R_i\sim π_i\)`
    - Expected value: `\(μ_i= E[R_i]\)`
    - `\(μ_1\geq μ_2\)`

--

- Principal
    - Goal: average total payoff `$$\max \ E\Big[\frac{1}{T}\textstyle{\sum}^T R^t\Big]$$`
    - Message: `\(\tilde{M}^t: H^{t-1}\to M^t\)`

--

- Agents: `\(t\in T\)` 
    - Appears sequentially
    - Goal: expected payoff `$$\max \ E\big[R^t\ |\ M^t\big]$$`
    - Strategy: `\(σ^t: M^t\to A\)`

---

## Timeline

- Full informational transparency:
    1. Agent `\(t\)` arrives, and asks for recommendation
    1. The principal send message to recommend one action, for example, `\(a_2\)`
        - If `\(R_1&lt;\mu_2\)`, the agent takes action `\(a_2\)`
        - Otherwise, the agent never takes action `\(a_2\)`  
        `\(\implies\)` the principal never finds out about `\(R_2\)` (!)

--

- Problems
    - Principal relies on agents to explore
    - Agents does not want to explore the action with worse expectation
    - By assumption, `\(Pr(R_2&gt;R_1)\neq 0\)`

--

- We want a mechanism such that…
    1. Agent `\(t\)` arrives, and asks for recommendation
    1. The principal send message to recommend one action that is __incentive compatible__
    1. The agent only knows the message, and how the principal forms this message
    1. The agent always follows it

---

## Threshold Mechanism

- For agent `\(t=1\)`, send message `\(M_1\)`
    - `\(R_1\)` is realised

--

- For each agent `\(t\geq 2\)`, assigning __ordered intervals__ `\(I^t\)`
    - For agent `\(t=2\)`, `\(I^2 = (-\infty,i^2]\)`
    - For agnet `\(t\geq 3\)`, `\(I^t=(i^{t-1}, i^t]\)`
    - If `\(i^{t-1}=i^t\)`, then `\(I^t=\emptyset\)`
- If `\(R_1\in I^τ\)` for some `\(τ\leq T\)`, agent `\(τ\)` is the first agent sent message `\(M_2\)`
    - `\(R_2\)` is realised
    - For all agents `\(τ'&gt;τ\)`, send message for whichever action with higher reward
- If `\(R_1\not\in I^t\)` for any `\(t\in[2,T]\)`, no agent is sent message `\(M_2\)`

--

&gt; *Lemma 6*  
&gt; The optimal recommendation mechanism is a threshold mechanism

---

## Example

- For agent 1:
    - Since `\(\mu_1&gt;\mu_2\)`, always send `\(M_1\implies R_1\)` is realised
    - We can show that this message is incentive compatible (on board)

| Message to agent 1 | `\(M_1\)`          |
| ------------------ | -------------- |
| Agent 1's action   | `\(a_1\)`          |
| New finding        | `\(R_1\begin{cases}&gt;\mu_{2}\\\leq\mu_{2}\end{cases}\)` |

---

## Example (cont.)

- For agent 2:
    - If `\(R_1&gt;\mu_2\)`, send `\(M_1\implies R_2\)` is unknown
    - Otherwise, send `\(M_2\implies R_2\)` is realised

| Agent 1's action  | `\(a_1\)`            | `\(a_1\)`            |
| ----------------- | ---------------- | ---------------- |
| Agent 1's finding | `\(R_1&gt;\mu_{2}\)`    | `\(R_1\leq\mu_{2}\)` |
| Message to agent 2 | `\(M_1\)`           | `\(M_2\)`            |
| Agent 2's action  | `\(a_1\)`            | `\(a_2\)`            |
| New finding       | Nothing          | `\(R_2\begin{cases}&lt;R_1\\\geq R_1\end{cases}\)` |

---

## Example (cont.)

- For agent 3:
    - If `\(R_2\)` is realised, send whichever recommendation with higher payoff
    - Otherwise…

| Agent 1's action   | `\(a_1\)`          | `\(a_1\)`            | `\(a_1\)`            |
| ------------------ | -------------- | ---------------- | ---------------- |
| Agent 1's finding  | `\(R_1&gt;\mu_{2}\)`  | `\(R_1\leq\mu_{2}\)` | `\(R_1\leq\mu_{2}\)` |
| Agent 2's action   | `\(a_1\)`          | `\(a_2\)`            | `\(a_2\)`            |
| Agent 2's finding  | Nothing        | `\(R_1 &gt; R_2\)`      | `\(R_1 \leq R_2\)`   |
| Message to agent 3 | …              | `\(M_1\)`            | `\(M_2\)`            |

--

&lt;svg width="100%" height="50" version="1.1" xmlns="http://www.w3.org/2000/svg" style="display: block;"&gt;
  &lt;line y2="100%" x2="10%" y1="0%" x1="40.5%" stroke-width="1.5" stroke="#666" fill="none"/&gt;
  &lt;line y2="100%" x2="90%" y1="0%" x1="56.5%" stroke-width="1.5" stroke="#666" fill="none"/&gt;
&lt;/svg&gt;

| For some `\(x\)`       | `\(R_1&gt;\mu_2+x\)` | `\(R_1\leq\mu_2+x\)` |
| ------------------ | ------------- | ---------------- |
| Message to agent 3 | `\(M_1\)`         | `\(M_2\)`            |

---

## Example: Incentive Compatiblity

- `\(M_2\)` is incentive compatible if
`$$\begin{aligned}E\big[R_2-R_1\ |\ M_2\big]&amp;\geq 0 \\
\int_{M_2} (R_2-R_1)\, dπ &amp;\geq 0\end{aligned}$$`

--

- When receiving `\(M_1\)`, agent 3's incentive compatiblity constraint:
`$$\int_{R_2&lt;R_1\leq\mu_{2}}(R_1 - R_2)\, dπ + \int_{R_1&gt;\mu_2+x}(R_1-\mu_2)\, dπ\geq 0$$`
- When receiving `\(M_2\)`:
`$$\int_{R_1\leq\mu_{2},\ R_1 \leq R_2}(R_2-R_1)\, dπ + \int_{R_1\leq\mu_2+x}(\mu_2-R_1)\, dπ\geq 0$$`

--

&gt; *Lemma 3*  
&gt; If `\(M_2\)` is incentive compatible, then `\(M_1\)` is also incentive compatible.

---

## Main Results (Infinite `\(T\)`)

- Increasing sequence of thresholds `\(\{i^{t,\infty}\}\)`  
- For agent `\(t=2\)`, `\(I^2 = (-\infty,i^{2,\ \infty}]\)`,  
`\(i^{2,\ \infty}\)` is characterised by:
`$$\begin{aligned}\underbrace{\int_{R_1\in I^2}(μ_2 - R_1)\, dπ}_{\begin{subarray}{l}\text{Expected loss if }M_2\\
\text{means to explore}\end{subarray}} = 0\end{aligned}$$`
- For agents `\(t\geq 3\)`, `\(I^t=(i^{t-1,\ \infty}, i^{t,\ \infty}]\)`, if `\(i^{t,\ \infty}&lt;\infty\)` and `\(i^{t+1,\ \infty}&lt;\infty\)`,  
`\(i^{t+1,\ \infty}\)` is characterised by:
`$$\begin{aligned} \underbrace{\int_{R_1\leq i^t,\ R_1&lt;R_2}(R_2-R_1)\, dπ}_{\begin{subarray}{l}\text{Expected gain if }M_2\\
\text{means to exploit}\end{subarray}}
+\underbrace{\int_{R_1\in I^t}(μ_2 - R_1)\, dπ}_{\begin{subarray}{l}\text{Expected loss if }M_2\\
\text{means to explore}\end{subarray}}=0\end{aligned}$$`

---

## Main Results (Finite `\(T\)`)

- If agent `\(t\)` explores, gain for all agents afterwards is:
`$$(T-t)E\big[\max\{R_2-R_1,\  0\}\big]$$`
Let `\(\theta_t\)` be the maximum value of `\(R_1\)` where
`$$\underbrace{(T-t)E\big[\max\{R_2-\theta_t,\  0\}\big]}_{\text{Expected gain if explore}} \geq \underbrace{\theta_t - \mu_2}_{\text{Expected gain if never explore}}$$`

--

&gt; *Theorem 1*  
&gt; Optimal mechanism `\(M^{opt}\)` is defined by a sequence of thresholds
&gt; `$$i^{t,T} = \min\{i^{t,\ \infty}, θ_τ\}$$`
&gt; where `\(τ\)` is the minimal index where `\(i^{t,\infty}&gt;θ_t\)`

---

## Extensions

- `\(\to\)` Imperfect infomation about location
- Stochastic reward
- Principal can make monetary transfers to incentivise exploration
- More than two alternatives

---

## Imperfect Infomation

- Agents: `\(t\in T\)`

 

- Groups of agents: `\(1=τ^1&lt;τ^2&lt;\dots&lt;τ^k=T+1\)`, `\(τ^j\)` are integers for all `\(j\)`
    - Group `\(i\)`: `\(t\in[τ^j, τ^{j+1})\)` for `\(j\leq k\)`
    - Agent `\(t=1\)` knows their exact position `\(t\)`
    - Agents `\(t\geq 2\)` only know about the group `\(j\)` which they belong to
    - When `\(k=T\)`, this case degenerate into the original case

---

## Threshold Mechanism (Imperfect Info)

- For agent `\(t=1\)`, send message `\(M_1\)`
- For all agents `\(t\geq 2\)`, assign `\(I^t = (i^{t-1}, i^t]\)`

--

- When receiving `\(M_2\)`, agent `\(t\in[τ^j, τ^{j+1})\)`'s incentive compatibility constraint:
`$$\frac{1}{τ^{j+1} - τ^j}\sum_{t=τ^j}^{τ^{j+1}-1}\Big(\underbrace{\int_{R_1\leq i^{t-1},\ R_1&lt;R_2}(R_2-R_1)\, dπ}_{\text{Expected gain from exploitation}}+
\underbrace{\int_{R_1\in I^t}(μ_2 - R_1)\, dπ}_{\text{Expected loss from exploration}} \Big)\geq 0$$`

--

- We can do better by lowering `\(i^t\)` for all agents…
    - Unchanged expected gain, lower expected loss
    - …except for the first agents in each group `\(t=τ^j\)`, otherwise the next group will be affected

---

## Main Results (Imperfect Info)

&gt; *Lemma 7*  
&gt; For every group `\(j\)`:  
&gt; The first agent `\(τ^j\)` has threshold `\(I^{τ^j}=(i^{τ^{j-1}}, i^{τ^j}]\)`.  
&gt; All the rest agents `\(t\in(τ^j, τ^{j+1})\)` has thresholds `\(I^t=\emptyset\)`.

　

&gt; *Theorem 3*  
&gt; The optimal mechanism is defined by a sequence of thresholds `\(\{θ_i\}\)`  
&gt; The only agent who explores is the first agent in group `\(j\)` where `\(r_1\in(θ_{j-1}, θ_j]\)`.

　

&gt; *Theorem 4*  
&gt; As the information that agents have about their location becomes coarser, the policy that the planner can implement generates less welfare loss.



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
