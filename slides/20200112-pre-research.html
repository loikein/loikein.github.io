<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Implementing the “Wisdom of the Crowd”</title>
    <meta charset="utf-8" />
    <meta name="author" content="loikein" />
    <script src="slides-libs/jquery-1.11.3/jquery.min.js"></script>
    <script src="slides-libs/elevate-section-attrs-2.0/elevate-section-attrs.js"></script>
    <link href="slides-libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="slides-libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="slides-libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Implementing the “Wisdom of the Crowd”
## Kremer, Mansour, Perry (2014)
### loikein
### 2020/01/27

---


## Motivation &amp; Background

- Online reputation systems
    - Principal (platform) gives recommendation
    - Agents (users) give feedback

--

- Examples
    - TripAdvisor, Booking, etc
    - Waze, Moovit, etc
    - Doctors recommendation platforms

???

These are my notes.

---

## The Model

- Two possible actions: `\(A=\{a_1,a_2\}\)`
- Unknown reward: `\(R_i\sim π_i\)`
    - Expected value: `\(μ_i= E[R_i]\)`
    - `\(μ_1\geq μ_2\)`

--

- Principal
    - Goal: aggregated social welfare `$$\max \ E\Big[\frac{1}{T}\textstyle{\sum}^T R^t\Big]$$`
    - Message: `\(\tilde{M}^t: H^{t-1}\to M^t\)`

--

- Agents: `\(t\in T\)` (appears sequentially)
    - Goal: expected payoff `$$\max \ E\big[R^t\ |\ M^t\big]$$`
    - Strategy: `\(σ^t: M^t\to A\)`

---

## Timeline

- Full informational transparency:
    1. Agent `\(t\)` arrives, and asks for recommendation
    1. The principal send message to recommend, for example, `\(a_2\)`
        - If `\(R_1&lt;\mu_2\)`, the agent takes action `\(a_2\)`
        - Otherwise, the agent never takes action `\(a_2\implies\)` the principal never finds out about `\(R_2\)` (!)

--

- Problems
    - Principal relies on agents to explore
    - Agents does not want to explore

--

- We want a mechanism such that…
    1. Agent `\(t\)` arrives, and asks for recommendation
    1. The principal send message to recommend one action that is __incentive compatible__
    1. The agent only knows the message, and how the principal forms this message
    1. The agent follows it

---

## Threshold Mechanism

- For agent `\(t=1\)`, send message `\(M_1\)`
    - `\(R_1\)` is realised
- For each agent `\(t\geq 2\)`, assigning __ordered intervals__ `\(I^t\)`
    - For agent `\(t=2\)`, `\(I^2 = (-\infty,i^2]\)`
    - For agnet `\(t\geq 3\)`, `\(I^t=(i^{t-1}, i^t]\)`
    - If `\(i^{t-1}=i^t\)`, then `\(I^t=\emptyset\)`

--

- If `\(R_1\in I^t\)` for some `\(t\leq T\)`, agent `\(t\)` is the first agent sent message `\(M_2\)`
    - `\(R_2\)` is realised
    - For all agents `\(t'&gt;t\)`, send message for whichever action with higher reward
- If `\(R_1\not\in I^t\)` for any `\(t\in[2,T]\)`, no agent is sent message `\(M_2\)`

--

&gt; *Lemma 6*  
&gt; The optimal recommendation mechanism is a threshold mechanism

---

## Example

- For agent 1:
    - Since `\(\mu_1&gt;\mu_2\)`, always send `\(M_1\implies R_1\)` is realised

| Message to agent 1 | `\(M_1\)`          |
| ------------------ | -------------- |
| Agent 1's action   | `\(a_1\)`          |
| Agent 1's finding  | `\(R_1&lt;&gt;\mu_{2}\)` |

---

## Example (cont.)

- For agent 2:
    - If `\(R_1&gt;\mu_2\)`, send `\(M_1\implies R_2\)` is unknown
    - Otherwise, send `\(M_2\implies R_2\)` is realised

| Agent 1's action  | `\(a_1\)`            | `\(a_1\)`            |
| ----------------- | ---------------- | ---------------- |
| New finding       | `\(R_1&gt;\mu_{2}\)`    | `\(R_1\leq\mu_{2}\)` |
| Message to agent 2 | `\(M_1\)`           | `\(M_2\)`            |
| Agent 2's action  | `\(a_1\)`            | `\(a_2\)`            |
| Agent 2's finding | Nothing          | `\(R_1 &lt;&gt; R_2\)`     |

---

## Example (cont.)

- For agent 3:

| Agent 1's action   | `\(a_1\)`          | `\(a_1\)`            | `\(a_1\)`            |
| ------------------ | -------------- | ---------------- | ---------------- |
| New finding        | `\(R_1&gt;\mu_{2}\)`  | `\(R_1\leq\mu_{2}\)` | `\(R_1\leq\mu_{2}\)` |
| Agent 2's action   | `\(a_1\)`          | `\(a_2\)`            | `\(a_2\)`            |
| New finding        | Nothing        | `\(R_1 &gt; R_2\)`      | `\(R_1 \leq R_2\)`   |
| Message to agent 3 | …              | `\(M_1\)`            | `\(M_2\)`            |

--

&lt;svg width="100%" height="50" version="1.1" xmlns="http://www.w3.org/2000/svg" style="display: block;"&gt;
  &lt;line y2="100%" x2="10%" y1="0%" x1="40.5%" stroke-width="1.5" stroke="#666" fill="none"/&gt;
  &lt;line y2="100%" x2="90%" y1="0%" x1="56.5%" stroke-width="1.5" stroke="#666" fill="none"/&gt;
&lt;/svg&gt;

| For some `\(x\)`       | `\(R_1&gt;\mu_2+x\)` | `\(R_1\leq\mu_2+x\)` |
| ------------------ | ------------- | ---------------- |
| Message to agent 3 | `\(M_1\)`         | `\(M_2\)`            |

---

## Incentive Compatiblity

- Incentive compatible mechanism: recommending `\(a_2\)` is IC if
`$$\begin{aligned}&amp;&amp; E\big[R_2-R_1\ |\ a^t=a_2\big]&amp;\geq 0 \\
&amp;&amp; \frac{\int_{a^t=a_2} (R_2-R_1)\, dπ}{Pr(a^t=a_2)} &amp;\geq 0 \\
(IC) &amp;&amp; \int_{a^t=a_2} (R_2-R_1)\, dπ &amp;\geq 0\end{aligned}$$`

---

## Main Results

- Infinite `\(T\)`: increasing sequence of thresholds `\(\{i^{t,\infty}\}\)`  
    - For agent `\(t=2\)`, `\(I^2 = (-\infty,i^{2,\ \infty}]\)`,  
    `\(i^{2,\ \infty}\)` is characterised by:
`$$\begin{aligned}\underbrace{\int_{R_1=-\infty}^{i^{2,\ \infty}}(R_1 - μ_2)\, dπ}_{\begin{subarray}{l}\text{Expected loss if recommended}\\
\text{to explore }a_2\end{subarray}} = 0\end{aligned}$$`
    - For agents `\(t&gt;3\)`, `\(I^t=(i^{t-1,\ \infty}, i^{t,\ \infty}]\)`, if `\(i^{t,\ \infty}&lt;\infty\)` and `\(i^{t+1,\ \infty}&lt;\infty\)`,  
    `\(i^{t+1,\ \infty}\)` is characterised by:
`$$\begin{aligned} \underbrace{\int_{R_1\leq i^t,\ R_2&gt;R_1}(R_2-R_1)\, dπ}_{\begin{subarray}{l}\text{Expected gain if recommended}\\
\text{to exploit }a_2\end{subarray}}
= \underbrace{\int_{R_1=i^{t,\ -\infty}}^{i^{t+1,\ \infty}}(R_1 - μ_2)\, dπ}_{\begin{subarray}{l}\text{Expected loss if recommended}\\
\text{to explore }a_2 \end{subarray}}\end{aligned}$$`

---

## Main Results (cont.)

- Finite `\(T\)`
    - Expected gain in exploration:
    `$$(T-t)\cdot\max \{R_2-r_1, 0\}$$`
    - For agent `\(t\)`, `\(θ_t\)` is the maximum value of `\(r_1\)` where exploration is optimal
    `$$(T-t)\cdot E\big[\max \{R_2 - θ_t, 0\}\big] =θ_t -μ_2$$`

--

&gt; *Theorem 1 (Finite `\(T\)`)*  
&gt; The optimal mechanism is defined by a sequence of thresholds
&gt; `$$i^{t,T} = \min\{i^{t,\infty}, θ_τ\}$$`
&gt; where `\(τ\)` is the minimal index where `\(i^{t,\infty}&gt;θ_t\)`

---

## Main Results (cont.)

&gt; *Theorem 2 (Arbitrary `\(T\)`)*  
&gt; Exploration is limited to a bounded number of agents `\(t^* = \min\{t\ |\ i^t = \infty\}\)`:
&gt; `$$t^*\leq\frac{μ_1 - μ_2}{α}$$`
&gt; where
&gt; `$$\begin{aligned}α &amp;= \int_{R_1\leq i^2,\ R_2&gt;R_1} (R_2-R_1)\, dπ \\&amp;\geq Pr(R_2\geq μ_2)\cdot Pr(R_1&lt;μ_2)\cdot\\&amp;\phantom{\ggg}\big( E\big[R_2\ |\ R_2\geq μ_2\big] - E\big[R_1\ |\ R_1&lt;μ_2\big]\big)\end{aligned}$$`

---

## Extensions

- `\(\to\)` Imperfect infomation about location
- Stochastic case
- Principal can make monetory transfers to incentivise agents to explore
- More than two alternatives

---

## Imperfect Infomation

- Agents: `\(t\in T\)`

 

- Groups of agents: `\(1=τ^1&lt;τ^2&lt;\dots&lt;τ^k=T+1\)`, `\(τ^j\)` are integers for all `\(j\)`
    - Group `\(i\)`: `\(t\in[τ^j, τ^{j+1})\)` for `\(j\leq k\)`
    - Agent `\(t=1\)` knows their exact position `\(t\)`
    - Agents `\(t\geq 2\)` only know the group `\(j\)` which they belongs to
    - When `\(k=T\)`, this case degenerate into the original case

 

- Threshold mechanism: ordered intervals `\(I^t\)`

--

&gt; *Lemma 7*  
&gt; For every group `\(j\)`:  
&gt; The first agent `\(τ^j\)` has threshold `\(I^{τ^j}=(i^{τ^{j-1}}, i^{τ^j}]\)`.  
&gt; All the rest agents `\(t\in(τ^j, τ^{j+1})\)` has thresholds `\(I^t=\emptyset\)`.

---

## Imperfect Infomation (cont.)

&gt; *Theorem 3 (Imperfect Infomation)*  
&gt; The optimal mechanism is defined by a sequence of thresholds `\(\{θ_i\}\)`  
&gt; The only agent who explores is the first agent in group `\(j\)` where `\(r_1\in(θ_{j-1}, θ_j]\)`.

　

--

&gt; *Theorem 4*  
&gt; As the information that agents have about their location becomes coarser, the policy that the planner can implement generates less welfare loss.



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
