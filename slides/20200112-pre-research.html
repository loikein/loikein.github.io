<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Implementing the “Wisdom of the Crowd”</title>
    <meta charset="utf-8" />
    <meta name="author" content="loikein" />
    <script src="slides-libs/jquery-1.11.3/jquery.min.js"></script>
    <script src="slides-libs/elevate-section-attrs-2.0/elevate-section-attrs.js"></script>
    <link href="slides-libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="slides-libs/remark-css-0.0.1/metropolis.css" rel="stylesheet" />
    <link href="slides-libs/remark-css-0.0.1/metropolis-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Implementing the “Wisdom of the Crowd”
## Kremer, Mansour, Perry (2014)
### loikein
### 2020/01/17

---


## Overview

&lt;!-- MarkdownTOC --&gt;

- [Motivation &amp; Background](#motivation--background)
- [Example Timeline](#example-timeline)
- [Model](#model)
- [Incentive Compatiblity](#incentive-compatiblity)
- [Mechanisms](#mechanisms)
- [Main Results](#main-results)
- [Extensions](#extensions)
- [Imperfect Infomation](#imperfect-infomation)

&lt;!-- /MarkdownTOC --&gt;

---

## Motivation &amp; Background

- Online reputation systems
    - Principle (platform) gives recommendation
    - Agents (users) give feedback

--

- Examples
    - TripAdvisor, Booking, etc
    - Waze, Moovit, etc
    - Doctors recommendation platforms

--

- Problems with full transparency
    - Principle relies on agents to explore (gather information)
    - Agents does not want to explore (freeride on the recommendation)
    - Things get recommended (restaurants, hotels, etc) may try to abuse the system

---

## Example Timeline

- (Full transparency)
    1. Each agent asks for recommendation
    1. The principle decides whether to let them explore or exploit
    1. But the agent sees all the previous results, and decides not to follow recommendation
    1. The principle does not obtain the desired information (!)

---

## Model

- Set of possible actions: `\(A=\{a_1,a_2\}\)`
- Unknown reward of `\(a_i\)`: `\(R_i\sim π_i\)`
    - Expected value: `\(μ_i= E_{R_i\sim π_i}\big[R_i\big]\)`, `\(μ_1\geq μ_2\)`

--

- Principle
    - Goal: aggregated social welfare `$$\max \ E\Big[\frac{1}{T}\textstyle{\sum}^T R^t\Big]$$`
    - Mechanism (message policy): `\(\tilde{M}^t: H^{t-1}\to M^t\)`

--

- Agents: `\(t\in T\)`
    - Goal: expected payoff `$$\max \ E\big[R^t\ |\ M^t\big]$$`
    - Strategy: `\(σ^t: M^t\to A\)`

---

## Incentive Compatiblity

- Incentive compatible action: for each `\(a_i\in A\)`, `$$E\big[R_i-R_{-i}\ |\ a^t = a_i\big]\geq 0$$`

--

- Incentive compatible mechanism: recommending `\(a_2\)` is IC if
`$$\begin{aligned}(IC) &amp;&amp; E\big[R_2-R_1\ |\ a^t=a_2\big]&amp;\geq 0 \\
&amp;&amp; Pr(a^t=a_2)\times E\big[R_2-R_1\big]&amp;\geq 0 \\
&amp;&amp; \int_{a^t=a_2} (R_2-R_1)\, dπ &amp;\geq 0\end{aligned}$$`

---

## Mechanisms

- Recommendation mechanism: recommending `\(a^t\in A\)` to each agent `\(t\)` that is IC

 

--

- Partition mechanism: for each agent `\(t\geq 2\)`, assigning set `\(I_t\)` that are __disjoint__
    - Since `\(μ_1&gt;μ_2\)`, agent `\(1\)` will always choose `\(a_1\)`
    - if `\(I^t=\emptyset\)`, agent `\(t\)` never explores
    - If `\(r_1\in I^t\)` for some `\(t\leq T\)`, agent `\(t\)` is the first agent to explore `\(a_2\)`
    - If `\(r_1\not\in I^t\)` for all `\(t\in[2,T]\)`, no agent explores

--

- Threshold mechanism: a partition mechanism where `\(I^t\)` are __ordered intervals__
    - For agent `\(t=2\)`, `\(I^2 = (-\infty,i^2]\)`
    - For agnet `\(t\geq 3\)`, `\(I^t=(i^{t-1}, i^t]\)`
    - If `\(i^{t-1}=i^t\)`, then `\(I^t=\emptyset\)`

--

&gt; *Lemma 6*  
&gt; The optimal recommendation mechanism is a threshold mechanism

---

## Main Results

- Infinite `\(T\)`: increasing sequence of thresholds `\(\{i^{t,\infty}\}\)`  
    - For agent `\(t=2\)`, `\(I^2 = (-\infty,i^{2,\ \infty}]\)`,  
    `\(i^{2,\ \infty}\)` is characterised by:
`$$\begin{aligned}\underbrace{\int_{R_1=-\infty}^{i^{2,\ \infty}}(R_1 - μ_2)\, dπ}_{\begin{subarray}{l}\text{Expected loss if recommended}\\
\text{to explore }a_2\end{subarray}} = 0\end{aligned}$$`
    - For agents `\(t&gt;3\)`, `\(I^t=(i^{t-1,\ \infty}, i^{t,\ \infty}]\)`, if `\(i^{t,\ \infty}&lt;\infty\)` and `\(i^{t+1,\ \infty}&lt;\infty\)`,  
    `\(i^{t+1,\ \infty}\)` is characterised by:
`$$\begin{aligned} \underbrace{\int_{R_1\leq i^t,\ R_2&gt;R_1}(R_2-R_1)\, dπ}_{\begin{subarray}{l}\text{Expected gain if recommended}\\
\text{to exploit }a_2\end{subarray}}
= \underbrace{\int_{R_1=i^{t,\ -\infty}}^{i^{t+1,\ \infty}}(R_1 - μ_2)\, dπ}_{\begin{subarray}{l}\text{Expected loss if recommended}\\
\text{to explore }a_2 \end{subarray}}\end{aligned}$$`

---

## Main Results (cont.)

- Finite `\(T\)`
    - Expected gain in exploration:
    `$$(T-t)\cdot\max \{R_2-r_1, 0\}$$`
    - For agent `\(t\)`, `\(θ_t\)` is the maximum value of `\(r_1\)` where exploration is optimal
    `$$(T-t)\cdot E\big[\max \{R_2 - θ_t, 0\}\big] =θ_t -μ_2$$`

--

&gt; *Theorem 1 (Finite `\(T\)`)*  
&gt; The optimal mechanism is defined by a sequence of thresholds
&gt; `$$i^{t,T} = \min\{i^{t,\infty}, θ_τ\}$$`
&gt; where `\(τ\)` is the minimal index where `\(i^{t,\infty}&gt;θ_t\)`

---

## Main Results (cont.)

&gt; *Theorem 2 (Arbitrary `\(T\)`)*  
&gt; Exploration is limited to a bounded number of agents `\(t^* = \min\{t\ |\ i^t = \infty\}\)`:
&gt; `$$t^*\leq\frac{μ_1 - μ_2}{α}$$`
&gt; where
&gt; `$$\begin{aligned}α &amp;= \int_{R_1\leq i^2,\ R_2&gt;R_1} (R_2-R_1)\, dπ \\&amp;\geq Pr(R_2\geq μ_2)\cdot Pr(R_1&lt;μ_2)\cdot\\&amp;\phantom{\ggg}\big( E\big[R_2\ |\ R_2\geq μ_2\big] - E\big[R_1\ |\ R_1&lt;μ_2\big]\big)\end{aligned}$$`

---

## Extensions

- `\(\to\)` Imperfect infomation about location
- Stochastic case
- Principle can make monetory transfers to incentivise agents to explore
- More than two alternatives

---

## Imperfect Infomation

- Agents: `\(t\in T\)`

 

- Groups of agents: `\(1=τ^1&lt;τ^2&lt;\dots&lt;τ^k=T+1\)`, `\(τ^j\)` are integers for all `\(j\)`
    - Group `\(i\)`: `\(t\in[τ^j, τ^{j+1})\)` for `\(j\leq k\)`
    - Agent `\(t=1\)` knows their exact position `\(t\)`
    - Agents `\(t\geq 2\)` only know the group `\(j\)` which they belongs to
    - When `\(k=T\)`, this case degenerate into the original case

 

- Threshold mechanism: ordered intervals `\(I^t\)`

--

&gt; *Lemma 7*  
&gt; For every group `\(j\)`:  
&gt; The first agent `\(τ^j\)` has threshold `\(I^{τ^j}=(i^{τ^{j-1}}, i^{τ^j}]\)`.  
&gt; All the rest agents `\(t\in(τ^j, τ^{j+1})\)` has thresholds `\(I^t=\emptyset\)`.

---

## Imperfect Infomation (cont.)

&gt; *Theorem 3 (Imperfect Infomation)*  
&gt; The optimal mechanism is defined by a sequence of thresholds `\(\{θ_i\}\)`  
&gt; The only agent who explores is the first agent in group `\(j\)` where `\(r_1\in(θ_{j-1}, θ_j]\)`.

　

--

&gt; *Theorem 4*  
&gt; As the information that agents have about their location becomes coarser, the policy that the planner can implement generates less welfare loss.



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
