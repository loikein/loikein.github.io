<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.52" />


<title>Notes on Econometrics - loikein</title>
<meta property="og:title" content="Notes on Econometrics - loikein">



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">
<link rel="stylesheet" href="/css/clumsy-toc.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/loikein-logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/">Posts</a></li>
    
    <li><a href="/tags/">Tags</a></li>
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/loikein">GitHub</a></li>
    
    <li><a href="https://twitter.com/LeiqiongWan">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">34 min read</span>
    

    <h1 class="article-title">Notes on Econometrics</h1>

    
    <span class="article-date">2019/01/05</span>
    
    
    
    <span class="tags">
    
    
    Tags:
    
    <a href='/tags/notes'>notes</a>
    
    <a href='/tags/r'>R</a>
    
    <a href='/tags/stat'>stat</a>
    
    
    
    </span>
    

    <div class="article-content">
      

<div id="TOC">
<ul>
<li><a href="#tests-statistics">Tests &amp; Statistics</a><ul>
<li><a href="#r2"><span class="math inline">\(R^2\)</span></a></li>
<li><a href="#t-test">t-test</a></li>
<li><a href="#f-test-joint-test">F-test (Joint Test)</a></li>
<li><a href="#wald-test">Wald Test</a></li>
<li><a href="#j-test">J Test</a></li>
<li><a href="#pe-test">PE Test</a></li>
<li><a href="#chow-breakpoint-test">Chow (Breakpoint) Test</a></li>
<li><a href="#sargan-test-overidentifying-restrictions-test">Sargan Test (Overidentifying Restrictions Test)</a></li>
<li><a href="#durbin-wu-hausman-test-endogeneity-test">Durbin-Wu-Hausman Test (Endogeneity Test)</a></li>
<li><a href="#read-regression-table">Read Regression Table</a></li>
</ul></li>
<li><a href="#gauss-markov-assumptions">Gauss-Markov Assumptions</a></li>
<li><a href="#ols-error">OLS Error</a><ul>
<li><a href="#normality-of-">Normality of <span class="math inline">\(ε\)</span></a></li>
<li><a href="#unbiasedness-of-s2">Unbiasedness of <span class="math inline">\(s^2\)</span></a></li>
<li><a href="#consistency-of-s2">Consistency of <span class="math inline">\(s^2\)</span></a></li>
</ul></li>
<li><a href="#ols-estimator">OLS Estimator</a><ul>
<li><a href="#derive-ols">Derive OLS</a></li>
<li><a href="#normality-of-b">Normality of <span class="math inline">\(b\)</span></a></li>
<li><a href="#unbiasedness-of-b">Unbiasedness of <span class="math inline">\(b\)</span></a></li>
<li><a href="#consistent-of-b-asymptotic">Consistent of <span class="math inline">\(b\)</span> (Asymptotic)</a></li>
<li><a href="#efficienty">Efficienty</a></li>
<li><a href="#blue-best-linear-unbiased-estimator">BLUE (Best Linear Unbiased Estimator)</a></li>
<li><a href="#can-consistent-and-asymptotically-normal">CAN (Consistent and Asymptotically Normal)</a></li>
</ul></li>
<li><a href="#ols-model">OLS Model</a><ul>
<li><a href="#converge-in-probability-mean-square">Converge in Probability &amp; Mean Square</a></li>
<li><a href="#lln-law-of-large-numbers">LLN (Law of Large Numbers)</a></li>
<li><a href="#elasticity">Elasticity</a></li>
</ul></li>
<li><a href="#iv-instrument-variable">IV (Instrument Variable)</a><ul>
<li><a href="#derive-iv">Derive IV</a></li>
<li><a href="#iv-estimator-hat_iv">IV Estimator <span class="math inline">\(\hat{β}_{IV}\)</span></a></li>
<li><a href="#give-generalized-iv-estimator">GIVE (Generalized IV Estimator)</a></li>
</ul></li>
<li><a href="#heteroskedasticity-autocorrelation">Heteroskedasticity &amp; Autocorrelation</a><ul>
<li><a href="#white-hetero-consistent-standard-error">White (Hetero-Consistent) Standard Error</a></li>
<li><a href="#hac-robust-standard-error">HAC (Robust) Standard Error</a></li>
</ul></li>
<li><a href="#maximum-likelihood-model">Maximum Likelihood Model</a></li>
<li><a href="#compare-models">Compare Models</a><ul>
<li><a href="#choose-explanatory-variables">Choose Explanatory Variables</a></li>
<li><a href="#choose-approach-ols-gls">Choose Approach: OLS / GLS</a></li>
<li><a href="#weird-models">Weird Models</a></li>
</ul></li>
<li><a href="#r-workout">R Workout</a><ul>
<li><a href="#data-generation">Data Generation</a></li>
<li><a href="#ols-regression">OLS Regression</a></li>
</ul></li>
</ul>
</div>

<p>Warning: under proofreading</p>
<p>All I wanted was a clear &amp; complete guidance (for upcoming exams).<br />
Textbook: <a href="http://93.174.95.27/book/index.php?md5=744048ECF4C4A865F45A5877AA7C2BD5">A Guide to Modern Econometrics</a></p>
<!--
## Matrix Algebra

> PS1.Q1.1  
> Consider the matrix:  
> $$M = \begin{pmatrix} \frac{1}{4} & \frac{\sqrt{3}}{4} \\ \frac{\sqrt{3}}{4} &  \frac{1}{4} \end{pmatrix}$$   
> Calculate the trace of $λM′M$ where $λ$ is a scalar.   
> Check whether $M$ is invertible and find the inverse if possible.

> PS1.Q1.2  
> Consider the matrix:  
> $$X = \begin{pmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{pmatrix}$$   
> and recall that $X′ X$ is equivalent to $\sum^3_{i=1} x_i x_i′$.   
> Define the vectors $x_1 , \dots  , x_3$.   
> Check that the notations are indeed equivalent.
-->
<div id="tests-statistics" class="section level2">
<h2>Tests &amp; Statistics</h2>
<div id="r2" class="section level3">
<h3><span class="math inline">\(R^2\)</span></h3>
<p>(textbook P21, slide 3 P4)<br />
<span class="math display">\[\begin{aligned}  &amp;&amp; &amp;R^2 = 1 - \frac{\sum^N_{i-1} e_i^2}{\sum^N_{i-1} (y_i-\bar{y})^2} \\
\text{adjusted:} &amp;&amp; &amp;\bar{R^2} = 1 - \frac{\frac{1}{N-K}\sum^N_{i-1} e_i^2}{\frac{1}{N-1}\sum^N_{i-1} (y_i-\bar{y})^2}\end{aligned}\]</span> ~</p>
<blockquote>
<p>PS2.Q1<br />
Consider two linear regression models:<br />
<span class="math display">\[\begin{aligned} y &amp;= X_1 b_{11} + e_1 \\
y &amp;= X_1 b_{21} + X_2 b_{22} + e_2\end{aligned}\]</span><br />
The solution to the minimization problem of the first model is given by <span class="math inline">\(b_{11}\)</span> and for the second model by <span class="math inline">\(b_{21}\)</span> for the regressors in <span class="math inline">\(X_1\)</span> and <span class="math inline">\(b_{22}\)</span> for the regressors in <span class="math inline">\(X_2\)</span>.<br />
<span class="math inline">\(e_1\)</span> and <span class="math inline">\(e_2\)</span> denote the residuals.<br />
Denote the <span class="math inline">\(R^2\)</span> for the first model by <span class="math inline">\(R^2_1\)</span> and for the second model by <span class="math inline">\(R^2_2\)</span>.<br />
Show that <span class="math inline">\(R^2_2 ≥ R^2_1\)</span> (<span class="math inline">\(R^2\)</span> always increases if we increase the number of regressors in models with intercept)</p>
</blockquote>
<p>According to the models,<br />
<span class="math display">\[\begin{aligned} R^2_1 &amp;= 1- \frac{e_1&#39;e_1}{\sum_{i=1}^N (y_i - \bar{y})^2} \\
R^2_2 &amp;= 1- \frac{e_2&#39;e_2}{\sum_{i=1}^N (y_i - \bar{y})^2}\end{aligned}\]</span><br />
Note for model 2, <span class="math inline">\(b_{21}\)</span> and <span class="math inline">\(b_{22}\)</span> are s.t.<br />
<span class="math display">\[S(\mathbf{\tilde{b_2}}) = \sum^N_{i=1} (y_i - x_i&#39; \mathbf{\tilde{b_2}})\]</span><br />
is minimized. Therefore,<br />
<span class="math display">\[\begin{aligned} \sum^N_{i=1} \big(y_i - x_i&#39; (b_{21},b_{22})&#39; \big) &amp;\leq \sum^N_{i=1} \big(y_i - x_i&#39; (b_{1}, 0)&#39; \big) \\
e_2&#39;e_2 &amp;\leq e_1&#39;e_1 \\ 
R^2_2 &amp;\geq R^2_1 \end{aligned}\]</span> ~</p>
</div>
<div id="t-test" class="section level3">
<h3>t-test</h3>
<p>(textbook P26, slide 3 P16)<br />
<span class="math display">\[t = \frac{\hat{β} - β_0}{se(\hat{β})}\]</span><br />
<span class="math inline">\(H_0: t ∼ t_{N-K} ∼^a \mathcal{N}(0,1)\)</span><br />
<span class="math inline">\(N\)</span> is the number of observations, <span class="math inline">\(K\)</span> is the number of <span class="math inline">\(β\)</span> 's.</p>
<blockquote>
<p>PS4.Q2.9<br />
T/F?<br />
When testing multiple restrictions, a t-test cannot be used.</p>
</blockquote>
<p>True.</p>
<blockquote>
<p>PS4.Q2.8<br />
T/F?<br />
When testing a single restriction, a t-test must be used as an F-test cannot be computed for <span class="math inline">\(J = 1\)</span>.</p>
</blockquote>
<p>False.<br />
When testing a single restriction, F-test is equivalent to t-test squared.<br />
Check this complete proof by <a href="https://jmcanovas.netlify.com/2018/10/29/when-does-the-f-test-reduce-to-t-test/">Juan Manuel</a>.</p>
</div>
<div id="f-test-joint-test" class="section level3">
<h3>F-test (Joint Test)</h3>
<p>(textbook P26, slide 3 P18)<br />
<span class="math display">\[F = \frac{\frac{1}{J} \cdot (R^2_1 - R^2_0)}{\frac{1}{N-K} \cdot (1-R^2_1)}\]</span><br />
<span class="math inline">\(H_0: F ∼ F_{N-K}^J\)</span><br />
<span class="math inline">\(N\)</span> is the number of observations, <span class="math inline">\(K\)</span> is the number of <span class="math inline">\(β\)</span> 's, and <span class="math inline">\(J\)</span> is the number of <span class="math inline">\(β\)</span> 's to be tested (<span class="math inline">\(= 0\)</span>), <span class="math inline">\(F \geq J + 1\)</span>.</p>
</div>
<div id="wald-test" class="section level3">
<h3>Wald Test</h3>
<p>(textbook P30, slide 3 P17)</p>
<ul>
<li><p>Wald statistic:
<span class="math display">\[ξ = \frac{1}{σ^2} (Rb - q)&#39; \big[R(X&#39; X)^{-1} R&#39; \big](Rb - q)\]</span><br />
<span class="math inline">\(H_0: ξ ∼ Χ^2_J\)</span><br />
<span class="math inline">\(K\)</span> is the number of <span class="math inline">\(β\)</span> 's, <span class="math inline">\(J\)</span> is the number of <span class="math inline">\(β\)</span> 's to be tested, <span class="math inline">\(R\)</span> is a <span class="math inline">\(J\times K\)</span> matrix, <span class="math inline">\(Rβ - q = 0\)</span>.</p></li>
<li>F-statistic:<br />
<span class="math display">\[F = \frac{1}{J \cdot s^2} (Rb - q)&#39; \big[R(X&#39; X)^{-1} R&#39; \big](Rb - q)\]</span><br />
<span class="math inline">\(H_0: F ∼ F^J_{N-K}\)</span><br />
</li>
<li><p>Note: Wald statistic and F-statistic are always two-sided</p></li>
</ul>
<blockquote>
<p>PS3.Q1.2<br />
For model <span class="math inline">\(\log{Y_t} = \log τ + β_1\log K_t + β_2\log L_t (+ ε_t)\)</span>, consider the regression output:<br />
<img src="/post-img/notes-stat1--PS3Q1.png" width="406" /><br />
<span class="math inline">\((X&#39; X)^{-1} = \begin{pmatrix} 5649.38 &amp;&amp; 307.02 &amp;&amp; \\ &amp;&amp; 16.85 &amp;&amp; -29.39 \\ -540.33 &amp;&amp; &amp;&amp; 51.68\end{pmatrix}\)</span><br />
3. Test the hypothesis of constant returns to scale at <span class="math inline">\(5\%\)</span> level.</p>
</blockquote>
<p>Constant return to scale <span class="math inline">\(\implies β_1 + β_2 = 1\)</span><br />
Let <span class="math inline">\(R = \begin{pmatrix} 0 &amp;&amp; 1 &amp;&amp; 1\end{pmatrix}\)</span>, <span class="math inline">\(β = \begin{pmatrix} \log τ \\ β_1 \\ β_2 \end{pmatrix}\)</span>, <span class="math inline">\(q = 1 \implies J = 1\)</span>.<br />
<span class="math inline">\(H_0\)</span>: <span class="math inline">\(Rβ = q\)</span><br />
<span class="math display">\[F = \frac{1}{J \cdot s^2} (Rb - q)&#39; \big[R(X&#39; X)^{-1} R&#39; \big](Rb - q)\]</span><br />
Since <span class="math inline">\(X&#39;X\)</span> is symmetric, <span class="math inline">\((X&#39; X)^{-1} = \begin{pmatrix} 5649.38 &amp;&amp; 307.02 &amp;&amp; -540.33 \\ 307.02 &amp;&amp; 16.85 &amp;&amp; -29.39 \\ -540.33 &amp;&amp; -29.39 &amp;&amp; 51.68\end{pmatrix}\)</span><br />
Calculate F-statistics:</p>
<pre class="r"><code>matrix_x &lt;- matrix(c(5649.38, 307.02, -540.33, 307.02, 16.85, -29.39, -540.33, -29.39, 51.68), nrow = 3, byrow = TRUE)
matrix_r &lt;- matrix(c(0,1,1), nrow = 1, byrow = TRUE)
matrix_b &lt;- matrix(c(-5.9395, 0.3593, 0.8045), nrow = 3)
scalar_q &lt;- 1

# parts

result_1 &lt;- 1 / 0.0381^2
result_2 &lt;-matrix_r %*% matrix_b - scalar_q
result_3 &lt;- matrix_r %*% matrix_x %*% t(matrix_r)

# calculate F-statistics

result_1 * t(result_2) * result_3^(-1) * result_2
##          [,1]
## [1,] 1.895716</code></pre>
<p>Calculate critical value at <span class="math inline">\(5\%\)</span> level, for one restriction and <span class="math inline">\(df = 40\)</span>:</p>
<pre class="r"><code>qf( 0.95, 1, 40)
## [1] 4.084746</code></pre>
<p>Therefore, <span class="math inline">\(H_0\)</span> is not rejected.</p>
</div>
<div id="j-test" class="section level3">
<h3>J Test</h3>
<p>(textbook P72, slide 5 P11)<br />
<span class="math display">\[\begin{aligned} \text{Model A: } y_i &amp;= x_i&#39; β + ε_i \\
\text{Model B: } y_i &amp;= z_i&#39; γ + v_i \\
\text{Test Model: } y_i &amp;= \underbrace{(1-δ)x_i&#39; β}_{\text{Model A}} + \underbrace{δ z_i&#39; γ }_{\text{Model B}} + u_i \\
&amp;=^{H_0} x_i&#39; + δ \hat{y}_{i,B} + u_i \end{aligned}\]</span><br />
<span class="math inline">\(H_0: \ t_{δ} = \frac{δ}{se(δ)} ∼ t\)</span></p>
</div>
<div id="pe-test" class="section level3">
<h3>PE Test</h3>
<p>(textbook P72, slide 5 P13)<br />
<span class="math display">\[\begin{aligned}\text{Linear Model: }&amp;\hat{y_i} = \dots \\ \text{Log Model: }&amp;\log\tilde{y_i} = \dots \end{aligned}\]</span> ~<br />
<span class="math display">\[\begin{aligned}\text{Test Models: }\phantom{\log} y_i &amp;= x_i&#39; β + δ_{Lin} (\log \underbrace{\hat{y_i}}_{\text{Linear Model}} - \underbrace{\log\tilde{y_i}}_{\text{Log Model}}) + u_i \\
\log y_i &amp;= (\log x_i)&#39; γ + δ_{Log} \big(\overbrace{\hat{y_i}} - \exp(\overbrace{\log\tilde{y_i}})\big) + u_i \end{aligned}\]</span> ~
<span class="math display">\[\begin{aligned}H_0\text{&#39;s: } &amp;t_{δ_{Lin}} ∼ t \implies\text{linear model is preferred} \\
&amp;t_{δ_{Log}} ∼ t \implies\text{log model is preferred}\end{aligned}\]</span><br />
If both <span class="math inline">\(H_0\)</span>’s are rejected, should consider more general models.</p>
<blockquote>
<p>PS4.Q2.5<br />
T/F?<br />
The PE test cannot be applied to compare a linear to an exponential specification.</p>
</blockquote>
<p>False.<br />
Just <span class="math inline">\(\log\)</span> both of them.</p>
</div>
<div id="chow-breakpoint-test" class="section level3">
<h3>Chow (Breakpoint) Test</h3>
<p>(textbook P75, slide 5 P17)<br />
<span class="math display">\[\begin{aligned}\text{Pooled Model: } y_i &amp;= x_i&#39; β + g_i x_i&#39; γ + ε_i \\ S_R &amp;= e&#39;e \\
\text{Split Models: } y_i &amp;=  x_i&#39;β_1 + ε_i &amp;&amp; i = 1, \dots, T^* \\
y_i &amp;=  x_i&#39;β_2 + ε_i &amp;&amp; i = T^*+1, \dots, n \\
S_{UR} &amp;= e_1&#39; e_1 + e_2&#39; e_2\end{aligned}\]</span><br />
<span class="math inline">\(H_0: \frac{\frac{1}{K} S_R - S_{UR}}{\frac{1}{N-2K}S_{UR}} ∼ F_{N-2K}^K\)</span><br />
<span class="math inline">\(N\)</span> is the number of observations, <span class="math inline">\(K\)</span> is the number of <span class="math inline">\(β\)</span> 's, and <span class="math inline">\(T^*\)</span> is the hypothetical breakpoint.<br />
Note: Chow test can also test two or more breakpoints / groups.</p>
<blockquote>
<p>PS4.Q2.6<br />
T/F?<br />
The Chow test cannot be applied in a cross-sectional regression.</p>
</blockquote>
<p>False.<br />
Chow test can also be used to detach groups in a cross-sectional regression.</p>
</div>
<div id="sargan-test-overidentifying-restrictions-test" class="section level3">
<h3>Sargan Test (Overidentifying Restrictions Test)</h3>
<p>(textbook P168, slide 9-2 P14)</p>
</div>
<div id="durbin-wu-hausman-test-endogeneity-test" class="section level3">
<h3>Durbin-Wu-Hausman Test (Endogeneity Test)</h3>
<p>(textbook P154, slide 9-2 P15)</p>
</div>
<div id="read-regression-table" class="section level3">
<h3>Read Regression Table</h3>
<blockquote>
<p>PS3.Q1.2<br />
For model <span class="math inline">\(\log{Y_t} = \log τ + β_1\log K_t + β_2\log L_t (+ ε_t)\)</span>, consider the regression output:<br />
<img src="/post-img/notes-stat1--PS3Q1.png" width="406" /><br />
1. Test the significance of the elasticities at <span class="math inline">\(5\%\)</span> level.</p>
</blockquote>
<p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(β_1 = 0\)</span>, <span class="math inline">\(β_2 = 0\)</span><br />
<span class="math inline">\(H_1\)</span>: <span class="math inline">\(β_1 &gt; 0\)</span>, <span class="math inline">\(β_2 &gt;0\)</span><br />
Calculate critical value at <span class="math inline">\(5\%\)</span> and <span class="math inline">\(1\%\)</span> level, <span class="math inline">\(df = 40\)</span>:</p>
<pre class="r"><code>qt( 0.95, 40 )
## [1] 1.683851
qt( 0.99, 40 )
## [1] 2.423257</code></pre>
<p>Therefore, <span class="math inline">\(K\)</span> is significant at <span class="math inline">\(5\%\)</span> level, and <span class="math inline">\(L\)</span> is significant at <span class="math inline">\(1\%\)</span> level.</p>
<blockquote>
<p>PS3.Q1.2<br />
For model <span class="math inline">\(\log{Y_t} = \log τ + β_1\log K_t + β_2\log L_t (+ ε_t)\)</span>, consider the regression output:<br />
<img src="/post-img/notes-stat1--PS3Q1.png" width="406" /><br />
2. Conduct a test of explanatory power (all coefficients are zero except the intercept).</p>
</blockquote>
<p><span class="math inline">\(H_0\)</span>: <span class="math inline">\(R^2_U = 0\)</span><br />
Calculate critical value at <span class="math inline">\(5\%\)</span> level, for two restrictions and <span class="math inline">\(df = 40\)</span>:</p>
<pre class="r"><code>qf( 0.95, 2, 40)
## [1] 3.231727</code></pre>
<p>As <span class="math inline">\(F = 1936 &gt;\)</span> 3.231727, <span class="math inline">\(H_0\)</span> is rejected.</p>
</div>
</div>
<div id="gauss-markov-assumptions" class="section level2">
<h2>Gauss-Markov Assumptions</h2>
<p>(A1) ~ (A4): textbook P15, slide 2 P23 = BLUE<br />
(A5): textbook P19, slide 2 P29 ~~~~~ = (A1) + (A3) + (A4) + normal<br />
(A6): textbook P34, slide 4 P10<br />
(A7): textbook P35, slide 4 P11 ~~~ <span class="math inline">\(\Longleftarrow\)</span> (A1), (A2) or (A8)<br />
(A8): textbook P37 ~~~~~~~~~~~~~ <span class="math inline">\(\Longleftarrow\)</span> (A2)</p>
<p>(A1): <span class="math inline">\(E[ε_i] = 0\)</span><br />
(A2): <span class="math inline">\(\{ε_1, \dots , ε_n\}\)</span> and <span class="math inline">\(\{x_1, \dots , x_N\}\)</span> are independent<br />
(A3): <span class="math inline">\(V(ε_i) = σ^2\)</span><br />
(A4): <span class="math inline">\(Cov(ε_i, ε_j) = 0\)</span> for all <span class="math inline">\(i\neq j\)</span></p>
<p>(A5): <span class="math inline">\(ε ∼ \mathcal{N} (0, σ^2I_N) \iff ε_i ∼ NID (0, σ^2)\)</span><br />
(A6): <span class="math inline">\(\frac{1}{N} \sum^N_{i=1} x_ix_i&#39; \to^p \exists Σ_{xx}\)</span><br />
(A7): <span class="math inline">\(E[ε_i x_i] = 0\)</span><br />
(A8): <span class="math inline">\(ε_i\)</span> and <span class="math inline">\(x_i\)</span> are independent for all <span class="math inline">\(i\)</span></p>
<blockquote>
<p>PS2.Q2<br />
Consider the linear regression model:<br />
<span class="math display">\[y_t = β x_t + ε_t\]</span><br />
where <span class="math inline">\(ε_t ∼ N(0,σ_ε^2)\)</span>, <span class="math inline">\(x_t ∼ N(0,σ_x^2)\)</span>, and <span class="math inline">\(ε_t\)</span> and <span class="math inline">\(x_t\)</span> are independent.<br />
1. What does the independence imply for <span class="math inline">\(E[x_t ε_t]\)</span>?</p>
</blockquote>
<p>Since <span class="math inline">\(ε_t\)</span> and <span class="math inline">\(x_t\)</span> are independent, they are not correlated
<span class="math display">\[\begin{aligned} Corr(x_t, ε_t) = \frac{Cov(x_t, ε_t)}{\sqrt{Var(x_t) Var(ε_t)}} &amp;= 0 \\
Cov(x_t ε_t) &amp;= 0 \\ &amp;= E[x_t ε_t] - E[x_t] E[ε_t] \\ &amp;= E[x_t ε_t]\end{aligned}\]</span><br />
<span class="math inline">\(\implies E[x_t ε_t] = 0\)</span></p>
</div>
<div id="ols-error" class="section level2">
<h2>OLS Error</h2>
<div id="normality-of-" class="section level3">
<h3>Normality of <span class="math inline">\(ε\)</span></h3>
</div>
<div id="unbiasedness-of-s2" class="section level3">
<h3>Unbiasedness of <span class="math inline">\(s^2\)</span></h3>
<blockquote>
<p>PS1.Q2<br />
Consider the linear regression model:<br />
<span class="math display">\[\begin{aligned} y_i =x^′_i β+ε_i &amp;&amp; i=1, \dots ,N\end{aligned}\]</span><br />
with <span class="math inline">\(εi ∼ NID(0,σ2)\)</span>.<br />
Denote by K the number of regressors including the intercept.<br />
Show that <span class="math inline">\(\tilde{s}^2 = \frac{1}{N-1} \sum^N_{i=1} e_i^2\)</span> is a biased estimator, and <span class="math inline">\(s^2 = \frac{1}{N-K} \sum^N_{i=1} e_i^2\)</span> an unbiased estimator for <span class="math inline">\(σ^2\)</span>.<br />
Hint: Treat <span class="math inline">\(X\)</span> as fixed or deterministic (or alternatively, think of working conditionally on outcomes <span class="math inline">\(X\)</span>).<br />
~~~~ <span class="math inline">\(tr(AB) = tr(BA)\)</span> and <span class="math inline">\(tr(A + B) = tr(A) + tr(B)\)</span>.</p>
</blockquote>
<p>Rewrite the model in matrix form:<br />
<span class="math display">\[\mathbb{y} = X β + ε\]</span><br />
Let <span class="math inline">\(P_X \equiv X(X&#39; X)^{-1} X&#39;\)</span>, <span class="math inline">\(M_X = I_N - P_X\)</span>,<br />
<span class="math display">\[\begin{aligned} M_X X &amp;= \big( I_N − X (X&#39; X)^{-1} X&#39;\big) \cdot X \\
&amp;= X − X (X&#39; X)^{-1} X&#39; X \\ &amp;= X − X \\ &amp;= 0\end{aligned}\]</span><br />
Therefore,<br />
<span class="math display">\[\begin{aligned} \mathbb{e} &amp;= M_X \cdot \mathbb{y} \\ &amp;= M_X (X β + ε) \\ &amp;= M_X Xβ + M_X ε \\ &amp;= M_X ε\end{aligned}\]</span><br />
Sum of squared error:<br />
<span class="math display">\[\begin{aligned} \sum^N_{i=1} e_i^2 &amp;= \mathbb{e}&#39;\mathbb{e} \\ &amp;= tr(\mathbb{e}&#39;\mathbb{e}) \\
&amp;= tr(ε&#39; M_X&#39; M_X ε) \\ &amp;= tr(ε&#39; M_X ε) \\ &amp;= tr(M_X ε ε&#39;)\end{aligned}\]</span><br />
Take expectation of both sides<br />
<span class="math display">\[\begin{aligned} \mathbb{E} [\mathbb{e}&#39;\mathbb{e}] &amp;= tr \big( M_X \mathbb{E} (ε ε&#39;) \big) \\
&amp;= σ^2 \cdot tr(M_X) \\ &amp;= σ^2 \cdot tr \big( I_N - X(X&#39; X)^{-1} X&#39; \big) \\
&amp;= σ^2 \cdot tr(I_N) - σ^2 \cdot tr \big( X(X&#39; X)^{-1} X&#39; \big) \\
&amp;= σ^2 \cdot tr(I_N) - σ^2 \cdot tr \big( X&#39; X(X&#39; X)^{-1} \big) \\
&amp;= σ^2 \cdot tr(I_N) - σ^2 \cdot tr(I_K) \\ &amp;= σ^2(N - K)\end{aligned}\]</span><br />
Therefore,<br />
<span class="math display">\[\begin{aligned} E[s^2] &amp;= \frac{1}{N-K} \cdot E \left[ \sum^N_{i=1} e_i^2 \right] \\
&amp;= \frac{1}{N-K} \cdot σ^2(N - K) \\ &amp;= σ^2\end{aligned}\]</span><br />
<span class="math inline">\(\implies s^2\)</span> is unbiased.<br />
And <span class="math inline">\(E[\tilde{s}^2] = \frac{N-K}{N-1} \cdot σ^2 \implies \tilde{s}^2\)</span> is biased for small <span class="math inline">\(N\)</span>’s.</p>
</div>
<div id="consistency-of-s2" class="section level3">
<h3>Consistency of <span class="math inline">\(s^2\)</span></h3>
<ul>
<li>The least square estimator <span class="math inline">\(s^2\)</span> for error variance <span class="math inline">\(σ^2\)</span> is consistent if: (textbook P35)<br />
<span class="math display">\[\begin{cases} V(ε_i) = σ^2 &amp;&amp; \text{(A3)} \\ \frac{1}{N} \sum^N_{i=1} x_ix_i&#39; \to^p \exists Σ_{xx} &amp;&amp; \text{(A6)} \\ E[ε_i x_i] = 0 &amp;&amp; \text{(A7)}  \end{cases}\]</span> ~</li>
</ul>
</div>
</div>
<div id="ols-estimator" class="section level2">
<h2>OLS Estimator</h2>
<div id="derive-ols" class="section level3">
<h3>Derive OLS</h3>
<blockquote>
<p>PS1.Q1.3<br />
Consider the linear model <span class="math inline">\(y = Xβ + ε\)</span>.<br />
The OLS estimator minimizes the sum of squared residuals:<br />
<span class="math display">\[S(\tilde{β}) = ε′ε = (y − X\tilde{β})&#39; (y − X\tilde{β})\]</span><br />
1. Derive the first order condition of <span class="math inline">\(S(\tilde{β})\)</span>.<br />
~~ Find the solution of the minimization problem and state all needed assumptions.<br />
2. Check whether the solution in (1) is a minimum. Do you need any further assumption to verify that?</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>Solve minimization problem<br />
<span class="math display">\[\begin{aligned} S(β) &amp;= (y − Xβ)&#39; (y − Xβ) \\
&amp;= (y&#39; - β&#39;X&#39;)(y − Xβ) \\ &amp;= y&#39;y - y&#39;Xβ -β&#39; X&#39; y + β&#39; X&#39; Xβ \\
&amp;= y&#39;y -2β&#39; X&#39; y + β&#39;X&#39;Xβ\end{aligned}\]</span><br />
FOC<br />
<span class="math display">\[\begin{aligned} \frac{\partial S(β)}{\partial β} = -2 X&#39; y + 2X&#39;X \hat{β} &amp;= 0 \\
X&#39;X \hat{β} &amp;= X&#39; y \\ \hat{β} &amp;= (X&#39;X)^{-1} X&#39; y\end{aligned}\]</span><br />
Assuming <span class="math inline">\(X&#39;X\)</span> is invertible <span class="math inline">\(\Longleftarrow X\)</span> is full-rank.<br />
</li>
<li>Check whether is minimum<br />
SOC<br />
<span class="math display">\[\frac{\partial ^2 S(β)}{\partial β \partial β&#39;} = 2X&#39; X\]</span><br />
<span class="math inline">\(\implies\)</span> the solution is a minimum if <span class="math inline">\(X&#39; X\)</span> is positive semi-definite.<br />
Take arbitrary <span class="math inline">\(a\neq 0\)</span>, let<br />
<span class="math display">\[\begin{aligned} q &amp;= a&#39; X&#39; Xa \\ &amp;= \sum^N_{i=0} x_i^2 a_i^2 \\ &amp;\geq 0\end{aligned}\]</span><br />
<span class="math inline">\(\implies\)</span> For all <span class="math inline">\(a\neq 0\)</span>, <span class="math inline">\(q &gt; 0\)</span>, <span class="math inline">\(X&#39; X\)</span> is positive semi-definite, no further assumptions needed.</li>
</ol>
<blockquote>
<p>PS2.Q2<br />
Consider the linear regression model:<br />
<span class="math display">\[y_t = β x_t + ε_t\]</span><br />
where <span class="math inline">\(ε_t ∼ N(0,σ_ε^2)\)</span>, <span class="math inline">\(x_t ∼ N(0,σ_x^2)\)</span>, and <span class="math inline">\(ε_t\)</span> and <span class="math inline">\(x_t\)</span> are independent.<br />
2. Verify that the OLS estimator can be written as<br />
<span class="math display">\[b = β + \left[ \sum_{t=1}^N x_t^2 \right] ^{-1} \sum_{t=1}^N x_t ε_t\]</span> ~</p>
</blockquote>
<p><span class="math display">\[\begin{aligned} b &amp;= (X′X)^{−1} X′ y \\
&amp;= β + (X&#39;X)^{-1} X&#39;ε \\
&amp;= β + \left[ \sum_{t=1}^N x_t^2 \right] ^{-1} \sum_{t=1}^N x_t ε_t \end{aligned}\]</span> ~</p>
</div>
<div id="normality-of-b" class="section level3">
<h3>Normality of <span class="math inline">\(b\)</span></h3>
<ul>
<li>OLS estimator is normally distributed with <span class="math inline">\(b ∼ \mathcal{N}\big(β, σ^2(X&#39;X)^{-1} \big)\)</span></li>
<li>Obtained if: (slide 2 P30)<br />
<span class="math display">\[\begin{cases} \{ε_1, \dots , ε_n\} \text{ and } \{x_1, \dots , x_N\} \text{ are independent} &amp;&amp; \text{(A2)} \\ε ∼ \mathcal{N} (0, σ^2I_N) &amp;&amp; \text{(A5)} \end{cases}\]</span> ~</li>
</ul>
<blockquote>
<p>PS4.Q2.2<br />
T/F?<br />
Under heteroskedastiticity and/or autocorrelation of the errors, the covariance matrix of the OLS estimator does not change in comparison to the case of homoscedasticity and absence of autocorrelation.</p>
</blockquote>
<p>False.<br />
Under heteroskedastiticity or autocorrelation,<br />
the covariance of the errors is different from benchmark case,<br />
thus the covariance matrix of OLS estimator also changes.</p>
<blockquote>
<p>PS2.Q2<br />
Consider the linear regression model:<br />
<span class="math display">\[y_t = β x_t + ε_t\]</span><br />
where <span class="math inline">\(ε_t ∼ N(0,σ_ε^2)\)</span>, <span class="math inline">\(x_t ∼ N(0,σ_x^2)\)</span>, and <span class="math inline">\(ε_t\)</span> and <span class="math inline">\(x_t\)</span> are independent.<br />
3. Use the LLN to find the appropriate scaling factor <span class="math inline">\(k\)</span> in <span class="math inline">\(\frac{1}{N^k} \sum^N_{t=1} x_t^2\)</span>.<br />
~~ What does <span class="math inline">\(\frac{1}{N^k} \sum^N_{t=1} x_t^2\)</span> converge to?<br />
Hint: Find <span class="math inline">\(E[x_t]\)</span> to characterize the object to which the scaled sum converges.</p>
</blockquote>
<p>LLN suggests <span class="math inline">\(k=1\)</span><br />
Since <span class="math inline">\(x_t ∼ N(0,σ_x^2)\)</span>, <span class="math inline">\(E[x_t] = 0\)</span>, <span class="math inline">\(Var(x_t) = σ_x^2\)</span><br />
<span class="math display">\[\begin{aligned} \frac{1}{N} \sum^N_{t=1} x_t^2 &amp;\to^p E[x_t^2] \\
&amp;= σ_x^2\end{aligned}\]</span> ~</p>
<blockquote>
<p>PS2.Q2<br />
Consider the linear regression model:<br />
<span class="math display">\[y_t = β x_t + ε_t\]</span><br />
where <span class="math inline">\(ε_t ∼ N(0,σ_ε^2)\)</span>, <span class="math inline">\(x_t ∼ N(0,σ_x^2)\)</span>, and <span class="math inline">\(ε_t\)</span> and <span class="math inline">\(x_t\)</span> are independent.<br />
4. To find the appropriate scaling factor <span class="math inline">\(k\)</span> in <span class="math inline">\(N^{−k} \sum^N_{t=1} x_t ε_t\)</span>, first find the variance of <span class="math inline">\(N^{−k} \sum^N_{t=1} x_t ε_t\)</span>.<br />
For which unique choice of <span class="math inline">\(k\)</span> does this variance neither approach zero, nor explode?<br />
Hence, find the value for <span class="math inline">\(k\)</span> which stabilizes the variance.</p>
</blockquote>
<p><span class="math display">\[\begin{aligned}Var\left( N^{−k} \textstyle{\sum}^N_{t=1} x_t ε_t \right) &amp;= (N^{-k})^2 \cdot Var\left( \textstyle{\sum}^N_{i=1} x_t ε_t \right) \\
&amp;= N^{-2k} \cdot Var\left( \textstyle{\sum}^N_{i=1} x_t ε_t \right)\\
Var\left( \textstyle{\sum}^N_{i=1} x_t ε_t \right) &amp;= Var(x_1 ε_1 + x_2 ε_2 + \cdots +x_N ε_N) \\
&amp;= \textstyle{\sum}^N_{i=1} Var(x_t ε_t ) \\
&amp;= \textstyle{\sum}^N_{i=1} \left[ Var(x_t) \cdot Var( ε_t ) \right] \\
&amp;= \textstyle{\sum}^N_{i=1} σ^2_x σ^2_ε  \\
&amp;= N \cdot σ^2_x σ^2_ε \\
Var\left( N^{−k} \textstyle{\sum}^N_{t=1} x_t ε_t \right) &amp;= N^{-2k} \cdot N \cdot σ^2_x σ^2_ε \\
&amp;= N^{1-2k} \cdot σ^2_x σ^2_ε\end{aligned}\]</span><br />
Set <span class="math inline">\(1 - 2k = 1\)</span>, <span class="math inline">\(k = \frac{1}{2}\)</span><br />
<span class="math display">\[\frac{1}{\sqrt{N}}\sum^N_{t=1} x_t ε_t \to^p σ^2_x σ^2_ε\]</span> ~</p>
<blockquote>
<p>PS2.Q2<br />
Consider the linear regression model:<br />
<span class="math display">\[y_t = β x_t + ε_t\]</span><br />
where <span class="math inline">\(ε_t ∼ N(0,σ_ε^2)\)</span>, <span class="math inline">\(x_t ∼ N(0,σ_x^2)\)</span>, and <span class="math inline">\(ε_t\)</span> and <span class="math inline">\(x_t\)</span> are independent.<br />
5. Consider the difference <span class="math inline">\(b − β\)</span>.<br />
Combine (3) and (4) to find the implied appropriate scaling (i.e. the convergence rate) of <span class="math inline">\(b − β\)</span>.</p>
</blockquote>
<p>We know<br />
<span class="math display">\[b-β = \left(\sum^N_{i=1} x_t^2 \right)^{-1} \sum^N_{i=1}x_t ε_t\]</span><br />
From (3) and (4):<br />
<span class="math display">\[\begin{aligned} \frac{1}{N} \sum^N_{t=1} x_t^2 &amp;\to^p σ_x^2 \\
\frac{1}{\sqrt{N}}\sum^N_{t=1} x_t ε_t &amp;\to^p σ_x^2 σ_ε^2 \end{aligned}\]</span><br />
Assume <span class="math inline">\(N^k(b-β)\)</span> converges in distribution, therefore<br />
<span class="math display">\[\begin{aligned} N^k(b-β) &amp;= N^k \cdot \left(\sum^N_{i=1} x_t^2 \right)^{-1} \sum^N_{i=1} x_t ε_t \\
&amp;= \left(\frac{1}{N}\sum^N_{i=1} x_t^2 \right)^{-1} \frac{1}{\sqrt{N}}\sum^N_{i=1} x_t ε_t \\
N^k &amp;= N \cdot \frac{1}{\sqrt{N}} \\
k &amp;= \frac{1}{2}\end{aligned}\]</span><br />
Therefore <span class="math inline">\(\sqrt{N}\)</span> is the implied convergence rate of <span class="math inline">\(b − β\)</span>.<br />
<span class="math display">\[\sqrt{N}(b − β) \to^d \mathcal{N} \big(0, (σ_x^2 σ_ε^2)(σ_x^2)^{-1} \big)\]</span><br />
Note that under standard notation, <span class="math inline">\(σ^2 \equiv σ_x^2 σ_ε^2\)</span>, <span class="math inline">\(Σ_{xx} \equiv σ_x^2\)</span><br />
<span class="math inline">\(\implies \sqrt{N}(b − β) \to^d \mathcal{N} (0, σ^2 Σ_{xx}^{-1})\)</span></p>
</div>
<div id="unbiasedness-of-b" class="section level3">
<h3>Unbiasedness of <span class="math inline">\(b\)</span></h3>
<ul>
<li>OLS estimator is unbiased when <span class="math inline">\(E(b) = β\)</span></li>
<li>Obtained if: (textbook P16, slide 2 P24)<br />
<span class="math display">\[\begin{cases} E[ε_i] = 0 &amp;&amp; \text{(A1)} \\ \{ε_1, \dots , ε_n\} \text{ and } \{x_1, \dots , x_N\} \text{ are independent} &amp;&amp; \text{(A2)}\end{cases}\]</span> ~</li>
</ul>
<blockquote>
<p>PS4.Q2.1<br />
T/F?<br />
Under heteroskedastiticity and/or autocorrelation of the errors, the OLS estimator is still unbiased.</p>
</blockquote>
<p>True.<br />
Unbiasedness is only conditional on the average of errors and independence between error and <span class="math inline">\(x\)</span>'s.</p>
</div>
<div id="consistent-of-b-asymptotic" class="section level3">
<h3>Consistent of <span class="math inline">\(b\)</span> (Asymptotic)</h3>
<ul>
<li>See the whole section: <a href="#ols-model">#OLS Model</a></li>
<li>OLS estimator is consistent when <span class="math inline">\(\lim P\{ |b_k - β_k| &gt; δ \} = 0​\)</span> for all <span class="math inline">\(δ &gt; 0​\)</span></li>
<li>Obtained if: (textbook P35)<br />
<span class="math display">\[\begin{cases} \frac{1}{N} \sum^N_{i=1} x_ix_i&#39; \to^p \exists Σ_{xx} &amp;&amp; \text{(A6)} \\ E[ε_i x_i] = 0 &amp;&amp; \text{(A7)}  \end{cases}\]</span> ~</li>
</ul>
<blockquote>
<p>PS5.Q4<br />
Consider the model with a latent (unobserved) variable <span class="math inline">\(y_i^*\)</span>:<br />
<span class="math display">\[y_i^∗ = x&#39;_i β + ε_i\]</span><br />
where <span class="math inline">\(ε_i ∼^{iid} N(0,σ^2_ε)\)</span> and is independent of <span class="math inline">\(x_i\)</span>, and <span class="math inline">\(x_i ∼^{iid} N(μ,σ^2_x)\)</span>.<br />
The dependent variable is observed with measurement error, i.e., <span class="math inline">\(y_i = y_i^∗ + v_i\)</span>,<br />
where <span class="math inline">\(v_i ∼^{iid} N(0,σ^2_v)\)</span> is independent of both <span class="math inline">\(x_j\)</span> and <span class="math inline">\(ε_j ∀j\)</span>.<br />
(The unobserved <span class="math inline">\(y_i\)</span> is sometimes called a latent variable.)<br />
Regress <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span> to obtain an OLS estimate <span class="math inline">\(\tilde{β}\)</span>.<br />
1. Is <span class="math inline">\(\tilde{β}\)</span> a consistent estimator of <span class="math inline">\(β\)</span> ?</p>
</blockquote>
<p>Rewrite the model:<br />
<span class="math display">\[\begin{aligned} y_i - v_i &amp;= x_i&#39; β + ε_i \\ y_i &amp;= x_i&#39; β + ε_i + v_i\end{aligned}\]</span><br />
where <span class="math inline">\(ε_i\)</span> and <span class="math inline">\(v_i\)</span> are uncorrelated.<br />
Let <span class="math inline">\(ε_i + v_i = u_i \implies u_i ∼^{iid}N(0, σ^2_ε + σ^2_v)\)</span><br />
OLS estimator can be written as:<br />
<span class="math display">\[\begin{aligned} \tilde{β} &amp;= \left(\frac{1}{N}\sum^N_{i=1} x_i x_i&#39; \right) ^{-1} \frac{1}{N} \sum^N_{i=1} x_i y_i \\
&amp;= β + \left(\frac{1}{N}\sum^N_{i=1} x_i x_i&#39; \right) ^{-1} \frac{1}{N}\sum^N_{i=1} x_i u_i \\
\tilde{β} - β &amp;= \left(\frac{1}{N}\sum^N_{i=1} x_i x_i&#39; \right) ^{-1} \frac{1}{N}\sum^N_{i=1} x_i u_i \end{aligned}\]</span><br />
Apply LLN:
<span class="math display">\[\begin{aligned} \tilde{β} - β &amp;\to^p \text{plim}(\tilde{β} - β) \\
&amp;= \frac{\text{plim}\left(\frac{1}{N}\sum^N_{i=1} x_i u_i \right)}{\text{plim}\left(\frac{1}{N}\sum^N_{i=1} x_i x_i&#39; \right)} \\
&amp;= \frac{E[x_i u_i]}{Σ_{xx}}\end{aligned}\]</span></p>
<p>where <span class="math inline">\(x&#39;x \to Σ_{xx}\)</span> (A6).<br />
As long as <span class="math inline">\(E[x_i u_i] = 0\)</span> (A7), <span class="math inline">\(\tilde{β}\)</span> is consistent.</p>
<blockquote>
<p>PS5.Q4<br />
Consider the model<br />
<span class="math display">\[y^∗_i = x&#39;_i β + ε_i\]</span><br />
2. What happens to the consistency of <span class="math inline">\(\tilde{β}\)</span> if <span class="math inline">\(ε_i\)</span> and <span class="math inline">\(v_i\)</span> are correlated?</p>
</blockquote>
<p>Variance of <span class="math inline">\(u_i\)</span> changes as follows:<br />
<span class="math display">\[Var(u_i) = σ^2_ε + ε^2_v + 2Cov(ε_i, v_i)\]</span><br />
<span class="math inline">\(\implies u_i ∼^{iid}N \left( 0, σ^2_ε + ε^2_v + 2Cov(ε_i, v_i) \right)\)</span><br />
While everything remaining is unchanged, therefore the consistency of <span class="math inline">\(\tilde{β}\)</span>.</p>
</div>
<div id="efficienty" class="section level3">
<h3>Efficienty</h3>
<blockquote>
<p>PS1.Q4<br />
Assume that all Gauss-Markov assumptions are valid, and treat the regressor matrix as fixed.<br />
Consider the linear and unbiased OLS estimator:<br />
<span class="math display">\[b = (X′X)^{−1} X′y = Cy\]</span><br />
And another linear and unbiased estimator:<br />
<span class="math display">\[\tilde{b} = Wy, \ W = W(X)\]</span><br />
1. Verify that <span class="math inline">\(W X = I\)</span> holds due to unbiasedness (similar to <span class="math inline">\(CX = I\)</span>).</p>
</blockquote>
<p>Take expectation<br />
<span class="math display">\[\begin{aligned} E[\tilde{b}] &amp;= E[Wy] \\ &amp;= E\big[ W(Xβ + ε)\big] \\
&amp;= WXβ + W \cdot E[ε] \\ &amp;= WXβ\end{aligned}\]</span><br />
Since <span class="math inline">\(\tilde{b}\)</span> is unbiased,<br />
<span class="math display">\[\begin{aligned} E[\tilde{b}] = WXβ &amp;= β \\ WX &amp;= I\end{aligned}\]</span> ~</p>
<blockquote>
<p>PS1.Q4<br />
Consider the linear and unbiased OLS estimator:<br />
<span class="math display">\[b = (X′X)^{−1} X′y = Cy\]</span><br />
And another linear and unbiased estimator:<br />
<span class="math display">\[\tilde{b} = Wy, \ W = W(X)\]</span><br />
2. Find the covariance matrix of <span class="math inline">\(\tilde{b}\)</span>, i.e. <span class="math inline">\(V(\tilde{b})\)</span>.</p>
</blockquote>
<p><span class="math display">\[\begin{aligned} V(\tilde{b}) &amp;= E \big[ (\tilde{b} - β)(\tilde{b} - β)&#39; \big] \\
&amp;= E \big[ (WXβ + Wε - β)(WXβ + Wε - β)&#39; \big] \\ &amp;= E \big[ (Iβ + Wε - β)(Iβ + Wε - β)&#39; \big] \\ &amp;= E[ Wε ε&#39; W&#39;] \\
&amp;= W \cdot E[ε ε&#39;] \cdot W&#39; \\ &amp;= σ^2 WW&#39; \end{aligned}\]</span> ~</p>
<blockquote>
<p>PS1.Q4<br />
Consider the linear and unbiased OLS estimator:<br />
<span class="math display">\[b = (X′X)^{−1} X′y = Cy\]</span><br />
And another linear and unbiased estimator:<br />
<span class="math display">\[\tilde{b} = Wy, \ W = W(X)\]</span><br />
3. Compare the covariance matrices of <span class="math inline">\(\tilde{b}\)</span> and <span class="math inline">\(b\)</span> by building their difference <span class="math inline">\(V (\tilde{b}) − V (b)\)</span>.<br />
~~ Show in the final step that the difference between the covariance matrices is positive semi-definite.</p>
</blockquote>
<p><span class="math display">\[\begin{aligned} V (\tilde{b}) − V (b) &amp;= σ^2 WW&#39; - σ^2 (X&#39;X)^{-1} \\
&amp;= σ^2 \big[ WW&#39; - (X&#39;X)^{-1} \big] \\
&amp;= σ^2 \big[ WW&#39; - WX \cdot (X&#39;X)^{-1}\cdot (WX)&#39; \big] \\
&amp;= σ^2 \big[ WW&#39; - WX \cdot (X&#39;X)^{-1} \cdot X&#39;W&#39; \big] \\
&amp;= σ^2 \left\{ W \big[ I - X(X&#39;X)^{-1}X&#39; \big] W&#39; \right\} \\
&amp;= σ^2 W M_X W&#39;\end{aligned}\]</span><br />
Take arbitrary <span class="math inline">\(a \neq 0\)</span><br />
<span class="math display">\[\begin{aligned} a&#39; (σ^2 W M_X W&#39;) a &amp;= σ^2 a&#39; W M_X W&#39; a \\
&amp;= σ^2 a&#39; W M_X M_X W&#39; a \\
&amp;= σ^2 (M_X W&#39; a)&#39; (M_X W&#39; a) \\ &amp;\geq 0\end{aligned}\]</span><br />
Therefore, the difference <span class="math inline">\(V (\tilde{b}) − V (b)\)</span> is positive semi-definite.<br />
<span class="math inline">\(\implies\)</span> Any unbiased linear estimator cannot offer a variance smaller than the OLS estimator.</p>
</div>
<div id="blue-best-linear-unbiased-estimator" class="section level3">
<h3>BLUE (Best Linear Unbiased Estimator)</h3>
<ul>
<li>OLS estimator is BLUE when <span class="math inline">\(V(\tilde{b}) \geq V(b) \iff V(\tilde{b}) - V(b)\)</span> is positive semi-definite</li>
<li>Obtained if (A1) ~ (A4) are all satisfied (textbook P17, slide 2 P26)</li>
</ul>
</div>
<div id="can-consistent-and-asymptotically-normal" class="section level3">
<h3>CAN (Consistent and Asymptotically Normal)</h3>
<ul>
<li>OLS estimator is CAN when <span class="math inline">\(\sqrt{N} (b-β) \to^a \mathcal{N}(0, σ^2 Σ_{xx}^{-1})\)</span></li>
<li>Obtained if (A1) ~ (A4) + (A6) are satisfied (textbook P36, slide 4 P13)</li>
<li><span class="math inline">\(\sqrt{N}\)</span> is called the rate of convergence</li>
</ul>
</div>
</div>
<div id="ols-model" class="section level2">
<h2>OLS Model</h2>
<div id="converge-in-probability-mean-square" class="section level3">
<h3>Converge in Probability &amp; Mean Square</h3>
<ul>
<li>Vector sequence <span class="math inline">\(\{x_n\}\)</span> converges in p when <span class="math inline">\(\mathbb{P}\{ \|x_n - x\| &gt; \exists δ\} \to 0\)</span> as <span class="math inline">\(N \to \infty\)</span></li>
<li>Also: <span class="math inline">\(x_n \to^p x\)</span> or <span class="math inline">\(\text{plim} x_n = x\)</span></li>
<li><span class="math inline">\(x_n \to^p x \implies g(x_n) \to^p g(x)\)</span> for all continuous function <span class="math inline">\(g\)</span></li>
</ul>
<p>~</p>
<ul>
<li>Scalar sequence <span class="math inline">\(\{x_n\}\)</span> converges in ms when <span class="math inline">\(E\big[ (x_n - x)^2 \big] \to 0\)</span> as <span class="math inline">\(N \to \infty\)</span></li>
<li>Also: <span class="math inline">\(x_n \to^{ms} x\)</span></li>
<li><span class="math inline">\(x_n \to^{ms} x \implies x_n \to^p x\)</span></li>
<li><span class="math inline">\(x_n \to^{ms} α \implies E[x_n] \to α\)</span> and <span class="math inline">\(Var(x_n) \to 0\)</span></li>
</ul>
</div>
<div id="lln-law-of-large-numbers" class="section level3">
<h3>LLN (Law of Large Numbers)</h3>
<ul>
<li><span class="math inline">\(\frac{1}{N}\sum^N_{i=1} x_n \to^p E[x]\)</span> as <span class="math inline">\(N \to \infty\)</span>, if <span class="math inline">\(\{x_n\}\)</span> are <span class="math inline">\(i.i.d\)</span> copies of <span class="math inline">\(\mathbb{x}\)</span></li>
<li>(A6) + (A7) + LLN = <a href="#consistent-of-b-asymptotic">Asymptotic Consistency of OLS estimator</a></li>
</ul>
<p>Under LLN:<br />
<span class="math display">\[\begin{aligned}\frac{1}{N} \sum^N_{i=1} x_i x_i&#39; \to^p Σ_{xx}^{-1} &amp;&amp; \text{(A6)}\end{aligned}\]</span><br />
and<br />
<span class="math display">\[\begin{aligned} \frac{1}{N}\sum x_i ε_i &amp;\to^p E[x_i ε_i] \\
b - β = \left( \frac{1}{N} \sum x_i x_i’ \right)^{-1} \left( \frac{1}{N} \sum x_i ε_i \right) &amp;\to^p Σ_{xx}^{-1} \cdot E[x_i ε_i] \end{aligned}\]</span><br />
If <span class="math inline">\(E[x_i ε_i]\)</span> (A7), then <span class="math inline">\(b - β \to 0\)</span></p>
<blockquote>
<p>PS4.Q2.7<br />
T/F?<br />
The LLN states that sample moments converge in probability to their population counterparts.</p>
</blockquote>
<p>True.<br />
According to LLN:<br />
<span class="math display">\[\begin{aligned} \frac{1}{N}\sum x_i ε_i &amp;\to^p E[x_i ε_i] \\
\implies \frac{1}{N}\sum y_i^2 &amp;\to^p E[y_i^2]\end{aligned}\]</span><br />
~</p>
</div>
<div id="elasticity" class="section level3">
<h3>Elasticity</h3>
<ul>
<li>Elasticity measures the relative change in the dependent variable owing to a relative change in one of the <span class="math inline">\(x_i\)</span> variables.<br />
</li>
<li>For <span class="math inline">\(\log y_i = (\log x_i)&#39; γ + v_i\)</span>, elasticity:<br />
<span class="math display">\[\begin{aligned} \frac{\partial E[y_i|x_i] / E[y_i|x_i]}{\partial x_{ik} / x_{ik}} &amp;= \frac{\partial E[y_i|x_i]}{\partial x_{ik}} \cdot \frac{\partial x_{ik}}{E[y_i|x_i]} \\
&amp;\approx \frac{\partial E[\log y_i | \log x_i]}{\partial \log x_{ik}} \\ &amp;= γ_k \end{aligned}\]</span><br />
</li>
<li>For <span class="math inline">\(y_i = x_i&#39; β + ε_i\)</span>, elasticity:<br />
<span class="math display">\[\begin{aligned} \frac{\partial E[y_i|x_i] / E[y_i|x_i]}{\partial x_{ik} / x_{ik}} &amp;= \frac{\partial E[y_i|x_i]}{\partial x_{ik}} \cdot \frac{\partial x_{ik}}{E[y_i|x_i]} \\
&amp;= β_k \cdot \frac{x_{ik}}{x_i&#39; β} \end{aligned}\]</span> ~</li>
</ul>
<blockquote>
<p>PS3.Q1.1<br />
Consider the Cobb-Douglas production function:<br />
<span class="math display">\[Y =F(K,L)=τK^{β_1}L^{β_2}\]</span><br />
where <span class="math inline">\(F\)</span> is the production function, <span class="math inline">\(K\)</span> capital and <span class="math inline">\(L\)</span> labor.<br />
Technology is assumed to be constant and represented by <span class="math inline">\(τ\)</span>.<br />
1. Which coefficients are elasticities?</p>
</blockquote>
<p>For <span class="math inline">\(K\)</span>:<br />
<span class="math display">\[\begin{aligned} \frac{\partial Y}{\partial K}\cdot\frac{K}{Y} &amp;= τβ_1K^{β_1-1}L^{β_2} \cdot \frac{K}{Y} \\
&amp;= \frac{τβ_1K^{β_1}L^{β_2}}{τK^{β_1}L^{β_2}} \\
&amp;= β_1\end{aligned}\]</span><br />
For <span class="math inline">\(L\)</span>:<br />
<span class="math display">\[\begin{aligned} \frac{\partial Y}{\partial L}\cdot\frac{L}{Y} &amp;= τβ_2K^{β_1}L^{β_2-1} \cdot \frac{K}{Y} \\
&amp;= \frac{τβ_2K^{β_1}L^{β_2}}{τK^{β_1}L^{β_2}} \\
&amp;= β_2\end{aligned}\]</span><br />
Therefore, <span class="math inline">\(β_1\)</span> and <span class="math inline">\(β_2\)</span> are the elasticities.</p>
<blockquote>
<p>PS3.Q1.1<br />
2. Specify the econometric model you think is appropriate to estimate the coefficients of the production function.</p>
</blockquote>
<p>Take log:<br />
<span class="math display">\[\log{Y_t} = \log τ + β_1\log K_t + β_2\log L_t (+ ε_t)\]</span> ~</p>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
<img src="/post-img/notes-stat1--PS2Q3-6.png" width="393" /><br />
6. Find the estimated elasticities for the effect of experience for males and females.<br />
At which conventional level is the gender effect of experience significant?<br />
Is the gender effect (measured by the male dummy coefficient) significant at <span class="math inline">\(10 \%\)</span> or <span class="math inline">\(5 \%\)</span> level when using a one-sided alternative (namely, positive coefficient)?</p>
</blockquote>
<p>Elasticity for female: <span class="math inline">\(0.207\)</span><br />
Elasticity for male: <span class="math inline">\(0.207 + 0.041 = 0.248\)</span><br />
Gender effect: <span class="math inline">\(0.041\)</span><br />
t-ratio: <span class="math inline">\(1.891\)</span></p>
<pre class="r"><code># calculate critical value
qnorm( 0.90 )
## [1] 1.281552
qnorm( 0.95 )
## [1] 1.644854</code></pre>
<p>Therefore, <span class="math inline">\(H_0\)</span> is rejected at <span class="math inline">\(10 \%\)</span> level, but not rejected at <span class="math inline">\(5 \%\)</span> level.</p>
</div>
</div>
<div id="iv-instrument-variable" class="section level2">
<h2>IV (Instrument Variable)</h2>
<div id="derive-iv" class="section level3">
<h3>Derive IV</h3>
<p>(textbook P150, slide 9-1)<br />
Consider model<br />
<span class="math display">\[y_i = x_{1i}&#39; β_1 + x_{2i} β_2 + ε_i\]</span><br />
Moment (expectation) conditions:<br />
<span class="math display">\[\begin{aligned} E[ε_i x_{1i}] &amp;= E\left[ (y_i - x_{1i}&#39; β_1 + x_{2i} β_2) x_{1i}\right] = 0 &amp;&amp; \text{(1)}\\
E[ε_i x_{2i}] &amp;= E\left[ (y_i - x_{1i}&#39; β_1 + x_{2i} β_2) x_{2i}\right] = 0 &amp;&amp; \text{(2)}\end{aligned}\]</span><br />
If <span class="math inline">\(E[ε_i x_{2i}] \neq 0\)</span>, OLS gives inconsistent estimator of <span class="math inline">\(β_2\)</span>, need to find instrument variable <span class="math inline">\(z_{2i}\)</span> s.t. (exogeneity)<br />
<span class="math display">\[\begin{aligned} E[ε_i z_{2i}] = E\left[ (y_i - x_{1i}&#39; β_1 + x_{2i} β_2) z_{2i}\right] = 0  &amp;&amp; \text{(3)} \end{aligned}\]</span><br />
Solving (1) and (3) gives consistent estimator <span class="math inline">\(\hat{β}_{IV}\)</span>.</p>
<p>Test relavance of <span class="math inline">\(z_{2i}\)</span>:<br />
<span class="math display">\[\text{Auxiliary Regression: }x_{2i} = x_{1i}&#39; π_1 + z_{2i} π_2 + v_i\]</span><br />
should yield <span class="math inline">\(π_2 \neq 0\)</span>.</p>
</div>
<div id="iv-estimator-hat_iv" class="section level3">
<h3>IV Estimator <span class="math inline">\(\hat{β}_{IV}\)</span></h3>
<blockquote>
<p>PS5.Q1<br />
Show that <span class="math inline">\(\hat{β}_{IV} =(X&#39;ZW_N Z′X)^{−1}X&#39;ZW_N Z&#39;y\)</span> minimizes<br />
<span class="math display">\[\begin{aligned} Q_N(β) &amp;= \left[ \frac{1}{N}\sum_{i=1}^N (y_i - x_i&#39; β)z_i \right]&#39; W_N \left[ \frac{1}{N}\sum_{i=1}^N (y_i - x_i&#39; β)z_i \right] \\
&amp;= \left[ \frac{1}{N}Z&#39; (y - X β) \right]&#39; W_N \left[ \frac{1}{N}Z&#39; (y - X β) \right] \end{aligned}\]</span> ~</p>
</blockquote>
<p><span class="math display">\[\begin{aligned} Q_N(β) &amp;=  \frac{1}{N^2}\left[ (y&#39; - β&#39;X&#39;)Z W_N (Z&#39;y - Z&#39;X β) \right]
\\ &amp;= \frac{1}{N^2}\left[ y&#39;ZW_NZ&#39;y - y&#39;ZW_N Z&#39;Xβ - β&#39;X&#39;Z W_N Z&#39;y + β&#39;X&#39; Z W_N Z&#39;yβ \right] \\
&amp;= \frac{1}{N^2}\left[ y&#39;ZW_NZ&#39;y - 2\left( β&#39;X&#39;Z W_N Z&#39;y \right)  + β&#39;X&#39; Z W_N Z&#39;yβ \right] \end{aligned}\]</span><br />
FOC<br />
<span class="math display">\[\begin{aligned}\frac{\partial Q}{\partial β} &amp;= -2 X&#39;ZW_N Z&#39;y + 2X&#39;ZW_N Z&#39;Xβ = 0  \\ \hat{β} &amp;= (X&#39;ZW_N Z&#39;X)^{-1} (X&#39;ZW_N Z&#39;y) \end{aligned}\]</span> ~</p>
<blockquote>
<p>PS5.Q2<br />
In model <span class="math inline">\(y = Xβ+ε\)</span>,<br />
Derive the covariance matrix of IV estimator <span class="math inline">\(\hat{β}_{IV} = (Z&#39;X)^{−1}Z&#39;y\)</span> with homoskedastic and serially uncorrelated errors.<br />
Argue why the correlation between instruments and regressors matter for the precision of the IV estimator.<br />
What happens in the limiting case when this correlation approaches zero?</p>
</blockquote>
<p><span class="math display">\[\begin{aligned}\hat{β}_{IV} &amp;= (Z&#39;X)^{−1}Z&#39;y \\
&amp;= (X&#39;ZZ&#39;X)^{-1}X&#39;ZZ&#39;y \\
&amp;= (Z&#39;X)^{-1}(X&#39;Z)^{-1}(X&#39;Z)^{-1}Z&#39;y \\
&amp;= (Z&#39;X)^{-1}Z&#39;y \\
&amp;= (Z&#39;X)^{-1}Z&#39;(Xβ+ε) \\
&amp;= (Z&#39;X)^{-1}Z&#39;Xβ + (Z&#39;X)^{-1}Z&#39;ε \\
&amp;= β + (Z&#39;X)^{-1}Z&#39;ε\end{aligned}\]</span><br />
Derive <span class="math inline">\(V(\hat{β}_{IV})\)</span>:<br />
<span class="math display">\[\begin{aligned}V(\hat{β}_{IV}) &amp;= E\left[ (\hat{β}_{IV} - β)^2 \right] \\
&amp;= E \left[ \left( (Z&#39;X)^{-1}Z&#39;ε \right)^2 \right] \\
&amp;= σ^2 (Z&#39;X)^{-1}Z&#39;Z(X&#39;Z)^{-1}\end{aligned}\]</span> ~</p>
</div>
<div id="give-generalized-iv-estimator" class="section level3">
<h3>GIVE (Generalized IV Estimator)</h3>
<p>(textbook P166, slide 9-2 P8)<br />
Also known as: 2SLS (two-stage least squares) estimator</p>
<blockquote>
<p>PS5.Q3<br />
Derive the GIVE.<br />
1. Start with the first stage regression <span class="math inline">\(X = Zγ + v\)</span> and obtain its fitted values <span class="math inline">\(\hat{X}\)</span>.<br />
~~ Show that the first stage fit <span class="math inline">\(\hat{X}\)</span> is given by <span class="math inline">\(P_ZX\)</span> with <span class="math inline">\(P_Z = Z(Z′Z)^{−1}Z′\)</span>.<br />
2. Consider OLS estimator in the second stage, based on fitted values <span class="math inline">\(\hat{X}\)</span>: <span class="math inline">\(y = \hat{X} β + ε\)</span>.<br />
~~ Show that the resulting estimator equals the GIVE.<br />
3. Derive the covariance matrix of the GIVE under homoskedasticity and absence of autocorrelation.<br />
~~ Compare the results with PS5.Q2.</p>
</blockquote>
<ol style="list-style-type: decimal">
<li><p>First stage regression: <span class="math inline">\(X = Zγ + v\)</span><br />
According to OLS, <span class="math inline">\(\hat{γ} = (Z&#39;Z)^{-1}Z&#39;X\)</span><br />
Fitted values:<br />
<span class="math display">\[\begin{aligned}\hat{X} &amp;= Z \cdot\hat{γ} \\&amp;= Z(Z&#39;Z)^{-1}ZX \\ &amp;= P_Z X\end{aligned}\]</span><br />
<span class="math inline">\(P_Z\)</span> is symmetric:<br />
<span class="math display">\[\begin{aligned} P_Z&#39; &amp;= \left[Z(Z&#39;Z)^{-1}Z&#39; \right]&#39; \\ &amp;= Z(Z&#39;Z)^{-1}Z&#39; \\ &amp;= P_Z\end{aligned}\]</span><br />
<span class="math inline">\(P_Z\)</span> is idopotent:<br />
<span class="math display">\[\begin{aligned} P_ZP_Z &amp;= Z(Z&#39;Z)^{-1}Z&#39; \cdot Z(Z&#39;Z)^{-1}Z&#39; \\
&amp;= Z(Z&#39;Z)^{-1}Z \\
&amp;= P_Z\end{aligned}\]</span> ~</p></li>
<li><p>Second stage regression: <span class="math inline">\(y = \hat{X}β+ ε\)</span><br />
According to OLS, <span class="math inline">\(b = \big(\hat{X}&#39;\hat{X}\big)^{-1} \hat{X}&#39;y\)</span><br />
Therefore<br />
<span class="math display">\[\begin{aligned} \hat{X} &amp;= P_Z X \\
\hat{X}&#39; &amp;= X&#39;P_Z&#39; \\
&amp;= X&#39;P_Z\end{aligned}\]</span><br />
<span class="math display">\[\begin{aligned}b &amp;= (X&#39;P_Z P_ZX)^{-1} X&#39;P_Z y \\
&amp;= (X&#39;P_ZX)^{-1} X&#39;P_Z y \\
&amp;= \big( X&#39;Z(Z&#39;Z)^{-1}ZX\big)^{-1} X&#39;Z(Z&#39;Z)^{-1}Z&#39; y \end{aligned}\]</span> ~</p></li>
<li><p>Variance: (needs check)<br />
<span class="math display">\[\begin{aligned}V(b) &amp;= σ^2 (\hat{X}&#39; \hat{X})^{-1} \\
&amp;= σ^2 (X&#39;P_Z X)^{-1} \\
&amp;= σ^2 (X&#39;Z(Z&#39;Z)^{-1}Z X)^{-1} \\
&amp;= σ^2 (Z&#39;X)^{-1}Z&#39;Z(X&#39;Z)^{-1}\end{aligned}\]</span> ~</p></li>
</ol>
</div>
</div>
<div id="heteroskedasticity-autocorrelation" class="section level2">
<h2>Heteroskedasticity &amp; Autocorrelation</h2>
<div id="white-hetero-consistent-standard-error" class="section level3">
<h3>White (Hetero-Consistent) Standard Error</h3>
<p>(slide 6 P18)</p>
<blockquote>
<p>PS4.Q2.4<br />
T/F?<br />
When applying White robust standard errors under heteroskedasticity, the t-statistics change in comparison to the case of regular standard errors assuming homoskedasticity.</p>
</blockquote>
<p>True.</p>
</div>
<div id="hac-robust-standard-error" class="section level3">
<h3>HAC (Robust) Standard Error</h3>
<p>(textbook P128, slide 7 P14)</p>
<blockquote>
<p>PS4.Q2.3<br />
T/F?<br />
When applying robust standard errors, the <strong>OLS point estimates</strong> (estimators) do not change.</p>
</blockquote>
<p>True.</p>
</div>
</div>
<div id="maximum-likelihood-model" class="section level2">
<h2>Maximum Likelihood Model</h2>
</div>
<div id="compare-models" class="section level2">
<h2>Compare Models</h2>
<div id="choose-explanatory-variables" class="section level3">
<h3>Choose Explanatory Variables</h3>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
1. Compute F-statistics for comparing the loglinear model with and without squared experience.<br />
<img src="/post-img/notes-stat1--PS2Q3-3.png" width="419" /><br />
<img src="/post-img/notes-stat1--PS2Q3-4.png" width="420" /></p>
</blockquote>
<p><span class="math display">\[\begin{aligned} F &amp;= \frac{\frac{1}{J} \cdot (R^2_1 - R^2_0)}{\frac{1}{N-K} \cdot (1-R^2_1)} \\
&amp;= \frac{\frac{1}{1} \cdot (0.3783 - 0.3761)}{\frac{1}{1472 - 5} \cdot (1 - 0.3783)} \\
&amp;= 5.19\end{aligned}\]</span> ~</p>
<pre class="r"><code># quantiles of F distribution
#       (1-α),     (J),       (N - K)
qf( p = 0.95, df1 = 1, df2 = (1472-5) )
## [1] 3.847805
qf( p = 0.99, df1 = 1, df2 = (1472-5) )
## [1] 6.652194</code></pre>
<p>Therefore, <span class="math inline">\(H_0\)</span> is rejected.</p>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
2. In the loglinear model without squared experience, what is expected wage gap in percentages?<br />
<img src="/post-img/notes-stat1--PS2Q3-4.png" width="420" /></p>
</blockquote>
<p>The model:<br />
<span class="math display">\[\begin{aligned} \log y_i &amp;= 1.145 + 0.120 \cdot male + \dots \\ 
y_i &amp;= e^{1.145} + e^{0.120} \cdot e^{male} + \dots\end{aligned}\]</span><br />
Wage gap: <span class="math inline">\(e^{0.120} \cdot 100 \% - 100 \% = 12.7 \%\)</span></p>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
3. Consider the implied estimates regarding the expected increases of log wage due to increases in education (slide 5 P27).<br />
<img src="/post-img/notes-stat1--PS2Q3-4.png" width="420" /><br />
<img src="/post-img/notes-stat1--PS2Q3-0.png" width="497" /><br />
Redo the computation for the loglinear model with squared experience and compare the results:<br />
<img src="/post-img/notes-stat1--PS2Q3-3.png" width="419" /></p>
</blockquote>
<p>Specification 4:<br />
<span class="math display">\[\begin{aligned} 0.437 \times [\log(5) - \log(1)] = 0.703 \\
0.437 \times [\log(4) - \log(1)] = 0.606 \\
0.437 \times [\log(3) - \log(1)] = 0.480 \\
0.437 \times [\log(2) - \log(1)] = 0.302
 \end{aligned}\]</span><br />
Specification 3:<br />
<span class="math display">\[\begin{aligned} 0.442 \times [\log(5) - \log(1)] = 0.711 \\
0.437 \times [\log(4) - \log(1)] = 0.613 \\
0.437 \times [\log(3) - \log(1)] = 0.486 \\
0.437 \times [\log(2) - \log(1)] = 0.306
 \end{aligned}\]</span> ~</p>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
4. Explain why the comparison of specifications 5 and 4 involve <span class="math inline">\(J = 3\)</span> restrictions in the corresponding F-statistic.<br />
<img src="/post-img/notes-stat1--PS2Q3-4.png" width="420" /><br />
<img src="/post-img/notes-stat1--PS2Q3-5.png" width="425" /></p>
</blockquote>
<p>Specification 5 have 3 more explanatory variables.</p>
<blockquote>
<p>PS2.Q3<br />
Consider the wage regressions from slide 5 (text P86).<br />
5. Compute an F-statistic for comparing specifications 6 and 5.<br />
<img src="/post-img/notes-stat1--PS2Q3-6.png" width="393" /><br />
<img src="/post-img/notes-stat1--PS2Q3-5.png" width="425" /></p>
</blockquote>
<p><span class="math display">\[\begin{aligned} F &amp;= \frac{\frac{1}{J} \cdot (R^2_1 - R^2_0)}{\frac{1}{N-K} \cdot (1-R^2_1)} \\
&amp;= \frac{\frac{1}{5} \cdot (0.4032 - 0.3976)}{\frac{1}{1472 - 12} \cdot (1 - 0.4032)} \\
&amp;= 2.740\end{aligned}\]</span> ~</p>
<pre class="r"><code># quantiles of F distribution
#       (1-α),     (J),       (N - K)
qf( p = 0.95, df1 = 5, df2 = (1472 - 12) )
## [1] 2.220228
qf( p = 0.99, df1 = 5, df2 = (1472 - 12) )
## [1] 3.029772</code></pre>
<p>Therefore, <span class="math inline">\(H_0\)</span> is rejected at <span class="math inline">\(5 \%\)</span> level, but not rejected at <span class="math inline">\(1 \%\)</span> level.</p>
</div>
<div id="choose-approach-ols-gls" class="section level3">
<h3>Choose Approach: OLS / GLS</h3>
<blockquote>
<p>PS3.Q2<br />
<code>hetero.txt</code> contains data on <span class="math inline">\(y_i\)</span> and three regressors <span class="math inline">\(x_{2i}\)</span>, <span class="math inline">\(x_{3i}\)</span>, <span class="math inline">\(x_{4i}\)</span> for <span class="math inline">\(i = 1, 2, ..., 250\)</span>.<br />
The task is to find the underlying data generating process (DGP).<br />
The regression model to be estimated shall contain an intercept and all three regressors.<br />
Apply a nominal significance level of five percent in each testing situation.
1. Apply OLS to find the estimated coefficients of the regression model <span class="math inline">\(y_i =β_1 +β_2x_{2i} +β_3x_{3i} +β_4x_{4i} +ε_i\)</span><br />
~~ Report the results in a Table (see PS3.Q1).</p>
</blockquote>
<pre class="r"><code>PS3Q2_data = read.table( file=&quot;hetero.txt&quot;, header = TRUE )
N = dim( PS3Q2_data )[1]
colnames( PS3Q2_data )
## [1] &quot;y&quot;  &quot;x1&quot; &quot;x2&quot; &quot;x3&quot;
str( PS3Q2_data )
## &#39;data.frame&#39;:    250 obs. of  4 variables:
##  $ y : num  10.93 3.99 5.26 1.41 -15.51 ...
##  $ x1: num  0.269 1.708 1.356 0.767 0.74 ...
##  $ x2: num  1.897 0.126 1.472 1.9 -2.64 ...
##  $ x3: num  2.255 7.186 0.994 0.365 4.796 ...

# plot
# library(ggplot2)
# ggplot(PS3Q2_data, aes(x = x1, y = y)) + geom_point()
# ggplot(PS3Q2_data, aes(x = x2, y = y)) + geom_point()
# ggplot(PS3Q2_data, aes(x = x3, y = y)) + geom_point()

# regression
PS3Q2_reg = lm(data = PS3Q2_data, y ~ x1 + x2 + x3)
summary( PS3Q2_reg )
## 
## Call:
## lm(formula = y ~ x1 + x2 + x3, data = PS3Q2_data)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -32.543  -2.827   0.024   3.518  19.846 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  0.78915    0.44450   1.775   0.0771 .  
## x1          -0.87944    0.40636  -2.164   0.0314 *  
## x2           0.98717    0.23097   4.274 2.75e-05 ***
## x3           0.89088    0.09544   9.334  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.01 on 246 degrees of freedom
## Multiple R-squared:  0.294,  Adjusted R-squared:  0.2854 
## F-statistic: 34.16 on 3 and 246 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="r"><code># autogen regression tables
library(stargazer)</code></pre>
<blockquote>
<p>PS3.Q2<br />
2. Compute the correlation matrix of the regressors to see if there is any sign of multi-collinearity.</p>
</blockquote>
<pre class="r"><code>X.mat  = cbind( PS3Q2_data$x1,PS3Q2_data$x2,PS3Q2_data$x3 )
cor( X.mat )
##            [,1]        [,2]        [,3]
## [1,] 1.00000000  0.07205362  0.07434651
## [2,] 0.07205362  1.00000000 -0.05255600
## [3,] 0.07434651 -0.05255600  1.00000000</code></pre>
<p>There is no strong sign of multicollinarity.</p>
<blockquote>
<p>PS3.Q2<br />
3. Generate scatter plots of (i) the residuals and the fitted values and (ii) the residuals and each individual regressor.<br />
Are there any visual signs of heteroskedasticity?</p>
</blockquote>
<pre class="r"><code>res = reg$res
fit = reg$fit
par(mfrow=c(2,2))
plot(fit,res)
plot(dat$x1,res)
plot(dat$x2,res)
plot(dat$x3,res)</code></pre>
<p><span class="math inline">\(x_2\)</span> may have heteroskedasticity</p>
<blockquote>
<p>PS3.Q2<br />
4. In order to investigate more formally whether the residual variances are constant, run heteroskedasticity OLS regressions of the following type<br />
<span class="math display">\[e^2_i =α_1+α_2x^2_{ji}+v_i\]</span><br />
where <span class="math inline">\(j = 2,3,4\)</span>.<br />
Test in each of these simple regressions <span class="math inline">\(H_0 : α2 = 0\)</span>.<br />
Find an appropriate alternative hypothesis.<br />
Explain why we can do these simple regressions using a single regressor safely? Or should we worry about an omitted variable bias here?</p>
</blockquote>
<p>since <span class="math inline">\(e^2\)</span> is var of residual,<br />
we want <span class="math inline">\(a_2\)</span> to be larger than 0 –&gt; one-sided</p>
<pre class="r"><code>#simple regressions
reg.res = lm(I(res^2) ~ I(dat$x1^2))
summary(reg.res)

qnorm(0.95)

reg.res = lm(I(res^2) ~ I(dat$x2^2))
summary(reg.res)

reg.res = lm(I(res^2) ~ I(dat$x3^2))
summary(reg.res)</code></pre>
<p>but: x’s are not correlated<br />
-/-&gt;<br />
transformed x’s are not correlated<br />
have to check for omitted variable bias also for transformed x’s</p>
<pre class="r"><code># sth on multicollinearity in the heterosked regressions
X2.mat  = cbind(dat$x1^2,dat$x2^2,dat$x3^2)
cor(X2.mat)</code></pre>
<blockquote>
<p>PS3.Q2<br />
5. Collect the significant regressors from the three simple regressions to estimated a joint model containing all significant regressors simultaneously.<br />
Such an approach would be called specific-to-general. What is your chosen specification?</p>
</blockquote>
<pre class="r"><code># using two regressors
reg.res = lm(I(res^2) ~ I(dat$x1^2) + I(dat$x2^2))
summary(reg.res)</code></pre>
<blockquote>
<p>PS3.Q2<br />
6. In contrast to specific-to-general, consider the reverse idea and start with a general model:<br />
<span class="math display">\[ e_{2i} = α_1 + α_2 x^2_{2i} + α_3 x^2_{3i} + α_4 x^2_{4i} + v_i \]</span><br />
Eliminate the most insignificant regressor (only one in each step) and re-estimate the restricted model.<br />
Continue until all contained regressors are significant. What is your chosen specification (model)?<br />
Compare the adjusted R2 values across all specifications you have estimated. Which is the one you would choose on the basis of the adjusted R2 measure?</p>
</blockquote>
<pre class="r"><code># using three regressors
reg.res = lm(I(res^2) ~ I(dat$x1^2) + I(dat$x2^2) + I(dat$x3^2))
summary(reg.res)</code></pre>
<p>x3 should be excluded –&gt; same conclusion as spscific to general approach</p>
<blockquote>
<p>PS3.Q2<br />
7. Plot the estimated h2i values. Are there any signs of heteroskedasticity?<br />
Transform the original regression model from a) to carry out a feasible GLS regression.<br />
Compare the FGLS regression results to the OLS results.</p>
</blockquote>
<pre class="r"><code># find the estimated h values
h.estim = reg.res$fit
ts.plot(h.estim)

# fit: 
ts.plot(res^2)
lines( h.estim, col=2)

# FGLS transform
y.star = dat$y/sqrt(h.estim)
x1.star = dat$x1/sqrt(h.estim)
x2.star = dat$x2/sqrt(h.estim)
x3.star = dat$x3/sqrt(h.estim)

reg.star = lm(y.star ~ x1.star + x2.star + x3.star)
summary(reg.star)</code></pre>
<blockquote>
<p>PS3.Q2<br />
8. Apply White standard errors to the OLS regression from (1).<br />
Recompute the t-values and compare the results. What do you find?</p>
</blockquote>
<pre class="r"><code># load sandwich package
library(sandwich)

# apply robust standard errors to OLS regression from line 10
vcovHC(reg)

# round standard deviations
round( sqrt( diag( vcovHC(reg) )), 3 )

# call summary again
summary(reg)

#### compare
summary(reg.star)`

# finally, compare the t-ratios
round(summary(reg)$coef[,1]/sqrt(diag(vcovHC(reg))),3)</code></pre>
<blockquote>
<p>PS3.Q2<br />
True model used in data generation:</p>
</blockquote>
<pre class="r"><code>sd.eps = 1
sd.x1 = 1
sd.x2 = 2
sd.x3 = 5

xi = rnorm( N, 0, sd.xi )
h = 1 + 10*x1^2 + 10*x2^2
y = 1 + 0.8*x2 +0.8*x3 + sqrt(h) * rnorm(N, 0, sd.eps)</code></pre>
</div>
<div id="weird-models" class="section level3">
<h3>Weird Models</h3>
<blockquote>
<p>PS4.Q1<br />
Consider the nonlinear consumption function:<br />
<span class="math display">\[c_t = α + γ y_t^δ\]</span><br />
where <span class="math inline">\(c_t\)</span> is real consumption and <span class="math inline">\(y_t\)</span> real income. <span class="math inline">\(α ≥ 0\)</span>, <span class="math inline">\(γ &gt; 0\)</span> and <span class="math inline">\(δ &gt; 0\)</span>.<br />
1. Can this function be linearized?<br />
2. Under which parameter restriction is the function linear?<br />
3. Could you estimate the parameters <span class="math inline">\(α, γ, δ\)</span> via OLS directly?</p>
</blockquote>
<ol style="list-style-type: decimal">
<li>No.<br />
<span class="math inline">\(\log(c_t) = \log(α + γ y_t^δ)\)</span><br />
</li>
<li><span class="math inline">\(δ=1\)</span> or <span class="math inline">\(α = 0\)</span><br />
</li>
<li>No.<br />
The estimation problem leads to an objective function that is non-linear.</li>
</ol>
<blockquote>
<p>PS4.Q1<br />
Consider the nonlinear consumption function:<br />
<span class="math display">\[c_t = α + γ y_t^δ\]</span><br />
4. An econometrician who favors OLS wants to estimate the parameters in a reasonable way.<br />
The following possibilities are considered. Comment on each of the possibilities (provide arguments), figure out when they could work (and when not), and choose the best option.<br />
i) <span class="math inline">\(c_t = β_1 + β_2 y_t + ε_t\)</span> s.t. hopefully <span class="math inline">\(b_1 = \hat{a}\)</span> and <span class="math inline">\(b_2 = \hat{γ}\)</span>.<br />
ii) <span class="math inline">\(\ln(c_t) = β_1 + β_2 \ln(y_t) + εt\)</span> s.t. hopefully <span class="math inline">\(b_1 = \ln(\hat{γ})\)</span> and <span class="math inline">\(b_2 = \hat{δ}\)</span>.<br />
iii) <span class="math inline">\(c_t = β_1 + β_2y_t^2 + ε_t\)</span> s.t. hopefully <span class="math inline">\(b1 = \hat{α}\)</span>, <span class="math inline">\(b_2 = \hat{γ}\)</span> and <span class="math inline">\(δ = 2\)</span>.<br />
iv) <span class="math inline">\(c_t = β + β \sqrt{y_t} + ε\)</span> s.t. hopefully <span class="math inline">\(b = \hat{α}\)</span>, <span class="math inline">\(b = \hat{γ}\)</span> and <span class="math inline">\(\hat{δ} = \frac{1}{2}\)</span>.<br />
v) <span class="math inline">\(c_t =β_1+β_2 y_t+ε_t\)</span> where <span class="math inline">\(y_t =y_t a\)</span> with <span class="math inline">\(a\in A =[\underline{a}, \bar{a}]\)</span> and <span class="math inline">\(0&lt;\underline{a}&lt;\bar{a}&lt;\infty\)</span>.<br />
~~ The regression is considered for many possible values of <span class="math inline">\(a\)</span>, hoping that <span class="math inline">\(δ\in A\)</span>.<br />
~~ So hopefully <span class="math inline">\(δ = \text{argmin}_{a\in A} s^2(a)\)</span>, and correspondingly, <span class="math inline">\(b_1 = α\)</span> and <span class="math inline">\(b_2 = γ\)</span> from the regression which provides the minimal value for <span class="math inline">\(s^2(a)\)</span>.</p>
</blockquote>
<pre class="r"><code>rm(list=ls())

# number of obs
N = 250

# generate log-normally distributed real income
y = rlnorm(N,3,1)
# set parameters (you can vary those, of course)
beta1 = 1
beta2 = 0.6
beta3 = 0.8
# generate real consumption according to the nonlinear consumption function + random error
c = beta1 + beta2*y^(beta3) + rnorm(N,0,0.1)
plot(x = y, y = c)</code></pre>
<ol style="list-style-type: lower-roman">
<li><span class="math inline">\(c_t = β_1 + β_2y_t + ε_t\)</span> such that hopefully <span class="math inline">\(b_1 = \hat{α}\)</span> and <span class="math inline">\(b_2 = \hat{γ}\)</span><br />
is good when <span class="math inline">\(δ = 1\)</span>:</li>
</ol>
<pre class="r"><code>reg1 = lm(c ~ y)
summary(reg1)</code></pre>
<ol start="2" style="list-style-type: lower-roman">
<li><span class="math inline">\(\ln(c_t) = β_1 + β_2 \ln(y_t) + ε_t\)</span> such that hopefully <span class="math inline">\(b_1 = \ln(\hat{γ})\)</span> and <span class="math inline">\(b_2 = \hat{δ}\)</span><br />
is good when <span class="math inline">\(α = 0\)</span>, <span class="math inline">\(\hat{γ} = \exp(b_1)\)</span> and <span class="math inline">\(\hat{δ} = b_2\)</span><br />
inference on a non-linear transormation of a parameter via the delta method</li>
</ol>
<pre class="r"><code>reg2 = lm(log(c) ~ log(y))
summary(reg2)
exp(reg2$coef[1])</code></pre>
<ol start="3" style="list-style-type: lower-roman">
<li><span class="math inline">\(c_t = β_1 + β_2y_t^2 + ε_t\)</span> such that hopefully <span class="math inline">\(b_1 = \hat{α}\)</span> and <span class="math inline">\(b_2 = \hat{γ}\)</span> and <span class="math inline">\(\hat{δ} = 2\)</span>.<br />
is good when <span class="math inline">\(δ = 2\)</span></li>
</ol>
<pre class="r"><code>reg3 = lm(c ~ I(y^2))
summary(reg3)</code></pre>
<ol start="4" style="list-style-type: lower-roman">
<li><span class="math inline">\(c = β_1 + β_2 \sqrt{y} + ε\)</span> such that hopefully <span class="math inline">\(b_1 = \hat{α}\)</span> and <span class="math inline">\(b_2 = \hat{γ}\)</span> and <span class="math inline">\(\hat{δ} = 1/2\)</span>.<br />
is good when <span class="math inline">\(δ = 1/2\)</span></li>
</ol>
<pre class="r"><code># (iv)
reg4 = lm(c ~ I(sqrt(y)))
summary(reg4)</code></pre>
<p>v)<span class="math inline">\(c_t =β_1+β_2\tilde{y}_t+ε_t\)</span> where <span class="math inline">\(\tilde{y}_t =y_t^a\)</span> with <span class="math inline">\(a\in A=[\underline{a},\bar{a}]\)</span> and <span class="math inline">\(0 &lt; \underline{a} &lt; \bar{a} \leq\infty\)</span><br />
hopefully works w/o restrictions</p>
<pre class="r"><code># (v)
beta3.fix = 1
reg5 = lm(c ~ I(y^beta3.fix))
summary(reg5)

beta3.fix = 1.1
reg5 = lm(c ~ I(y^beta3.fix))
summary(reg5)

beta3.fix = 0.9
reg5 = lm(c ~ I(y^beta3.fix))
summary(reg5)

beta3.fix = 0.8
reg5 = lm(c ~ I(y^beta3.fix))
summary(reg5)

beta3.fix = 1.2
reg5 = lm(c ~ I(y^beta3.fix))
summary(reg5)</code></pre>
<blockquote>
<p>PS4.Q1<br />
5. Use the US data from 1960:Q1 to 2009:Q4 with N = 200 observations available in the data set usdata.txt to estimate the parameters empirically.<br />
In the data set, the first column is real consumption, and the second is real disposable income.<br />
Comment on your findings and summarize them in a regression output table.<br />
6. Following (5), test the restriction <span class="math inline">\(δ = 1\)</span></p>
</blockquote>
<ol start="5" style="list-style-type: decimal">
<li>~</li>
</ol>
<pre class="r"><code># rm(list=ls())

USdata = read.table(file=&quot;usdata.txt&quot;)
c = USdata[,1]
y = USdata[,2]
N = length(y)

# note the data is trending
ts.plot(USdata)

# using (v)
beta.fix.vec = seq( 0.01, 2, 0.01 )
reg5.coef = matrix( NA, length(beta.fix.vec), 2 )
s2 = matrix( NA, length(beta.fix.vec), 1 )

for(i in 1:length( beta.fix.vec )){
    beta3.fix = beta.fix.vec[i]
    reg5 = lm( c ~ I( y^beta3.fix ))
    reg5.coef[i,] = reg5$coef
    s2[i,1] = summary(reg5)$sigma^2 }

# plot objective function
plot(s2)

# find the arg min and run this particular regression
which.min(s2)

i = which.min(s2)
(beta3.fix = beta.fix.vec[i])

reg5 = lm(c ~ I( y^beta3.fix ))
summary(reg5)
round(reg5$coef,2)</code></pre>
<ol start="6" style="list-style-type: decimal">
<li>~</li>
</ol>
<pre class="r"><code># restricted model
reg5r = lm( formula (c ~ y))
summary(reg5r)</code></pre>
<blockquote>
<p>PS4.Q1<br />
7. Plot the residuals and their autocorrelation function.<br />
Find the empirical first-order autocorrelation coefficient.<br />
Use the Box-Pierce test with one and five lags.<br />
State the null and the alternative hypotheses in both cases.<br />
Compute the test statistics and compare them to the appropriate critical values. Interpret your findings briefly.</p>
</blockquote>
<pre class="r"><code># residuals of unrestricted model
e = reg5$res
par( mfrow = c( 1, 2 ))

# plot
ts.plot( e )
abline( h = 0 )

# notice that the auto-correlations are getting smaller
## plot
acf( e )
## table
acf( e, plot = FALSE )</code></pre>
<p>Box-Pierce test with one and five lags:</p>
<pre class="r"><code>( rho1 = acf(e,plot=FALSE)$acf[2] )
( Q1 = N * rho1^2 )
qchisq( 0.95, 1 )
qchisq( 0.99, 1 )

( Q5 = N * sum( I( acf( e, plot = F)$acf[2:6]^2 )) )

qchisq( 0.95, 5 )

# choose lag
log(N)</code></pre>
</div>
</div>
<div id="r-workout" class="section level2">
<h2>R Workout</h2>
<pre class="r"><code>dir()
library(repr)
options(repr.plot.width=4, repr.plot.height=3)</code></pre>
<div id="data-generation" class="section level3">
<h3>Data Generation</h3>
<blockquote>
<p>PS1.Q3<br />
Conduct Monte Carlo simulations.<br />
The idea is to take a data generating process (DGP) with an assumed parameterization, simulate data from it, and estimate the parameters by OLS.<br />
1. The first DGP is given by <span class="math inline">\(y_t = β_1 + β_2 x_t + ε_t\)</span> where <span class="math inline">\(ε_t ∼ \mathcal{N} (0, 1)\)</span>.<br />
~~ The exogenous regressor <span class="math inline">\(x_t\)</span> is set to be always drawn from <span class="math inline">\(\mathcal{N} (0, 1)\)</span>.<br />
~~ Given this regressor, simulate 10000 samples for <span class="math inline">\(y\)</span> of sizes <span class="math inline">\(n = \{10, 50, 100, 500\}\)</span>, using <span class="math inline">\(β_1 = 1\)</span> and <span class="math inline">\(β_2 = 2\)</span>.<br />
2. For each of these samples, compute the OLS estimator <span class="math inline">\(b_2\)</span>.<br />
3. Save the result in a matrix and plot it as a smoothed histogram (i.e. kernel density estimator).<br />
4. Do the same for the DGP where <span class="math inline">\(x_t\)</span> and <span class="math inline">\(ε_t\)</span> are drawn individually from a <span class="math inline">\(t(5)\)</span>-distribution instead.<br />
5. What do you find? Are the assumptions A1–A4 are satisfied?</p>
</blockquote>
</div>
<div id="ols-regression" class="section level3">
<h3>OLS Regression</h3>
<blockquote>
<p>PS2.Q4<br />
For N = 500 observations, generate data from the linear regression model as follows:<br />
<span class="math display">\[y_t = 0.5x_t + ε_t\]</span><br />
with <span class="math inline">\(x_t ∼^{iid} N(0,σ_x^2)\)</span> independent of <span class="math inline">\(ε_t ∼^{iid} N(0,σ_ε^2)\)</span>.<br />
Fix <span class="math inline">\(σ_ε = 4\)</span> and choose different values for <span class="math inline">\(σ_x\)</span>, say 0.01 and 100.<br />
Compute the OLS estimator for the true model (excluding the intercept) and its standard deviation.<br />
1. Verify that the standard deviation can be computed as <span class="math inline">\(\sqrt{ \frac{\sum_t e_t^2}{N-1} \cdot\frac{1}{\sum_t x_t^2}}\)</span>, where <span class="math inline">\(e_t\)</span> denotes the OLS residual.<br />
2. Generate a scatter plot for different values of <span class="math inline">\(σ_x\)</span> to explain why an increasing variance of the regressor makes the OLS estimator more precise.<br />
3. Vary also the sample size and the variance of the errors and repeat (2). What do you find?</p>
</blockquote>
<pre class="r"><code># normal distribution
?rnorm
sd = 100

y &lt;- rnorm( 100, mean = 0, sd)
x &lt;- rnorm( 100, mean = 0, sd)

lm( formula = y ~ x - 1)

plot( x, y )</code></pre>
</div>
</div>

    </div>
  </article>

  
<section id="comments">
  <div id="disqus_thread"></div>
  <script>
  (function() { 
  var d = document, s = d.createElement('script');
  s.src = 'https://loikein-github.disqus.com/embed.js';
  s.setAttribute('data-timestamp', +new Date());
  (d.head || d.body).appendChild(s);
  })();
  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
</section>





</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
          <li>By loikein with love</li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="/js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>

